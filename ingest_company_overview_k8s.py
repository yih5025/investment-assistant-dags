from datetime import datetime, timedelta
import os
import requests
from decimal import Decimal
import json
import time

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# í‘œì¤€ ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_company_overview.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 8, 26),  # ì˜¤ëŠ˜ë¶€í„° ì‹œì‘
    'retries': None,
    'retry_delay': timedelta(minutes=1),
}

def verify_api_keys():
    """API í‚¤ ìœ íš¨ì„± ê²€ì¦ ë° ìƒíƒœ í™•ì¸"""
    api_keys = [
        Variable.get('ALPHA_VANTAGE_API_KEY_5', default_var=None),
        Variable.get('ALPHA_VANTAGE_API_KEY_6', default_var=None)
    ]
    
    valid_keys = []
    for i, key in enumerate(api_keys):
        if not key:
            print(f"âŒ API í‚¤ {i+1} ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            continue
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
        test_url = "https://www.alphavantage.co/query"
        params = {
            'function': 'OVERVIEW',
            'symbol': 'AAPL',
            'apikey': key
        }
        
        try:
            response = requests.get(test_url, params=params, timeout=10)
            data = response.json()
            
            if 'Note' in data and 'API call frequency' in data['Note']:
                print(f"âš ï¸ API í‚¤ {i+1}: í˜¸ì¶œ ì œí•œ ë„ë‹¬ - {data['Note']}")
            elif 'Error Message' in data:
                print(f"âŒ API í‚¤ {i+1}: ì˜¤ë¥˜ - {data['Error Message']}")
            elif 'Information' in data:
                print(f"âš ï¸ API í‚¤ {i+1}: ì •ë³´ ë©”ì‹œì§€ - {data['Information']}")
            elif data.get('Symbol') == 'AAPL':
                print(f"âœ… API í‚¤ {i+1}: ì •ìƒ")
                valid_keys.append(key)
            else:
                print(f"â“ API í‚¤ {i+1}: ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ")
                print(f"   ì‘ë‹µ ìƒ˜í”Œ: {str(data)[:200]}...")
                
        except Exception as e:
            print(f"âŒ API í‚¤ {i+1}: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ - {str(e)}")
        
        # í‚¤ ê°„ ë”œë ˆì´
        time.sleep(3)
    
    print(f"ğŸ”‘ ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤: {len(valid_keys)}/{len(api_keys)}ê°œ")
    return valid_keys

def get_reset_symbols_batch(**context):
    """
    SP500 íšŒì‚¬ë“¤ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ 50ê°œì”© ë¶„í• 
    ìƒˆë¡œìš´ batch_idë¡œ ì‹œì‘
    """
    execution_date = context['execution_date']
    
    # ê°•ì œë¡œ ë°°ì¹˜ 1ë¶€í„° ì‹œì‘í•˜ë„ë¡ ì„¤ì •
    day_of_month = execution_date.day
    batch_number = ((day_of_month - 1) % 10) + 1
    
    print(f"ğŸ“… ì‹¤í–‰ ë‚ ì§œ: {execution_date.strftime('%Y-%m-%d')}")
    print(f"ğŸ”„ ë°°ì¹˜ ë²ˆí˜¸: {batch_number}/10 (ì²˜ìŒë¶€í„° ì¬ì‹œì‘)")
    
    # DB ì—°ê²°
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # ìƒˆë¡œìš´ batch_id ê°•ì œ ìƒì„± (ê¸°ì¡´ ë°ì´í„°ì™€ êµ¬ë¶„)
    print("ğŸ†• ìƒˆë¡œìš´ ë°°ì¹˜ ID ìƒì„± (ì¬ì‹œì‘)")
    max_batch_query = "SELECT COALESCE(MAX(batch_id), 0) FROM company_overview"
    max_batch_result = hook.get_first(max_batch_query)
    batch_id = (max_batch_result[0] if max_batch_result else 0) + 1
    
    print(f"ğŸ†• ì‹ ê·œ ë°°ì¹˜ ID: {batch_id}")
    
    # ì „ì²´ ì‹¬ë³¼ ì¡°íšŒ (ì•ŒíŒŒë²³ ìˆœ ì •ë ¬)
    query = """
    SELECT symbol, company_name 
    FROM sp500_companies 
    ORDER BY symbol ASC
    """
    
    all_symbols = hook.get_records(query)
    
    if not all_symbols:
        raise ValueError("âŒ SP500 companies í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    print(f"ğŸ“Š ì „ì²´ SP500 ê¸°ì—… ìˆ˜: {len(all_symbols)}ê°œ")
    
    # 49ê°œì”© ë¶„í• 
    batch_size = 49
    start_idx = (batch_number - 1) * batch_size
    end_idx = start_idx + batch_size
    
    today_symbols = all_symbols[start_idx:end_idx]
    
    print(f"ğŸ¯ ì˜¤ëŠ˜ ìˆ˜ì§‘ ëŒ€ìƒ: {len(today_symbols)}ê°œ (ì¸ë±ìŠ¤ {start_idx}~{end_idx-1})")
    
    if not today_symbols:
        print("âš ï¸ ì˜¤ëŠ˜ ìˆ˜ì§‘í•  ì‹¬ë³¼ì´ ì—†ìŠµë‹ˆë‹¤ (ë°°ì¹˜ ë²”ìœ„ ì´ˆê³¼)")
        return {
            'batch_id': batch_id,
            'batch_number': batch_number,
            'symbols': [],
            'total_count': 0
        }
    
    symbol_list = [row[0] for row in today_symbols]
    
    print(f"ğŸ“‹ ìˆ˜ì§‘í•  ì‹¬ë³¼ë“¤: {symbol_list}")
    print(f"ğŸ·ï¸ ì‹ ê·œ ë°°ì¹˜ ID: {batch_id}")
    
    # XComì— ì €ì¥
    context['ti'].xcom_push(key='daily_symbols', value=symbol_list)
    context['ti'].xcom_push(key='batch_number', value=batch_number)
    context['ti'].xcom_push(key='batch_id', value=batch_id)
    
    return {
        'batch_id': batch_id,
        'batch_number': batch_number,
        'symbols': symbol_list,
        'total_count': len(symbol_list)
    }

def fetch_company_overview_with_detailed_logging(**context):
    """
    ìƒì„¸í•œ ë¡œê¹…ê³¼ í•¨ê»˜ Company Overview ë°ì´í„° ìˆ˜ì§‘
    ì‹¤íŒ¨ ì›ì¸ì„ ìì„¸íˆ ê¸°ë¡
    """
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    symbol_list = context['ti'].xcom_pull(task_ids='get_reset_symbols', key='daily_symbols')
    batch_number = context['ti'].xcom_pull(task_ids='get_reset_symbols', key='batch_number')
    
    if not symbol_list:
        print("âš ï¸ ì˜¤ëŠ˜ ì²˜ë¦¬í•  ì‹¬ë³¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return {'processed': 0, 'success': 0, 'errors': 0}
    
    # API í‚¤ ê²€ì¦
    print("ğŸ” API í‚¤ ê²€ì¦ ì¤‘...")
    valid_keys = verify_api_keys()
    
    if not valid_keys:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return {'processed': len(symbol_list), 'success': 0, 'errors': len(symbol_list)}
    
    print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤: {len(valid_keys)}ê°œ")
    
    # ìˆ˜ì§‘ íŒŒë¼ë¯¸í„°
    collected_data = []
    success_count = 0
    error_count = 0
    detailed_errors = {}
    
    base_url = "https://www.alphavantage.co/query"
    
    # ì‹¬ë³¼ë³„ ì²˜ë¦¬
    for i, symbol in enumerate(symbol_list):
        try:
            # API í‚¤ ì„ íƒ (ìˆœí™˜)
            api_key = valid_keys[i % len(valid_keys)]
            api_key_index = (i % len(valid_keys)) + 1
            
            print(f"ğŸ” [{i+1}/{len(symbol_list)}] {symbol} ì²˜ë¦¬ ì¤‘... (APIí‚¤ {api_key_index})")
            
            params = {
                'function': 'OVERVIEW',
                'symbol': symbol,
                'apikey': api_key
            }
            
            # API í˜¸ì¶œ
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            # ì›ë³¸ ì‘ë‹µ ë‚´ìš© í™•ì¸ (ë””ë²„ê¹…ìš©)
            response_text = response.text
            print(f"   ğŸ“¡ ì‘ë‹µ í¬ê¸°: {len(response_text)} ë°”ì´íŠ¸")
            
            # Rate Limit ì²´í¬
            if 'Note' in response_text and 'API call frequency' in response_text:
                print(f"âš ï¸ API í˜¸ì¶œ ì œí•œ ë„ë‹¬: {symbol}")
                print(f"   ë©”ì‹œì§€: {response_text[:200]}...")
                detailed_errors[symbol] = "API_RATE_LIMIT"
                error_count += 1
                
                # Rate limit ë°œìƒì‹œ ë” ê¸´ ëŒ€ê¸°
                wait_time = 60
                print(f"   â³ Rate Limit ëŒ€ê¸°: {wait_time}ì´ˆ...")
                time.sleep(wait_time)
                continue
            
            # JSON íŒŒì‹±
            try:
                data = response.json()
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {symbol}")
                print(f"   ì‘ë‹µ ë‚´ìš©: {response_text[:500]}...")
                detailed_errors[symbol] = f"JSON_PARSE_ERROR: {str(e)}"
                error_count += 1
                continue
            
            # ì‘ë‹µ ë°ì´í„° ìƒì„¸ ë¶„ì„
            print(f"   ğŸ“Š ì‘ë‹µ í‚¤ë“¤: {list(data.keys())}")
            
            # ë‹¤ì–‘í•œ ì—ëŸ¬ ì¼€ì´ìŠ¤ ì²´í¬
            if 'Error Message' in data:
                print(f"âŒ API ì˜¤ë¥˜: {symbol}")
                print(f"   ì—ëŸ¬ ë©”ì‹œì§€: {data.get('Error Message')}")
                detailed_errors[symbol] = f"API_ERROR: {data.get('Error Message')}"
                error_count += 1
                continue
                
            elif 'Information' in data:
                print(f"âš ï¸ ì •ë³´ ë©”ì‹œì§€: {symbol}")
                print(f"   ë©”ì‹œì§€: {data.get('Information')}")
                detailed_errors[symbol] = f"INFO_MESSAGE: {data.get('Information')}"
                error_count += 1
                continue
                
            elif 'Note' in data:
                print(f"âš ï¸ ì£¼ì˜ ì‚¬í•­: {symbol}")
                print(f"   ë©”ì‹œì§€: {data.get('Note')}")
                detailed_errors[symbol] = f"NOTE_MESSAGE: {data.get('Note')}"
                error_count += 1
                continue
            
            # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            symbol_field = data.get('Symbol', '')
            name_field = data.get('Name', '')
            
            print(f"   ğŸ·ï¸ Symbol: '{symbol_field}'")
            print(f"   ğŸ¢ Name: '{name_field}'")
            
            if not symbol_field or not name_field:
                print(f"âŒ í•„ìˆ˜ ë°ì´í„° ëˆ„ë½: {symbol}")
                print(f"   Symbol ì¡´ì¬: {'Yes' if symbol_field else 'No'}")
                print(f"   Name ì¡´ì¬: {'Yes' if name_field else 'No'}")
                print(f"   ì „ì²´ ì‘ë‹µ: {json.dumps(data, indent=2)[:1000]}...")
                detailed_errors[symbol] = "MISSING_REQUIRED_FIELDS"
                error_count += 1
                continue
            
            # ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘
            collected_data.append({
                'symbol': symbol,
                'data': data,
                'api_key_used': api_key_index
            })
            
            success_count += 1
            print(f"âœ… {symbol} ìˆ˜ì§‘ ì™„ë£Œ - {name_field}")
            print(f"   ì„¹í„°: {data.get('Sector', 'N/A')}")
            print(f"   ì‹œê°€ì´ì•¡: {data.get('MarketCapitalization', 'N/A')}")
            
            # ë‹¤ìŒ ìš”ì²­ ì „ ì¶©ë¶„í•œ ëŒ€ê¸° (Rate Limit ë°©ì§€)
            if i < len(symbol_list) - 1:
                delay = 25  # 25ì´ˆ ëŒ€ê¸°ë¡œ ì¦ê°€
                print(f"   â³ ë‹¤ìŒ ìš”ì²­ê¹Œì§€ {delay}ì´ˆ ëŒ€ê¸°...")
                time.sleep(delay)
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ {symbol} ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}")
            detailed_errors[symbol] = f"NETWORK_ERROR: {str(e)}"
            error_count += 1
            continue
            
        except Exception as e:
            print(f"âŒ {symbol} ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {str(e)}")
            detailed_errors[symbol] = f"UNKNOWN_ERROR: {str(e)}"
            error_count += 1
            continue
    
    print(f"ğŸ¯ ë°°ì¹˜ {batch_number} ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"   âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"   âŒ ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"   ğŸ“Š ì´ ì²˜ë¦¬: {len(symbol_list)}ê°œ")
    print(f"   ğŸ“ˆ ì„±ê³µë¥ : {success_count/len(symbol_list)*100:.1f}%")
    
    # ì‹¤íŒ¨ ì›ì¸ ìƒì„¸ ë¶„ì„
    if detailed_errors:
        print("\nğŸ“‹ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:")
        error_types = {}
        for symbol, error in detailed_errors.items():
            error_type = error.split(':')[0]
            if error_type not in error_types:
                error_types[error_type] = []
            error_types[error_type].append(symbol)
        
        for error_type, symbols in error_types.items():
            print(f"   {error_type}: {len(symbols)}ê°œ ({symbols[:5]}{'...' if len(symbols) > 5 else ''})")
    
    # XComì— ì €ì¥
    context['ti'].xcom_push(key='company_data', value=collected_data)
    context['ti'].xcom_push(key='detailed_errors', value=detailed_errors)
    
    return {
        'batch_number': batch_number,
        'processed': len(symbol_list),
        'success': success_count,
        'errors': error_count,
        'success_rate': round(success_count/len(symbol_list)*100, 1),
        'error_details': detailed_errors
    }

def process_and_store_reset_data(**context):
    """
    ìˆ˜ì§‘ëœ Company Overview ë°ì´í„°ë¥¼ PostgreSQLì— ì €ì¥
    """
    company_data = context['ti'].xcom_pull(task_ids='fetch_reset_data', key='company_data')
    batch_number = context['ti'].xcom_pull(task_ids='get_reset_symbols', key='batch_number')
    batch_id = context['ti'].xcom_pull(task_ids='get_reset_symbols', key='batch_id')
    
    if not company_data:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return {'stored': 0, 'success': 0, 'errors': 0}
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    print(f"ğŸ’¾ ë°°ì¹˜ {batch_number}: {len(company_data)}ê°œ ë°ì´í„° ì €ì¥ ì‹œì‘")
    print(f"ğŸ·ï¸ ë°°ì¹˜ ID: {batch_id}")
    
    success_count = 0
    error_count = 0
    
    for item in company_data:
        try:
            symbol = item['symbol']
            data = item['data']
            
            # ìˆ«ì ë°ì´í„° ì•ˆì „í•˜ê²Œ ë³€í™˜
            def safe_decimal(value):
                try:
                    if value == 'None' or value is None or value == '-' or value == '':
                        return None
                    return Decimal(str(value))
                except:
                    return None
            
            def safe_int(value):
                try:
                    if value == 'None' or value is None or value == '-' or value == '':
                        return None
                    return int(float(str(value)))
                except:
                    return None
            
            def safe_str(value, max_length=None):
                try:
                    if value is None or value == 'None':
                        return None
                    result = str(value).strip()
                    if max_length and len(result) > max_length:
                        result = result[:max_length]
                    return result if result else None
                except:
                    return None
            
            # SQL íŒŒë¼ë¯¸í„° ì¤€ë¹„
            params = {
                'batch_id': batch_id,
                'symbol': symbol,
                'asset_type': safe_str(data.get('AssetType'), 50),
                'name': safe_str(data.get('Name'), 200),
                'description': safe_str(data.get('Description')),
                'cik': safe_str(data.get('CIK'), 20),
                'exchange': safe_str(data.get('Exchange'), 20),
                'currency': safe_str(data.get('Currency'), 10),
                'country': safe_str(data.get('Country'), 50),
                'sector': safe_str(data.get('Sector'), 100),
                'industry': safe_str(data.get('Industry'), 200),
                'address': safe_str(data.get('Address'), 300),
                'official_site': safe_str(data.get('OfficialSite'), 200),
                'fiscal_year_end': safe_str(data.get('FiscalYearEnd'), 20),
                'latest_quarter': safe_str(data.get('LatestQuarter'), 20),
                'market_capitalization': safe_int(data.get('MarketCapitalization')),
                'ebitda': safe_int(data.get('EBITDA')),
                'pe_ratio': safe_decimal(data.get('PERatio')),
                'peg_ratio': safe_decimal(data.get('PEGRatio')),
                'book_value': safe_decimal(data.get('BookValue')),
                'dividend_per_share': safe_decimal(data.get('DividendPerShare')),
                'dividend_yield': safe_decimal(data.get('DividendYield')),
                'eps': safe_decimal(data.get('EPS')),
                'revenue_per_share_ttm': safe_decimal(data.get('RevenuePerShareTTM')),
                'profit_margin': safe_decimal(data.get('ProfitMargin')),
                'operating_margin_ttm': safe_decimal(data.get('OperatingMarginTTM')),
                'return_on_assets_ttm': safe_decimal(data.get('ReturnOnAssetsTTM')),
                'return_on_equity_ttm': safe_decimal(data.get('ReturnOnEquityTTM')),
                'revenue_ttm': safe_int(data.get('RevenueTTM')),
                'gross_profit_ttm': safe_int(data.get('GrossProfitTTM')),
                'diluted_eps_ttm': safe_decimal(data.get('DilutedEPSTTM')),
                'quarterly_earnings_growth_yoy': safe_decimal(data.get('QuarterlyEarningsGrowthYOY')),
                'quarterly_revenue_growth_yoy': safe_decimal(data.get('QuarterlyRevenueGrowthYOY')),
                'analyst_target_price': safe_decimal(data.get('AnalystTargetPrice')),
                'trailing_pe': safe_decimal(data.get('TrailingPE')),
                'forward_pe': safe_decimal(data.get('ForwardPE')),
                'price_to_sales_ratio_ttm': safe_decimal(data.get('PriceToSalesRatioTTM')),
                'price_to_book_ratio': safe_decimal(data.get('PriceToBookRatio')),
                'ev_to_revenue': safe_decimal(data.get('EVToRevenue')),
                'ev_to_ebitda': safe_decimal(data.get('EVToEBITDA')),
                'beta': safe_decimal(data.get('Beta')),
                'week_52_high': safe_decimal(data.get('52WeekHigh')),
                'week_52_low': safe_decimal(data.get('52WeekLow')),
                'day_50_moving_average': safe_decimal(data.get('50DayMovingAverage')),
                'day_200_moving_average': safe_decimal(data.get('200DayMovingAverage')),
                'shares_outstanding': safe_int(data.get('SharesOutstanding')),
                'dividend_date': safe_str(data.get('DividendDate'), 20),
                'ex_dividend_date': safe_str(data.get('ExDividendDate'), 20)
            }
            
            # SQL ì‹¤í–‰
            hook.run(UPSERT_SQL, parameters=params)
            success_count += 1
            
            print(f"âœ… {symbol} ì €ì¥ ì™„ë£Œ - {params['name']}")
            
        except Exception as e:
            print(f"âŒ {symbol} ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            error_count += 1
            continue
    
    print(f"ğŸ’¾ ë°°ì¹˜ {batch_number} ì €ì¥ ì™„ë£Œ:")
    print(f"   âœ… ì„±ê³µ: {success_count}ê°œ") 
    print(f"   âŒ ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"   ğŸ“ˆ ì €ì¥ë¥ : {success_count/len(company_data)*100:.1f}%")
    
    return {
        'batch_number': batch_number,
        'batch_id': batch_id,
        'stored': len(company_data),
        'success': success_count,
        'errors': error_count,
        'success_rate': round(success_count/len(company_data)*100, 1)
    }

# DAG ì •ì˜
with DAG(
    dag_id='ingest_company_overview_reset_k8s',
    default_args=default_args,
    schedule_interval='0 4 * * *',  # ë§¤ì¼ ìƒˆë²½ 4ì‹œ ì‹¤í–‰
    catchup=False,
    description='SP500 ê¸°ì—… ì •ë³´ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìˆ˜ì§‘ (ìƒì„¸ ë¡œê¹… + 25ì´ˆ ë”œë ˆì´)',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['sp500', 'alpha_vantage', 'company_overview', 'reset', 'detailed_logging'],
    max_active_runs=1,  # ë™ì‹œ ì‹¤í–‰ ë°©ì§€
) as dag:
    
    # 1. í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_company_overview_table',
        postgres_conn_id='postgres_default',
        sql='create_company_overview.sql',
    )
    
    # 2. ë¦¬ì…‹ëœ ì‹¬ë³¼ ë°°ì¹˜ ì„ ì •
    get_symbols = PythonOperator(
        task_id='get_reset_symbols',
        python_callable=get_reset_symbols_batch,
    )
    
    # 3. ìƒì„¸ ë¡œê¹…ê³¼ í•¨ê»˜ ë°ì´í„° ìˆ˜ì§‘
    fetch_data = PythonOperator(
        task_id='fetch_reset_data',
        python_callable=fetch_company_overview_with_detailed_logging,
        execution_timeout=timedelta(hours=3),  # 3ì‹œê°„ íƒ€ì„ì•„ì›ƒ
    )
    
    # 4. ë°ì´í„° ì €ì¥
    process_data = PythonOperator(
        task_id='process_and_store_reset_data',
        python_callable=process_and_store_reset_data,
    )
    
    # Task ì˜ì¡´ì„±
    create_table >> get_symbols >> fetch_data >> process_data