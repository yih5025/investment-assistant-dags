from datetime import datetime, timedelta
import os
import requests
from decimal import Decimal
import json
import time

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# í‘œì¤€ ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_company_overview.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 8, 26),
    'retries': None,
    'retry_delay': timedelta(minutes=1),
}

def get_or_create_collection_progress(**context):
    """
    ìˆ˜ì§‘ ì§„í–‰ìƒí™© ì¶”ì  ë° ë‹¤ìŒ ë°°ì¹˜ ì‹¬ë³¼ ì„ ì •
    ì‹¤íŒ¨í•œ ì‹¬ë³¼ë¶€í„° ì¬ì‹œì‘í•˜ëŠ” ë¡œì§
    """
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # ì§„í–‰ìƒí™© í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
    create_progress_table = """
    CREATE TABLE IF NOT EXISTS company_overview_progress (
        id SERIAL PRIMARY KEY,
        collection_name TEXT NOT NULL DEFAULT 'sp500_full',
        current_position INTEGER NOT NULL DEFAULT 0,
        total_symbols INTEGER NOT NULL,
        last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        status TEXT DEFAULT 'active'
    );
    """
    hook.run(create_progress_table)
    
    # í˜„ì¬ ì§„í–‰ìƒí™© ì¡°íšŒ
    progress_query = """
    SELECT current_position, total_symbols 
    FROM company_overview_progress 
    WHERE collection_name = 'sp500_full' AND status = 'active'
    ORDER BY last_updated DESC 
    LIMIT 1
    """
    
    progress_result = hook.get_first(progress_query)
    
    # ì „ì²´ ì‹¬ë³¼ ì¡°íšŒ (ì•ŒíŒŒë²³ ìˆœ ì •ë ¬)
    symbols_query = """
    SELECT symbol, company_name 
    FROM sp500_companies 
    ORDER BY symbol ASC
    """
    all_symbols = hook.get_records(symbols_query)
    
    if not all_symbols:
        raise ValueError("SP500 companies í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    total_count = len(all_symbols)
    print(f"ì „ì²´ S&P 500 ê¸°ì—… ìˆ˜: {total_count}ê°œ")
    
    # ì§„í–‰ìƒí™©ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if not progress_result:
        print("ìƒˆë¡œìš´ ìˆ˜ì§‘ ì„¸ì…˜ ì‹œì‘")
        current_position = 0
        
        # ì§„í–‰ìƒí™© ì´ˆê¸°í™”
        init_progress = """
        INSERT INTO company_overview_progress (collection_name, current_position, total_symbols)
        VALUES ('sp500_full', 0, %s)
        """
        hook.run(init_progress, parameters=[total_count])
    else:
        current_position = progress_result[0]
        print(f"ì´ì „ ì§„í–‰ìƒí™©ì—ì„œ ì¬ì‹œì‘: ìœ„ì¹˜ {current_position}")
    
    # ì˜¤ëŠ˜ ìˆ˜ì§‘í•  ì‹¬ë³¼ë“¤ ì„ ì • (ìµœëŒ€ 25ê°œ, ì•ˆì „í•˜ê²Œ)
    max_daily_requests = 20  # Rate Limit ê³ ë ¤í•˜ì—¬ ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
    end_position = min(current_position + max_daily_requests, total_count)
    
    if current_position >= total_count:
        print("ëª¨ë“  ì‹¬ë³¼ ìˆ˜ì§‘ ì™„ë£Œ!")
        # ìˆ˜ì§‘ ì™„ë£Œ í‘œì‹œ
        complete_query = """
        UPDATE company_overview_progress 
        SET status = 'completed', last_updated = CURRENT_TIMESTAMP
        WHERE collection_name = 'sp500_full' AND status = 'active'
        """
        hook.run(complete_query)
        
        return {
            'symbols': [],
            'current_position': current_position,
            'end_position': current_position,
            'completed': True,
            'total_count': total_count
        }
    
    # ì˜¤ëŠ˜ ìˆ˜ì§‘í•  ì‹¬ë³¼ë“¤
    today_symbols = all_symbols[current_position:end_position]
    symbol_list = [row[0] for row in today_symbols]
    
    print(f"ì˜¤ëŠ˜ ìˆ˜ì§‘ ë²”ìœ„: {current_position} ~ {end_position-1} (ì´ {len(symbol_list)}ê°œ)")
    print(f"ìˆ˜ì§‘ ì‹¬ë³¼ë“¤: {symbol_list}")
    print(f"ì „ì²´ ì§„í–‰ë¥ : {current_position}/{total_count} ({current_position/total_count*100:.1f}%)")
    
    # ë°°ì¹˜ ID ìƒì„±
    max_batch_query = "SELECT COALESCE(MAX(batch_id), 0) FROM company_overview"
    max_batch_result = hook.get_first(max_batch_query)
    batch_id = (max_batch_result[0] if max_batch_result else 0) + 1
    
    # XComì— ì €ì¥
    context['ti'].xcom_push(key='daily_symbols', value=symbol_list)
    context['ti'].xcom_push(key='current_position', value=current_position)
    context['ti'].xcom_push(key='end_position', value=end_position)
    context['ti'].xcom_push(key='batch_id', value=batch_id)
    
    return {
        'symbols': symbol_list,
        'current_position': current_position,
        'end_position': end_position,
        'completed': False,
        'total_count': total_count,
        'batch_id': batch_id
    }

def fetch_company_overview_progressive(**context):
    """
    ì§„í–‰í˜• ë°ì´í„° ìˆ˜ì§‘: ì„±ê³µí•œ ë§Œí¼ë§Œ ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸
    """
    symbol_list = context['ti'].xcom_pull(task_ids='get_collection_progress', key='daily_symbols')
    current_position = context['ti'].xcom_pull(task_ids='get_collection_progress', key='current_position')
    
    if not symbol_list:
        print("ì˜¤ëŠ˜ ì²˜ë¦¬í•  ì‹¬ë³¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return {'processed': 0, 'success': 0, 'errors': 0, 'successful_symbols': []}
    
    # API í‚¤ ì„¤ì •
    api_keys = [
        Variable.get('ALPHA_VANTAGE_API_KEY_3', default_var=None),
        Variable.get('ALPHA_VANTAGE_API_KEY_4', default_var=None)
    ]
    
    valid_keys = [key for key in api_keys if key]
    if not valid_keys:
        print("ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return {'processed': len(symbol_list), 'success': 0, 'errors': len(symbol_list), 'successful_symbols': []}
    
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤: {len(valid_keys)}ê°œ")
    
    # ìˆ˜ì§‘ ì‹œì‘
    collected_data = []
    successful_symbols = []  # ì„±ê³µí•œ ì‹¬ë³¼ë“¤ë§Œ ì¶”ì 
    success_count = 0
    error_count = 0
    
    base_url = "https://www.alphavantage.co/query"
    
    for i, symbol in enumerate(symbol_list):
        try:
            # API í‚¤ ìˆœí™˜ ì‚¬ìš©
            api_key = valid_keys[i % len(valid_keys)]
            
            print(f"[{i+1}/{len(symbol_list)}] {symbol} ì²˜ë¦¬ ì¤‘... (ìœ„ì¹˜: {current_position + i})")
            
            params = {
                'function': 'OVERVIEW',
                'symbol': symbol,
                'apikey': api_key
            }
            
            # API í˜¸ì¶œ
            response = requests.get(base_url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # Rate Limit ì²´í¬ (ì¦‰ì‹œ ì¤‘ë‹¨)
            if ('Information' in data and 'rate limit' in data['Information'].lower()) or \
               ('Note' in data and 'API call frequency' in data.get('Note', '')):
                
                print(f"Rate Limit ë„ë‹¬: {symbol}")
                print(f"í˜„ì¬ê¹Œì§€ {success_count}ê°œ ì„±ê³µ ìˆ˜ì§‘")
                break
            
            # ì—ëŸ¬ ì²´í¬
            if 'Error Message' in data:
                print(f"API ì˜¤ë¥˜: {symbol} - {data.get('Error Message')}")
                error_count += 1
                continue
            
            # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
            if not data.get('Symbol') or not data.get('Name'):
                print(f"í•„ìˆ˜ ë°ì´í„° ëˆ„ë½: {symbol}")
                error_count += 1
                continue
            
            # ì„±ê³µ ì²˜ë¦¬
            collected_data.append({
                'symbol': symbol,
                'data': data,
                'position': current_position + i
            })
            
            successful_symbols.append(symbol)
            success_count += 1
            
            print(f"âœ… {symbol} ìˆ˜ì§‘ ì™„ë£Œ - {data.get('Name')}")
            
            # ì•ˆì „í•œ ë”œë ˆì´
            if i < len(symbol_list) - 1:
                time.sleep(30)  # 30ì´ˆ ëŒ€ê¸°
                
        except Exception as e:
            print(f"âŒ {symbol} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            error_count += 1
            continue
    
    print(f"\nìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"   âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"   âŒ ì‹¤íŒ¨: {error_count}ê°œ")
    print(f"   ğŸ“ˆ ì„±ê³µë¥ : {success_count/(success_count+error_count)*100:.1f}%")
    
    # XComì— ì €ì¥
    context['ti'].xcom_push(key='company_data', value=collected_data)
    context['ti'].xcom_push(key='successful_symbols', value=successful_symbols)
    context['ti'].xcom_push(key='success_count', value=success_count)
    
    return {
        'processed': len(symbol_list),
        'success': success_count,
        'errors': error_count,
        'successful_symbols': successful_symbols,
        'success_rate': round(success_count/(success_count+error_count)*100, 1) if (success_count+error_count) > 0 else 0
    }

def update_progress_and_store_data(**context):
    """
    ì„±ê³µí•œ ì‹¬ë³¼ë“¤ë§Œí¼ ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸ + ë°ì´í„° ì €ì¥
    """
    company_data = context['ti'].xcom_pull(task_ids='fetch_progressive_data', key='company_data')
    successful_symbols = context['ti'].xcom_pull(task_ids='fetch_progressive_data', key='successful_symbols')
    success_count = context['ti'].xcom_pull(task_ids='fetch_progressive_data', key='success_count')
    current_position = context['ti'].xcom_pull(task_ids='get_collection_progress', key='current_position')
    batch_id = context['ti'].xcom_pull(task_ids='get_collection_progress', key='batch_id')
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # ë°ì´í„° ì €ì¥
    stored_count = 0
    if company_data:
        print(f"ë°ì´í„° ì €ì¥ ì‹œì‘: {len(company_data)}ê°œ")
        
        for item in company_data:
            try:
                symbol = item['symbol']
                data = item['data']
                
                # ë°ì´í„° ë³€í™˜ í•¨ìˆ˜ë“¤
                def safe_decimal(value):
                    try:
                        if value == 'None' or value is None or value == '-' or value == '':
                            return None
                        return Decimal(str(value))
                    except:
                        return None
                
                def safe_int(value):
                    try:
                        if value == 'None' or value is None or value == '-' or value == '':
                            return None
                        return int(float(str(value)))
                    except:
                        return None
                
                def safe_str(value, max_length=None):
                    try:
                        if value is None or value == 'None':
                            return None
                        result = str(value).strip()
                        if max_length and len(result) > max_length:
                            result = result[:max_length]
                        return result if result else None
                    except:
                        return None
                
                # SQL íŒŒë¼ë¯¸í„° ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)
                params = {
                    'batch_id': batch_id,
                    'symbol': symbol,
                    'asset_type': safe_str(data.get('AssetType'), 50),
                    'name': safe_str(data.get('Name'), 200),
                    'description': safe_str(data.get('Description')),
                    'cik': safe_str(data.get('CIK'), 20),
                    'exchange': safe_str(data.get('Exchange'), 20),
                    'currency': safe_str(data.get('Currency'), 10),
                    'country': safe_str(data.get('Country'), 50),
                    'sector': safe_str(data.get('Sector'), 100),
                    'industry': safe_str(data.get('Industry'), 200),
                    'address': safe_str(data.get('Address'), 300),
                    'official_site': safe_str(data.get('OfficialSite'), 200),
                    'fiscal_year_end': safe_str(data.get('FiscalYearEnd'), 20),
                    'latest_quarter': safe_str(data.get('LatestQuarter'), 20),
                    'market_capitalization': safe_int(data.get('MarketCapitalization')),
                    'ebitda': safe_int(data.get('EBITDA')),
                    'pe_ratio': safe_decimal(data.get('PERatio')),
                    'peg_ratio': safe_decimal(data.get('PEGRatio')),
                    'book_value': safe_decimal(data.get('BookValue')),
                    'dividend_per_share': safe_decimal(data.get('DividendPerShare')),
                    'dividend_yield': safe_decimal(data.get('DividendYield')),
                    'eps': safe_decimal(data.get('EPS')),
                    'revenue_per_share_ttm': safe_decimal(data.get('RevenuePerShareTTM')),
                    'profit_margin': safe_decimal(data.get('ProfitMargin')),
                    'operating_margin_ttm': safe_decimal(data.get('OperatingMarginTTM')),
                    'return_on_assets_ttm': safe_decimal(data.get('ReturnOnAssetsTTM')),
                    'return_on_equity_ttm': safe_decimal(data.get('ReturnOnEquityTTM')),
                    'revenue_ttm': safe_int(data.get('RevenueTTM')),
                    'gross_profit_ttm': safe_int(data.get('GrossProfitTTM')),
                    'diluted_eps_ttm': safe_decimal(data.get('DilutedEPSTTM')),
                    'quarterly_earnings_growth_yoy': safe_decimal(data.get('QuarterlyEarningsGrowthYOY')),
                    'quarterly_revenue_growth_yoy': safe_decimal(data.get('QuarterlyRevenueGrowthYOY')),
                    'analyst_target_price': safe_decimal(data.get('AnalystTargetPrice')),
                    'trailing_pe': safe_decimal(data.get('TrailingPE')),
                    'forward_pe': safe_decimal(data.get('ForwardPE')),
                    'price_to_sales_ratio_ttm': safe_decimal(data.get('PriceToSalesRatioTTM')),
                    'price_to_book_ratio': safe_decimal(data.get('PriceToBookRatio')),
                    'ev_to_revenue': safe_decimal(data.get('EVToRevenue')),
                    'ev_to_ebitda': safe_decimal(data.get('EVToEBITDA')),
                    'beta': safe_decimal(data.get('Beta')),
                    'week_52_high': safe_decimal(data.get('52WeekHigh')),
                    'week_52_low': safe_decimal(data.get('52WeekLow')),
                    'day_50_moving_average': safe_decimal(data.get('50DayMovingAverage')),
                    'day_200_moving_average': safe_decimal(data.get('200DayMovingAverage')),
                    'shares_outstanding': safe_int(data.get('SharesOutstanding')),
                    'dividend_date': safe_str(data.get('DividendDate'), 20),
                    'ex_dividend_date': safe_str(data.get('ExDividendDate'), 20)
                }
                
                # SQL ì‹¤í–‰
                hook.run(UPSERT_SQL, parameters=params)
                stored_count += 1
                print(f"âœ… {symbol} ì €ì¥ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ {symbol} ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                continue
    
    # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸: ì„±ê³µí•œ ë§Œí¼ë§Œ í¬ì¸í„° ì´ë™
    if success_count > 0:
        new_position = current_position + success_count
        
        update_progress = """
        UPDATE company_overview_progress 
        SET current_position = %s, last_updated = CURRENT_TIMESTAMP
        WHERE collection_name = 'sp500_full' AND status = 'active'
        """
        hook.run(update_progress, parameters=[new_position])
        
        print(f"ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸: {current_position} -> {new_position}")
        print(f"ì„±ê³µí•œ ì‹¬ë³¼ë“¤: {successful_symbols}")
    else:
        print("ì„±ê³µí•œ ìˆ˜ì§‘ì´ ì—†ì–´ ì§„í–‰ìƒí™© ìœ ì§€")
    
    return {
        'stored_count': stored_count,
        'success_count': success_count,
        'updated_position': current_position + success_count if success_count > 0 else current_position,
        'batch_id': batch_id
    }

# DAG ì •ì˜
with DAG(
    dag_id='ingest_company_overview_progressive_k8s',
    default_args=default_args,
    schedule_interval='0 6 * * *',  # ë§¤ì¼ ìƒˆë²½ 6ì‹œ
    catchup=False,
    description='S&P 500 ì§„í–‰í˜• ìˆ˜ì§‘: ì‹¤íŒ¨í•œ ì‹¬ë³¼ë¶€í„° ì¬ì‹œì‘',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['sp500', 'alpha_vantage', 'company_overview', 'progressive', 'resume'],
    max_active_runs=1,
) as dag:
    
    # 1. í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_company_overview_table',
        postgres_conn_id='postgres_default',
        sql='create_company_overview.sql',
    )
    
    # 2. ì§„í–‰ìƒí™© í™•ì¸ ë° ë‹¤ìŒ ë°°ì¹˜ ì„ ì •
    get_progress = PythonOperator(
        task_id='get_collection_progress',
        python_callable=get_or_create_collection_progress,
    )
    
    # 3. ì§„í–‰í˜• ë°ì´í„° ìˆ˜ì§‘
    fetch_data = PythonOperator(
        task_id='fetch_progressive_data',
        python_callable=fetch_company_overview_progressive,
        execution_timeout=timedelta(hours=2),
    )
    
    # 4. ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸ ë° ë°ì´í„° ì €ì¥
    update_and_store = PythonOperator(
        task_id='update_progress_and_store',
        python_callable=update_progress_and_store_data,
    )
    
    # Task ì˜ì¡´ì„±
    create_table >> get_progress >> fetch_data >> update_and_store