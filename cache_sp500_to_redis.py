# cache_sp500_to_redis.py - SP500 ë°ì´í„°ë¥¼ DBì—ì„œ ì¡°íšŒí•˜ì—¬ Redisì— ìºì‹±
from airflow import DAG
from airflow.decorators import task, dag
from airflow.hooks.postgres_hook import PostgresHook
from airflow.exceptions import AirflowSkipException
from datetime import datetime, timedelta, time
import json
import logging
import os
import redis
import pytz

logger = logging.getLogger(__name__)

# ì‹œì¥ ê°œì¥ ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜
def get_market_open_close_kst():
    """ë¯¸êµ­ ì‹œì¥ ê°œì¥/ë§ˆê° ì‹œê°„ì„ í•œêµ­ ì‹œê°„ìœ¼ë¡œ ë³€í™˜"""
    et_tz = pytz.timezone('US/Eastern')
    kst_tz = pytz.timezone('Asia/Seoul')
    now_et = datetime.now(et_tz)
    
    # ì˜¤ëŠ˜ ë‚ ì§œì˜ ê°œì¥ ì‹œê°„ (9:30 AM ET)
    market_open_et = et_tz.localize(datetime.combine(now_et.date(), time(9, 30)))
    market_open_kst = market_open_et.astimezone(kst_tz)
    
    # ì˜¤ëŠ˜ ë‚ ì§œì˜ ë§ˆê° ì‹œê°„ (4:00 PM ET) - KSTë¡œ ë³€í™˜í•˜ë©´ ë‹¤ìŒë‚ ì´ ë¨
    market_close_et = et_tz.localize(datetime.combine(now_et.date(), time(16, 0)))
    market_close_kst = market_close_et.astimezone(kst_tz)
    
    # ë§ˆê° ì‹œê°„ì´ ë‹¤ìŒë‚ ì´ ë˜ë„ë¡ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
    # (ê°œì¥ ì‹œê°„ì´ ë§ˆê° ì‹œê°„ë³´ë‹¤ í¬ë©´ ë§ˆê° ì‹œê°„ì„ ë‹¤ìŒë‚ ë¡œ ì„¤ì •)
    if market_open_kst.date() == market_close_kst.date():
        # ë§ˆê° ì‹œê°„ì´ ê°™ì€ ë‚ ì´ë©´ ë‹¤ìŒë‚ ë¡œ ì¡°ì •
        market_close_kst = market_close_kst + timedelta(days=1)
    
    return market_open_kst, market_close_kst

def is_us_market_open():
    """
    í˜„ì¬ ì‹œê°„ì´ ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ ê°œì¥ ì‹œê°„ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. (í•œêµ­ ì‹œê°„ ê¸°ì¤€)
    ì •ê·œ ì¥: 9:30 AM - 4:00 PM ET (23:30 - 06:00 KST, ì—¬ë¦„ / 22:30 - 05:00 KST, ê²¨ìš¸)
    """
    try:
        now_kst = datetime.now(pytz.timezone('Asia/Seoul'))
        market_open_kst, market_close_kst = get_market_open_close_kst()

        # ì£¼ë§ ì œì™¸ (í† , ì¼)
        if now_kst.weekday() >= 5:
            return False

        # ìì • ë„˜ì–´ê°€ëŠ” ê²½ìš° ì²˜ë¦¬ (ê°œì¥: ì˜¤ëŠ˜ 23:30, ë§ˆê°: ë‚´ì¼ 06:00)
        # í˜„ì¬ ì‹œê°„ì´ ê°œì¥ ì‹œê°„ ì´í›„ì´ê±°ë‚˜ ë§ˆê° ì‹œê°„ ì´ì „ì´ë©´ ì‹œì¥ ê°œì¥
        if market_open_kst.date() < market_close_kst.date():
            # ìì •ì„ ë„˜ê¸°ëŠ” ê²½ìš°
            return now_kst >= market_open_kst or now_kst < market_close_kst
        else:
            # ê°™ì€ ë‚ ì¸ ê²½ìš° (ì¼ë°˜ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•Šì§€ë§Œ ì•ˆì „ì¥ì¹˜)
            return market_open_kst <= now_kst < market_close_kst
    except Exception as e:
        logger.warning(f"ì‹œì¥ ê°œì¥ ì—¬ë¶€ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

default_args = {
    'owner': 'investment_assistant',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 16),
    'retries': None,
    'retry_delay': timedelta(minutes=2)
}

@dag(
    dag_id='cache_sp500_to_redis',
    default_args=default_args,
    description='SP500 ë°ì´í„° Redis ìºì‹± (ë³€í™”ìœ¨ ê³„ì‚° í¬í•¨)',
    schedule_interval='*/10 * * * *',  # ë§¤ 10ë¶„ë§ˆë‹¤ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,
    tags=['sp500', 'redis', 'caching']
)
def sp500_caching_dag():
    
    @task
    def fetch_sp500_current_data():
        """DBì—ì„œ SP500 í˜„ì¬ê°€ + íšŒì‚¬ëª… + ê±°ë˜ëŸ‰ ì¡°íšŒ"""
        market_open = is_us_market_open()
        
        # ì‹œì¥ ë§ˆê° ì¤‘: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ â†’ calculate_and_cacheì—ì„œ ê¸°ì¡´ ë°ì´í„° ìœ ì§€
        if not market_open:
            logger.info("ğŸ”’ ì‹œì¥ ë§ˆê° ì¤‘ - ê¸°ì¡´ Redis ë°ì´í„° ìœ ì§€ ëª¨ë“œ")
            return []  # Skip ëŒ€ì‹  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ calculate_and_cacheë¡œ ì „ë‹¬
        
        logger.info("ğŸ“Š SP500 í˜„ì¬ ë°ì´í„° ì¡°íšŒ ì‹œì‘ (ì‹œì¥ ê°œì¥ ì¤‘ - 10ë¶„ë§ˆë‹¤)")
        
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # ê° ì¢…ëª©ë³„ ìµœì‹  ê±°ë˜ ë°ì´í„° ì¡°íšŒ
        query = """
        WITH latest_trades AS (
            SELECT DISTINCT ON (symbol)
                symbol,
                price,
                volume,
                created_at
            FROM sp500_websocket_trades
            ORDER BY symbol, created_at DESC
        )
        SELECT 
            c.symbol,
            c.company_name,
            lt.price as current_price,
            lt.volume,
            lt.created_at as last_updated
        FROM sp500_companies c
        INNER JOIN latest_trades lt ON c.symbol = lt.symbol
        ORDER BY c.symbol
        """
        
        records = pg_hook.get_records(query)
        
        result = []
        for r in records:
            result.append({
                'symbol': r[0],
                'company_name': r[1],
                'current_price': float(r[2]) if r[2] else 0,
                'volume': int(r[3]) if r[3] else 0,
                'last_updated': r[4].isoformat() if r[4] else None
            })
        
        logger.info(f"âœ… {len(result)}ê°œ ì¢…ëª© í˜„ì¬ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ")
        return result
    
    @task
    def fetch_previous_close(current_data):
        """ì „ì¼ ì¢…ê°€ ì¡°íšŒ"""
        # ì‹œì¥ì´ ë‹«í˜€ìˆìœ¼ë©´ skip
        if not current_data:
            logger.info("ğŸ”’ ì‹œì¥ì´ ë‹«í˜€ìˆì–´ ì „ì¼ ì¢…ê°€ ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}
        
        logger.info("ğŸ“ˆ ì „ì¼ ì¢…ê°€ ì¡°íšŒ ì‹œì‘")
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # ì „ì¼ ì¢…ê°€ ì¡°íšŒ (ë¯¸êµ­ ì‹œê°„ ê¸°ì¤€)
        query = """
        WITH ranked_prices AS (
            SELECT 
                symbol, 
                price,
                created_at,
                ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY created_at DESC) as rn
            FROM sp500_websocket_trades
            WHERE created_at >= NOW() - INTERVAL '5 days'
                AND (created_at AT TIME ZONE 'UTC' AT TIME ZONE 'America/New_York')::date < 
                    (NOW() AT TIME ZONE 'America/New_York')::date
        )
        SELECT symbol, price
        FROM ranked_prices
        WHERE rn = 1
        """
        
        records = pg_hook.get_records(query)
        
        # {symbol: previous_close_price} í˜•íƒœë¡œ ë³€í™˜
        result = {}
        for r in records:
            result[r[0]] = float(r[1]) if r[1] else 0
        
        logger.info(f"âœ… {len(result)}ê°œ ì¢…ëª© ì „ì¼ ì¢…ê°€ ì¡°íšŒ ì™„ë£Œ")
        return result
    
    @task
    def fetch_24h_volume(current_data):
        """24ì‹œê°„ ëˆ„ì  ê±°ë˜ëŸ‰ ì¡°íšŒ"""
        # ì‹œì¥ì´ ë‹«í˜€ìˆìœ¼ë©´ skip
        if not current_data:
            logger.info("ğŸ”’ ì‹œì¥ì´ ë‹«í˜€ìˆì–´ 24ì‹œê°„ ê±°ë˜ëŸ‰ ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}
        
        logger.info("ğŸ“Š 24ì‹œê°„ ê±°ë˜ëŸ‰ ì¡°íšŒ ì‹œì‘")
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # ê³¼ê±° 24ì‹œê°„ ë™ì•ˆì˜ ê±°ë˜ëŸ‰ í•©ì‚°
        query = """
        SELECT 
            symbol,
            SUM(volume) as volume_24h
        FROM sp500_websocket_trades
        WHERE created_at >= NOW() - INTERVAL '24 hours'
        GROUP BY symbol
        """
        
        records = pg_hook.get_records(query)
        
        # {symbol: volume_24h} í˜•íƒœë¡œ ë³€í™˜
        result = {}
        for r in records:
            result[r[0]] = int(r[1]) if r[1] else 0
        
        logger.info(f"âœ… {len(result)}ê°œ ì¢…ëª© 24ì‹œê°„ ê±°ë˜ëŸ‰ ì¡°íšŒ ì™„ë£Œ")
        return result
    
    @task
    def calculate_and_cache(current_data, previous_close_map, volume_24h_map):
        """ë³€í™”ìœ¨ ê³„ì‚° + 24h ê±°ë˜ëŸ‰ ì¶”ê°€ í›„ Redisì— ìºì‹±"""
        # Redis ì—°ê²°
        try:
            # â­ Kubernetes Service í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
            redis_host = os.getenv('REDIS_SERVICE_HOST', os.getenv('REDIS_HOST', 'redis-master'))
            redis_port = int(os.getenv('REDIS_SERVICE_PORT', os.getenv('REDIS_PORT_6379_TCP_PORT', 6379)))
            redis_password = os.getenv('REDIS_PASSWORD', None)
            redis_db = int(os.getenv('REDIS_DB', 0))
            
            redis_client = redis.Redis(
                host=redis_host,
                port=redis_port,
                db=redis_db,
                password=redis_password,
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5
            )
            redis_client.ping()
            logger.info(f"âœ… Redis ì—°ê²° ì„±ê³µ: {redis_host}:{redis_port}")
        except Exception as e:
            logger.error(f"âŒ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
        
        # Redis Hash Key
        redis_key = "sp500_market_data"
        
        # ì‹œì¥ì´ ë‹«í˜€ìˆìœ¼ë©´: ê¸°ì¡´ Redis ë°ì´í„°ì—ì„œ ë³€í™”ìœ¨/ë³€í™”ì•¡ ìœ ì§€
        if not current_data:
            logger.info("ğŸ”’ ì‹œì¥ ë§ˆê° ì¤‘ - ê¸°ì¡´ Redis ë°ì´í„°ì˜ ë³€í™”ìœ¨/ë³€í™”ì•¡ ìœ ì§€")
            
            # ê¸°ì¡´ Redis ë°ì´í„° ì¡°íšŒ
            existing_data = redis_client.hgetall(redis_key)
            
            if not existing_data:
                logger.info("âš ï¸ Redisì— ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                redis_client.close()
                return []
            
            # ê¸°ì¡´ ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ì—¬ ë³€í™”ìœ¨/ë³€í™”ì•¡ ìœ ì§€
            pipeline = redis_client.pipeline()
            preserved_count = 0
            enriched_data = []
            
            for symbol, data_str in existing_data.items():
                try:
                    existing_json = json.loads(data_str)
                    
                    # ê¸°ì¡´ ë³€í™”ìœ¨/ë³€í™”ì•¡ ìœ ì§€ (í˜„ì¬ê°€ë§Œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•˜ë©´ ì—…ë°ì´íŠ¸, ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ìœ ì§€)
                    redis_data = {
                        'symbol': existing_json.get('symbol', symbol),
                        'company_name': existing_json.get('company_name', ''),
                        'current_price': existing_json.get('current_price', 0),
                        'change_amount': existing_json.get('change_amount', 0),  # ê¸°ì¡´ ê°’ ìœ ì§€
                        'change_percentage': existing_json.get('change_percentage', 0),  # ê¸°ì¡´ ê°’ ìœ ì§€
                        'volume': existing_json.get('volume', 0),
                        'volume_24h': existing_json.get('volume_24h', 0),
                        'last_updated': existing_json.get('last_updated', '')
                    }
                    
                    # Hashì— ì €ì¥ (ë³€í™”ìœ¨/ë³€í™”ì•¡ ìœ ì§€)
                    pipeline.hset(redis_key, symbol, json.dumps(redis_data))
                    preserved_count += 1
                    enriched_data.append(redis_data)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ {symbol} ê¸°ì¡´ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            # TTL ì„¤ì • (7ì¼)
            pipeline.expire(redis_key, 604800)
            
            try:
                pipeline.execute()
                logger.info(f"âœ… {preserved_count}ê°œ ì¢…ëª© ë³€í™”ìœ¨/ë³€í™”ì•¡ ìœ ì§€ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ Redis ì €ì¥ ì‹¤íŒ¨: {e}")
                redis_client.close()
                raise
            
            redis_client.close()
            return enriched_data
        
        # ì‹œì¥ ê°œì¥ ì¤‘: ìƒˆë¡œìš´ ë³€í™”ìœ¨/ë³€í™”ì•¡ ê³„ì‚°
        logger.info(f"ğŸ’¾ Redis ìºì‹± ì‹œì‘ ({len(current_data)}ê°œ ì¢…ëª©)")
        
        # Pipeline ì‚¬ìš©í•˜ì—¬ ì¼ê´„ ì €ì¥
        pipeline = redis_client.pipeline()
        cached_count = 0
        enriched_data = []  # DB ì €ì¥ìš© ë°ì´í„°
        
        for stock in current_data:
            symbol = stock['symbol']
            current_price = stock['current_price']
            previous_close = previous_close_map.get(symbol, 0)
            volume_24h = volume_24h_map.get(symbol, 0)
            
            # ë³€í™”ìœ¨ ê³„ì‚°
            if previous_close and previous_close > 0:
                change_amount = current_price - previous_close
                change_percentage = (change_amount / previous_close) * 100
            else:
                # ì „ì¼ ì¢…ê°€ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ê¸°ì¡´ Redis ë°ì´í„°ì—ì„œ ë³€í™”ìœ¨/ë³€í™”ì•¡ ê°€ì ¸ì˜¤ê¸° ì‹œë„
                try:
                    existing_data_str = redis_client.hget(redis_key, symbol)
                    if existing_data_str:
                        existing_json = json.loads(existing_data_str)
                        change_amount = existing_json.get('change_amount', 0)
                        change_percentage = existing_json.get('change_percentage', 0)
                        logger.debug(f"ğŸ“Œ {symbol}: ì „ì¼ ì¢…ê°€ ì—†ìŒ, ê¸°ì¡´ ë³€í™”ìœ¨ ìœ ì§€ ({change_percentage}%)")
                    else:
                        change_amount = 0
                        change_percentage = 0
                except Exception as e:
                    logger.debug(f"âš ï¸ {symbol}: ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨, ë³€í™”ìœ¨ 0ìœ¼ë¡œ ì„¤ì •: {e}")
                    change_amount = 0
                    change_percentage = 0
            
            # Redis ì €ì¥ìš© ë°ì´í„° (WebSocket ì‘ë‹µ í¬ë§· + 24h volume)
            redis_data = {
                'symbol': symbol,
                'company_name': stock['company_name'],
                'current_price': current_price,
                'change_amount': round(change_amount, 2),
                'change_percentage': round(change_percentage, 2),
                'volume': stock['volume'],
                'volume_24h': volume_24h,  # ğŸ†• 24ì‹œê°„ ê±°ë˜ëŸ‰
                'last_updated': stock['last_updated']
            }
            
            # Hashì— ì €ì¥
            pipeline.hset(redis_key, symbol, json.dumps(redis_data))
            cached_count += 1
            
            # DB ì €ì¥ìš©ìœ¼ë¡œë„ ì¶”ê°€
            enriched_data.append(redis_data)
        
        # TTL ì„¤ì • (7ì¼)
        pipeline.expire(redis_key, 604800)
        
        # ì¼ê´„ ì‹¤í–‰
        try:
            pipeline.execute()
            logger.info(f"âœ… {cached_count}ê°œ ì¢…ëª© Redis ìºì‹± ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ Redis ì €ì¥ ì‹¤íŒ¨: {e}")
            redis_client.close()
            raise
        
        # Pub/Sub ì‹ í˜¸ ë°œí–‰ (WebSocketì— ì—…ë°ì´íŠ¸ ì•Œë¦¼)
        try:
            message = json.dumps({
                'message': 'SP500 market data updated',
                'count': cached_count,
                'timestamp': datetime.utcnow().isoformat()
            })
            redis_client.publish('sp500_market_updates', message)
            logger.info("ğŸ“¢ SP500 market data updated signal published (sp500_market_updates)")
        except Exception as e:
            logger.warning(f"âš ï¸ Pub/Sub ë°œí–‰ ì‹¤íŒ¨: {e}")
        
        redis_client.close()
        
        logger.info(f"âœ… Redis ìºì‹± ì™„ë£Œ - {len(enriched_data)}ê°œ ë°ì´í„° ì¤€ë¹„ë¨")
        return enriched_data  # DB ì €ì¥ìš©ìœ¼ë¡œ ë°˜í™˜
    
    @task
    def save_to_database(enriched_data):
        """ìŠ¤ëƒ…ìƒ· ë°ì´í„°ë¥¼ DBì— ì €ì¥ (íˆìŠ¤í† ë¦¬ ë¶„ì„ìš©)"""
        if not enriched_data:
            logger.info("ğŸ”’ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ’¾ DB ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹œì‘ ({len(enriched_data)}ê°œ ì¢…ëª©)")
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        snapshot_time = datetime.now(pytz.timezone('Asia/Seoul'))
        saved_count = 0
        
        for data in enriched_data:
            try:
                # INSERT ì¿¼ë¦¬ (ì¤‘ë³µ ì‹œ ë¬´ì‹œ)
                insert_query = """
                INSERT INTO sp500_market_snapshots 
                (symbol, company_name, current_price, change_amount, change_percentage, 
                 volume, volume_24h, snapshot_time)
                VALUES (%(symbol)s, %(company_name)s, %(current_price)s, %(change_amount)s, 
                        %(change_percentage)s, %(volume)s, %(volume_24h)s, %(snapshot_time)s)
                ON CONFLICT (symbol, snapshot_time) DO NOTHING
                """
                
                params = {
                    'symbol': data['symbol'],
                    'company_name': data['company_name'],
                    'current_price': data['current_price'],
                    'change_amount': data['change_amount'],
                    'change_percentage': data['change_percentage'],
                    'volume': data['volume'],
                    'volume_24h': data['volume_24h'],
                    'snapshot_time': snapshot_time
                }
                
                pg_hook.run(insert_query, parameters=params)
                saved_count += 1
                
            except Exception as e:
                logger.warning(f"âš ï¸ {data['symbol']} ì €ì¥ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"âœ… DB ìŠ¤ëƒ…ìƒ· ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì¢…ëª©")
    
    # Task ì‹¤í–‰ íë¦„
    current_data = fetch_sp500_current_data()
    previous_close = fetch_previous_close(current_data)
    volume_24h = fetch_24h_volume(current_data)
    enriched_data = calculate_and_cache(current_data, previous_close, volume_24h)
    save_to_database(enriched_data)

# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
dag_instance = sp500_caching_dag()