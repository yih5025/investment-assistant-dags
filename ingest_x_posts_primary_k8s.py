from datetime import datetime, timedelta
import requests
import json
import os
import time

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_x_posts.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': 1,  # Rate Limit ì—ëŸ¬ ì‹œ 1íšŒ ìž¬ì‹œë„
    'retry_delay': timedelta(minutes=20),  # 20ë¶„ í›„ ìž¬ì‹œë„ (15ë¶„ + ì—¬ìœ ë¶„)
}

# ===== DB ê¸°ë°˜ ê³„ì • ìŠ¤ì¼€ì¤„ ì„¤ì • (user_id ì œê±°) =====
PRIMARY_ACCOUNT_SCHEDULE = {
    # ì´ˆê³ ì˜í–¥ ê³„ì • (ë§¤ì¼ or ê²©ì¼)
    'elonmusk': {
        'frequency': 'daily',          # ë§¤ì¼ (30íšŒ/ì›”)
        'max_results': 50,             # Free Tier ì œí•œ ê³ ë ¤
        'priority': 1
    },
    'RayDalio': {
        'frequency': 'every_2_days',   # 2ì¼ë§ˆë‹¤ (15íšŒ/ì›”)
        'max_results': 50,
        'priority': 1
    },
    'jimcramer': {
        'frequency': 'every_2_days',   # 2ì¼ë§ˆë‹¤ (15íšŒ/ì›”)
        'max_results': 50,
        'priority': 1
    },
    
    # ê³ ì˜í–¥ ê³„ì • (3ì¼ë§ˆë‹¤)
    'tim_cook': {
        'frequency': 'every_3_days',   # 3ì¼ë§ˆë‹¤ (10íšŒ/ì›”)
        'max_results': 50,
        'priority': 2
    },
    'satyanadella': {
        'frequency': 'every_3_days',   # 3ì¼ë§ˆë‹¤ (10íšŒ/ì›”)
        'max_results': 50,
        'priority': 2
    },
    
    # ì¤‘ê°„ì˜í–¥ ê³„ì • (ì£¼ 2íšŒ)
    'sundarpichai': {
        'frequency': 'twice_weekly',   # ì£¼ 2íšŒ (8íšŒ/ì›”)
        'max_results': 50,
        'priority': 3,
        'weekly_days': [1, 4]  # í™”, ê¸ˆ
    },
    'SecYellen': {
        'frequency': 'twice_weekly',   # ì£¼ 2íšŒ (8íšŒ/ì›”)
        'max_results': 50,
        'priority': 3,
        'weekly_days': [0, 3]  # ì›”, ëª©
    },
    
    # ì €ì˜í–¥ ê³„ì • (ì£¼ 1íšŒ)
    'VitalikButerin': {
        'frequency': 'weekly',         # ì£¼ 1íšŒ (4íšŒ/ì›”)
        'max_results': 50,
        'priority': 4,
        'weekly_day': 6  # ì¼ìš”ì¼
    }
}

def get_user_id_from_db(username):
    """DBì—ì„œ usernameìœ¼ë¡œ user_id ì¡°íšŒ"""
    try:
        hook = PostgresHook(postgres_conn_id='postgres_default')
        result = hook.get_first(
            "SELECT user_id FROM x_user_profiles WHERE username = %s",
            parameters=[username]
        )
        
        if result:
            user_id = result[0]
            print(f"âœ… DB ì¡°íšŒ ì„±ê³µ: {username} â†’ {user_id}")
            return user_id
        else:
            print(f"âŒ DBì—ì„œ {username}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
            
    except Exception as e:
        print(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {username} - {e}")
        return None

def should_run_account_today_primary(username):
    """ì²« ë²ˆì§¸ í† í° ê³„ì •ë“¤ì˜ ì˜¤ëŠ˜ ì‹¤í–‰ ì—¬ë¶€ íŒë‹¨"""
    config = PRIMARY_ACCOUNT_SCHEDULE.get(username)
    if not config:
        return False
    
    frequency = config['frequency']
    today = datetime.now()
    day_of_year = today.timetuple().tm_yday  # 1-365
    day_of_week = today.weekday()  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
    
    if frequency == 'daily':
        return True
    elif frequency == 'every_2_days':
        # ê³„ì •ë³„ë¡œ ë‹¤ë¥¸ ì‹œìž‘ì  (ì¶©ëŒ ë°©ì§€)
        offset = {'RayDalio': 0, 'jimcramer': 1}.get(username, 0)
        return (day_of_year + offset) % 2 == 0
    elif frequency == 'every_3_days':
        # ê³„ì •ë³„ë¡œ ë‹¤ë¥¸ ì‹œìž‘ì 
        offset = {'tim_cook': 0, 'satyanadella': 1}.get(username, 0)
        return (day_of_year + offset) % 3 == 0
    elif frequency == 'twice_weekly':
        # ê³„ì •ë³„ë¡œ ì •í™•í•œ ìš”ì¼ í• ë‹¹
        assigned_days = config.get('weekly_days', [0, 3])
        return day_of_week in assigned_days
    elif frequency == 'weekly':
        # ê³„ì •ë³„ë¡œ ì •í™•í•œ ìš”ì¼
        assigned_day = config.get('weekly_day', 6)
        return day_of_week == assigned_day
    
    return False

def get_todays_primary_accounts():
    """ì˜¤ëŠ˜ ìˆ˜ì§‘í•  ì²« ë²ˆì§¸ í† í° ê³„ì •ë“¤ ë°˜í™˜"""
    todays_accounts = []
    
    for username in PRIMARY_ACCOUNT_SCHEDULE.keys():
        if should_run_account_today_primary(username):
            todays_accounts.append(username)
    
    # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
    todays_accounts.sort(key=lambda x: PRIMARY_ACCOUNT_SCHEDULE[x]['priority'])
    
    return todays_accounts

def call_x_api_with_rate_limit(username, user_id, max_results=10):
    """start_time ì—†ì´ ìµœì‹  íŠ¸ìœ—ë§Œ ê°€ì ¸ì˜¤ê¸°"""
    try:
        bearer_token = Variable.get('X_API_BEARER_TOKEN_4')
        
        url = f"https://api.twitter.com/2/users/{user_id}/tweets"
        
        # âœ… start_time ì œê±° - ìµœì‹  íŠ¸ìœ—ë“¤ë§Œ ê°€ì ¸ì˜¤ê¸°
        params = {
            "max_results": min(max_results, 10),  # Free TierëŠ” 10ê°œ ì œí•œ
            "tweet.fields": "created_at,text,public_metrics,context_annotations,entities,lang,edit_history_tweet_ids",
            "expansions": "author_id",
            "user.fields": "name,username,verified,public_metrics"
        }
        
        headers = {
            "Authorization": f"Bearer {bearer_token}",
            "User-Agent": "InvestmentAssistant-Primary/2.0"
        }
        
        print(f"ðŸ” API í˜¸ì¶œ ì¤‘: {username} (user_id: {user_id}) - ìµœì‹  íŠ¸ìœ—")
        
        response = requests.get(url, headers=headers, params=params, timeout=30)
        
        if response.status_code == 400:
            print(f"âŒ 400 Bad Request:")
            print(f"   Response: {response.text}")
        
        response.raise_for_status()
        data = response.json()
        
        print(f"âœ… API í˜¸ì¶œ ì„±ê³µ: {username}")
        return data
        
    except Exception as e:
        print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {username} - {e}")
        raise

def process_tweet_data_primary(tweet, user_info, source_account):
    """íŠ¸ìœ— ë°ì´í„° ì²˜ë¦¬ (Primary Tokenìš©)"""
    
    # ê¸°ë³¸ íŠ¸ìœ— ì •ë³´
    processed_data = {
        'tweet_id': tweet['id'],
        'author_id': tweet['author_id'],
        'text': tweet['text'],
        'created_at': tweet['created_at'].replace('Z', '+00:00'),
        'lang': tweet.get('lang', 'en'),
        'source_account': source_account,
        'account_category': 'core_investors',
        'collection_source': 'primary_token',
    }
    
    # ì°¸ì—¬ë„ ì§€í‘œ
    metrics = tweet.get('public_metrics', {})
    processed_data.update({
        'retweet_count': metrics.get('retweet_count', 0),
        'reply_count': metrics.get('reply_count', 0),
        'like_count': metrics.get('like_count', 0),
        'quote_count': metrics.get('quote_count', 0),
        'bookmark_count': metrics.get('bookmark_count', 0),
        'impression_count': metrics.get('impression_count', 0),
    })
    
    # ì—”í‹°í‹° ì •ë³´ (JSONìœ¼ë¡œ ì €ìž¥)
    entities = tweet.get('entities', {})
    processed_data.update({
        'hashtags': json.dumps(entities.get('hashtags', [])) if entities.get('hashtags') else None,
        'mentions': json.dumps(entities.get('mentions', [])) if entities.get('mentions') else None,
        'urls': json.dumps(entities.get('urls', [])) if entities.get('urls') else None,
        'cashtags': json.dumps(entities.get('cashtags', [])) if entities.get('cashtags') else None,
        'annotations': json.dumps(entities.get('annotations', [])) if entities.get('annotations') else None,
    })
    
    # ì»¨í…ìŠ¤íŠ¸ ì£¼ì„
    context_annotations = tweet.get('context_annotations', [])
    processed_data['context_annotations'] = json.dumps(context_annotations) if context_annotations else None
    
    # ì‚¬ìš©ìž ì •ë³´
    if user_info:
        user_metrics = user_info.get('public_metrics', {})
        processed_data.update({
            'username': user_info.get('username', source_account),
            'display_name': user_info.get('name', ''),
            'user_verified': user_info.get('verified', False),
            'user_followers_count': user_metrics.get('followers_count', 0),
            'user_following_count': user_metrics.get('following_count', 0),
            'user_tweet_count': user_metrics.get('tweet_count', 0),
        })
    else:
        processed_data.update({
            'username': source_account,
            'display_name': '',
            'user_verified': False,
            'user_followers_count': 0,
            'user_following_count': 0,
            'user_tweet_count': 0,
        })
    
    # íŽ¸ì§‘ ì´ë ¥
    edit_history = tweet.get('edit_history_tweet_ids', [])
    processed_data['edit_history_tweet_ids'] = json.dumps(edit_history) if edit_history else None
    
    return processed_data

def fetch_primary_tweets_with_delay(**context):
    """ë°©ì•ˆ 2: 15ë¶„ ë”œë ˆì´ë¥¼ ë‘ê³  ìˆœì°¨ì ìœ¼ë¡œ ê³„ì • ìˆ˜ì§‘"""
    
    # ì˜¤ëŠ˜ ìˆ˜ì§‘í•  ê³„ì •ë“¤ ê²°ì •
    todays_accounts = get_todays_primary_accounts()
    
    if not todays_accounts:
        print("ðŸ“… [PRIMARY] ì˜¤ëŠ˜ì€ ìˆ˜ì§‘í•  ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤")
        context['ti'].xcom_push(key='collected_tweets', value=[])
        context['ti'].xcom_push(key='api_calls_made', value=0)
        return 0
    
    print(f"ðŸŽ¯ [PRIMARY TOKEN] ì˜¤ëŠ˜ ìˆ˜ì§‘ ëŒ€ìƒ: {len(todays_accounts)}ê°œ ê³„ì •")
    for account in todays_accounts:
        config = PRIMARY_ACCOUNT_SCHEDULE[account]
        print(f"   - {account}: {config['frequency']} (ìš°ì„ ìˆœìœ„ {config['priority']})")
    
    # Rate Limit ì²´í¬ (í•˜ë£¨ 17íšŒ ì œí•œ)
    if len(todays_accounts) > 17:
        print(f"âš ï¸ ê²½ê³ : ì˜¤ëŠ˜ ìˆ˜ì§‘ ê³„ì •({len(todays_accounts)}ê°œ)ì´ ì¼ì¼ ì œí•œ(17íšŒ)ì„ ì´ˆê³¼í•©ë‹ˆë‹¤")
        print(f"   ì²˜ìŒ 17ê°œ ê³„ì •ë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤")
        todays_accounts = todays_accounts[:17]
    
    # ì˜ˆìƒ ì†Œìš” ì‹œê°„ ê³„ì‚°
    estimated_time = (len(todays_accounts) - 1) * 15  # 15ë¶„ ê°„ê²©
    print(f"â° ì˜ˆìƒ ì†Œìš” ì‹œê°„: {estimated_time}ë¶„ (15ë¶„ ê°„ê²© Ã— {len(todays_accounts)-1}íšŒ ëŒ€ê¸°)")
    
    # ê° ê³„ì •ë³„ íŠ¸ìœ— ìˆ˜ì§‘ (15ë¶„ ë”œë ˆì´ í¬í•¨)
    all_tweets = []
    total_api_calls = 0
    successful_accounts = []
    failed_accounts = []
    
    for i, username in enumerate(todays_accounts):
        try:
            # ë‘ ë²ˆì§¸ ê³„ì •ë¶€í„° 15ë¶„ ëŒ€ê¸° (Rate Limit ì¤€ìˆ˜)
            if i > 0:
                wait_minutes = 15
                print(f"\nâ° [RATE LIMIT] {wait_minutes}ë¶„ ëŒ€ê¸° ì¤‘... (í˜„ìž¬ {i+1}/{len(todays_accounts)})")
                print(f"   ë‹¤ìŒ ê³„ì •: {username}")
                
                # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” 15ë¶„, í…ŒìŠ¤íŠ¸ì—ì„œëŠ” 1ë¶„ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥
                time.sleep(wait_minutes * 60)  # 15ë¶„ = 900ì´ˆ
                
                print(f"âœ… ëŒ€ê¸° ì™„ë£Œ! {username} ìˆ˜ì§‘ ì‹œìž‘")
            
            # DBì—ì„œ user_id ì¡°íšŒ
            user_id = get_user_id_from_db(username)
            if not user_id:
                print(f"âŒ {username}: DBì—ì„œ user_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤")
                failed_accounts.append(f"{username} (user_id ì—†ìŒ)")
                continue
            
            config = PRIMARY_ACCOUNT_SCHEDULE[username]
            max_results = config['max_results']
            
            print(f"\nðŸ” [{i+1}/{len(todays_accounts)}] {username} íŠ¸ìœ— ìˆ˜ì§‘ ì¤‘...")
            print(f"   User ID: {user_id}")
            print(f"   ìµœëŒ€ ê²°ê³¼: {max_results}ê°œ")
            
            # API í˜¸ì¶œ
            api_response = call_x_api_with_rate_limit(username, user_id, max_results)
            total_api_calls += 1
            
            if 'data' not in api_response or not api_response['data']:
                print(f"âš ï¸ {username}: ìµœê·¼ 24ì‹œê°„ ë‚´ íŠ¸ìœ— ì—†ìŒ")
                successful_accounts.append(f"{username} (íŠ¸ìœ— ì—†ìŒ)")
                continue
            
            # ì‚¬ìš©ìž ì •ë³´ ì¶”ì¶œ
            user_info = {}
            if 'includes' in api_response and 'users' in api_response['includes']:
                user_info = api_response['includes']['users'][0]
            
            # íŠ¸ìœ— ë°ì´í„° ì²˜ë¦¬
            account_tweets = []
            for tweet in api_response['data']:
                processed_tweet = process_tweet_data_primary(tweet, user_info, username)
                account_tweets.append(processed_tweet)
            
            all_tweets.extend(account_tweets)
            successful_accounts.append(f"{username} ({len(account_tweets)}ê°œ)")
            print(f"âœ… {username}: {len(account_tweets)}ê°œ íŠ¸ìœ— ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ {username} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            failed_accounts.append(f"{username} ({str(e)[:50]})")
            total_api_calls += 1  # ì‹¤íŒ¨í•´ë„ API í˜¸ì¶œì€ ì°¨ê°
            continue
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\nðŸ“Š [PRIMARY TOKEN] ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"   ðŸ“± ì´ íŠ¸ìœ—: {len(all_tweets)}ê°œ")
    print(f"   ðŸ”‘ API í˜¸ì¶œ: {total_api_calls}íšŒ / 17íšŒ (ì¼ì¼ ì œí•œ)")
    print(f"   âœ… ì„±ê³µ: {len(successful_accounts)}ê°œ ê³„ì •")
    if successful_accounts:
        for account in successful_accounts:
            print(f"      - {account}")
    
    if failed_accounts:
        print(f"   âŒ ì‹¤íŒ¨: {len(failed_accounts)}ê°œ ê³„ì •")
        for account in failed_accounts:
            print(f"      - {account}")
    
    # XComì— ê²°ê³¼ ì €ìž¥
    context['ti'].xcom_push(key='collected_tweets', value=all_tweets)
    context['ti'].xcom_push(key='api_calls_made', value=total_api_calls)
    context['ti'].xcom_push(key='successful_accounts', value=successful_accounts)
    context['ti'].xcom_push(key='failed_accounts', value=failed_accounts)
    
    return len(all_tweets)

def store_primary_tweets_to_db(**context):
    """ìˆ˜ì§‘ëœ í•µì‹¬ ê³„ì • íŠ¸ìœ—ì„ DBì— ì €ìž¥"""
    
    # XComì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    all_tweets = context['ti'].xcom_pull(key='collected_tweets') or []
    api_calls = context['ti'].xcom_pull(key='api_calls_made') or 0
    successful_accounts = context['ti'].xcom_pull(key='successful_accounts') or []
    failed_accounts = context['ti'].xcom_pull(key='failed_accounts') or []
    
    if not all_tweets:
        print("â„¹ï¸ ì €ìž¥í•  íŠ¸ìœ—ì´ ì—†ìŠµë‹ˆë‹¤")
        return 0
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    success_count = 0
    error_count = 0
    
    print(f"ðŸ’¾ [PRIMARY] DB ì €ìž¥ ì‹œìž‘: {len(all_tweets)}ê°œ íŠ¸ìœ—")
    
    for tweet_data in all_tweets:
        try:
            hook.run(UPSERT_SQL, parameters=tweet_data)
            success_count += 1
            
            # ì§„í–‰ë¥  í‘œì‹œ (50ê°œë§ˆë‹¤)
            if success_count % 50 == 0:
                print(f"ðŸ“Š ì €ìž¥ ì§„í–‰ë¥ : {success_count}/{len(all_tweets)}")
                
        except Exception as e:
            print(f"âŒ íŠ¸ìœ— ì €ìž¥ ì‹¤íŒ¨: {tweet_data.get('tweet_id', 'Unknown')} - {e}")
            error_count += 1
            continue
    
    print(f"âœ… [PRIMARY] ì €ìž¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
    
    # í†µê³„ ì¡°íšŒ ë° ìµœì¢… ë¦¬í¬íŠ¸
    try:
        # ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ primary í† í° íŠ¸ìœ—
        result = hook.get_first("""
            SELECT COUNT(*) FROM x_posts 
            WHERE collected_at >= NOW() - INTERVAL '1 day'
            AND collection_source = 'primary_token'
        """)
        primary_today = result[0] if result else 0
        
        # ì „ì²´ íŠ¸ìœ— ìˆ˜
        result = hook.get_first("SELECT COUNT(*) FROM x_posts")
        total_all = result[0] if result else 0
        
        print(f"\nðŸ“ˆ [ìµœì¢… ë¦¬í¬íŠ¸]")
        print(f"   ðŸ“Š ì˜¤ëŠ˜ Primary í† í° ìˆ˜ì§‘: {primary_today}ê°œ")
        print(f"   ðŸ“Š ì „ì²´ ì €ìž¥ëœ íŠ¸ìœ—: {total_all}ê°œ")
        print(f"   ðŸ”¥ ì˜¤ëŠ˜ API í˜¸ì¶œ: {api_calls}íšŒ / 17íšŒ")
        print(f"   âœ… ì„±ê³µ ê³„ì •: {len(successful_accounts)}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨ ê³„ì •: {len(failed_accounts)}ê°œ")
        
        # ë‚¨ì€ API í˜¸ì¶œ ìˆ˜
        remaining_calls = 17 - api_calls
        print(f"   ðŸ”‹ ë‚¨ì€ API í˜¸ì¶œ: {remaining_calls}íšŒ")
        
    except Exception as e:
        print(f"âš ï¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    return success_count

# DAG ì •ì˜
with DAG(
    dag_id='ingest_x_posts_primary_with_delay_k8s',
    default_args=default_args,
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì‹¤í–‰ (8ì‹œê°„ë§ˆë‹¤ â†’ 1ì¼ 1íšŒ)
    catchup=False,
    description='X API Rate Limit ì¤€ìˆ˜: 15ë¶„ ë”œë ˆì´ + DB ê¸°ë°˜ user_id ì¡°íšŒ (ë°©ì•ˆ 2)',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['x_api', 'twitter', 'rate_limit_safe', 'db_based', 'delay_strategy', 'k8s'],
) as dag:
    
    # í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_x_posts_table_primary',
        postgres_conn_id='postgres_default',
        sql='create_x_posts.sql',
    )
    
    # Rate Limit ì¤€ìˆ˜í•˜ì—¬ íŠ¸ìœ— ìˆ˜ì§‘
    fetch_tweets = PythonOperator(
        task_id='fetch_primary_tweets_with_delay',
        python_callable=fetch_primary_tweets_with_delay,
    )
    
    # DB ì €ìž¥
    store_tweets = PythonOperator(
        task_id='store_primary_tweets_to_db',
        python_callable=store_primary_tweets_to_db,
    )
    
    # Task ì˜ì¡´ì„±
    create_table >> fetch_tweets >> store_tweets