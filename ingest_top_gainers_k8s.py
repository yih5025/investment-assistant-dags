from datetime import datetime, timedelta
import os
import requests
from decimal import Decimal
import json

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# í‘œì¤€ ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_top_gainers.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': None,
    'retry_delay': timedelta(minutes=1),
}

def fetch_top_gainers_data(**context):
    """
    Alpha Vantage APIì—ì„œ Top Gainers/Losers/Most Active ë°ì´í„° ìˆ˜ì§‘
    """
    # API í‚¤ ê°€ì ¸ì˜¤ê¸°
    api_key = Variable.get('ALPHA_VANTAGE_API_KEY')
    if not api_key:
        raise ValueError("ðŸ”‘ ALPHA_VANTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # API ìš”ì²­
    url = "https://www.alphavantage.co/query"
    params = {
        'function': 'TOP_GAINERS_LOSERS',
        'apikey': api_key
    }
    
    print(f"ðŸš€ API ìš”ì²­ ì‹œìž‘: {url}")
    response = requests.get(url, params=params, timeout=60)
    response.raise_for_status()
    
    # API ì‘ë‹µ ê²€ì¦
    if 'Error Message' in response.text:
        raise ValueError(f"âŒ API ì˜¤ë¥˜: {response.text}")
    
    if 'Note' in response.text and 'API call frequency' in response.text:
        raise ValueError(f"âš ï¸ API í˜¸ì¶œ ì œí•œ: {response.text}")
    
    try:
        data = response.json()
    except json.JSONDecodeError as e:
        raise ValueError(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    # í•„ìˆ˜ í‚¤ í™•ì¸
    required_keys = ['top_gainers', 'top_losers', 'most_actively_traded', 'last_updated']
    for key in required_keys:
        if key not in data:
            raise ValueError(f"âŒ í•„ìˆ˜ í‚¤ ëˆ„ë½: {key}")
    
    # ë°ì´í„° ê°œìˆ˜ í™•ì¸
    gainers_count = len(data['top_gainers'])
    losers_count = len(data['top_losers'])
    active_count = len(data['most_actively_traded'])
    
    print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"   ðŸ“ˆ Top Gainers: {gainers_count}ê°œ")
    print(f"   ðŸ“‰ Top Losers: {losers_count}ê°œ") 
    print(f"   ðŸ”¥ Most Active: {active_count}ê°œ")
    print(f"   ðŸ“… Last Updated: {data['last_updated']}")
    
    # XComì— ì €ìž¥
    context['ti'].xcom_push(key='market_data', value=data)
    return {
        'gainers_count': gainers_count,
        'losers_count': losers_count,
        'active_count': active_count,
        'last_updated': data['last_updated']
    }

def process_and_store_data(**context):
    """
    ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ì—¬ PostgreSQLì— ì €ìž¥ (í˜¸ì¶œ íšŒì°¨ë³„ batch_id ë²„ì „)
    - batch_id: í˜¸ì¶œ íšŒì°¨ ì‹ë³„ìž (1, 2, 3, 4, ...)
    - top_gainers: 20ê°œ ì „ë¶€
    - top_losers: ìƒìœ„ 10ê°œ  
    - most_actively_traded: 20ê°œ ì „ë¶€
    """
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    data = context['ti'].xcom_pull(task_ids='fetch_top_gainers', key='market_data')
    
    if not data:
        raise ValueError("âŒ ì´ì „ íƒœìŠ¤í¬ì—ì„œ ë°ì´í„°ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
    
    # DB ì—°ê²°
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # ðŸ”‘ í˜„ìž¬ ìµœëŒ€ batch_id ì¡°íšŒí•´ì„œ +1 (í˜¸ì¶œ íšŒì°¨ ì¦ê°€)
    max_batch_result = hook.get_first("SELECT COALESCE(MAX(batch_id), 0) FROM top_gainers")
    current_batch_id = (max_batch_result[0] if max_batch_result else 0) + 1
    
    # API ì‘ë‹µì˜ íƒ€ìž„ìŠ¤íƒ¬í”„ë„ íŒŒì‹± (ì°¸ê³ ìš©)
    last_updated_str = data['last_updated']
    api_last_updated = datetime.strptime(
        last_updated_str.split(' US/Eastern')[0], 
        "%Y-%m-%d %H:%M:%S"
    )
    
    print(f"ðŸ†” ì´ë²ˆ í˜¸ì¶œ íšŒì°¨: {current_batch_id}")
    print(f"ðŸ“¡ API ì‘ë‹µ ì‹œê°„: {api_last_updated}")
    print(f"ðŸ”„ DAG ì‹¤í–‰ ì‹œê°„: {context['execution_date']}")
    
    # ì²˜ë¦¬í•  ë°ì´í„° ì •ì˜
    categories = [
        {
            'name': 'top_gainers',
            'data': data['top_gainers'][:20],  # 20ê°œ ì „ë¶€
            'limit': 20
        },
        {
            'name': 'top_losers', 
            'data': data['top_losers'][:10],   # ìƒìœ„ 10ê°œ
            'limit': 10
        },
        {
            'name': 'most_actively_traded',
            'data': data['most_actively_traded'][:20],  # 20ê°œ ì „ë¶€
            'limit': 20
        }
    ]
    
    total_success = 0
    total_error = 0
    
    print(f"ðŸš€ ë°°ì¹˜ {current_batch_id} ë°ì´í„° ì €ìž¥ ì‹œìž‘")
    
    for category in categories:
        category_name = category['name']
        category_data = category['data']
        
        print(f"ðŸ“Š ì²˜ë¦¬ ì¤‘: {category_name} ({len(category_data)}ê°œ)")
        
        success_count = 0
        error_count = 0
        
        for rank, item in enumerate(category_data, 1):
            try:
                # ë°ì´í„° ê²€ì¦ ë° ë³€í™˜
                if not item.get('ticker'):
                    print(f"âš ï¸ ticker ëˆ„ë½: {item}")
                    error_count += 1
                    continue
                
                # ðŸ”‘ íŒŒë¼ë¯¸í„° ì¤€ë¹„ (í˜¸ì¶œ íšŒì°¨ë³„ batch_id í¬í•¨)
                params = {
                    'batch_id': current_batch_id,  # ðŸ”‘ í•µì‹¬: í˜¸ì¶œ íšŒì°¨ ë²ˆí˜¸
                    'last_updated': api_last_updated,  # API ì‘ë‹µ ì‹œê°„ (ì°¸ê³ ìš©)
                    'symbol': item['ticker'][:10],  # VARCHAR(10) ì œí•œ
                    'category': category_name,
                    'rank_position': rank,
                    'price': Decimal(str(item.get('price', '0'))),
                    'change_amount': Decimal(str(item.get('change_amount', '0'))),
                    'change_percentage': item.get('change_percentage', '0%')[:20],  # VARCHAR(20) ì œí•œ
                    'volume': int(item.get('volume', '0'))
                }
                
                # SQL ì‹¤í–‰
                hook.run(UPSERT_SQL, parameters=params)
                success_count += 1
                
            except Exception as e:
                print(f"âŒ ë ˆì½”ë“œ ì €ìž¥ ì‹¤íŒ¨: {item.get('ticker', 'Unknown')} - {str(e)}")
                error_count += 1
                continue
        
        print(f"âœ… {category_name} ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
        total_success += success_count
        total_error += error_count
    
    # ìµœì¢… í†µê³„
    print(f"ðŸŽ¯ ë°°ì¹˜ {current_batch_id} ì €ìž¥ ì™„ë£Œ: {total_success}ê°œ ì„±ê³µ, {total_error}ê°œ ì‹¤íŒ¨")
    
    return {
        'batch_id': current_batch_id,
        'success_count': total_success,
        'error_count': total_error,
        'api_time': api_last_updated.isoformat(),
        'execution_time': context['execution_date'].isoformat()
    }
# DAG ì •ì˜
with DAG(
    dag_id='ingest_top_gainers_to_db',
    default_args=default_args,
    schedule_interval='@daily',  # ë§¤ì¼ ì‹¤í–‰
    catchup=False,
    description='Alpha Vantage APIì—ì„œ Top Gainers/Losers/Most Active ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['stocks', 'alpha_vantage', 'top_gainers', 'daily'],
) as dag:
    
    # 1. í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_top_gainers_table',
        postgres_conn_id='postgres_default',
        sql='create_top_gainers.sql',  # ì‹¤ì œ íŒŒì¼ëª…ì— ë§žì¶¤
    )
    
    # 2. API ë°ì´í„° ìˆ˜ì§‘
    fetch_data = PythonOperator(
        task_id='fetch_top_gainers',
        python_callable=fetch_top_gainers_data,
    )
    
    # 3. ë°ì´í„° ê°€ê³µ ë° ì €ìž¥
    process_data = PythonOperator(
        task_id='process_and_store_data',
        python_callable=process_and_store_data,
    )
    
    # Task ì˜ì¡´ì„±
    create_table >> fetch_data >> process_data