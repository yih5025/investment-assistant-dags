# monitoring_utils.py - DAG í´ë”ì— ì¶”ê°€í•  íŒŒì¼
import time
from datetime import datetime
from airflow.providers.postgres.hooks.postgres import PostgresHook
from contextlib import contextmanager

class DataCollectionMonitor:
    """ë°ì´í„° ìˆ˜ì§‘ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, dag_id, task_id, run_id, execution_date):
        self.dag_id = dag_id
        self.task_id = task_id
        self.run_id = run_id
        self.execution_date = execution_date
        self.hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # ë©”íŠ¸ë¦­ ë°ì´í„° ì´ˆê¸°í™”
        self.metrics = {
            'api_calls_attempted': 0,
            'api_calls_successful': 0,
            'api_calls_failed': 0,
            'api_rate_limited': 0,
            'records_fetched': 0,
            'records_processed': 0,
            'records_inserted': 0,
            'records_updated': 0,
            'records_skipped': 0,
            'duplicate_records': 0,
            'invalid_records': 0,
            'error_message': None,
            'warning_message': None
        }
        
        self.start_time = time.time()
        print(f"ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œì‘: {dag_id}.{task_id}")
    
    def log_api_call(self, success=True, rate_limited=False):
        """API í˜¸ì¶œ ê²°ê³¼ ê¸°ë¡"""
        self.metrics['api_calls_attempted'] += 1
        
        if success:
            self.metrics['api_calls_successful'] += 1
            print(f"âœ… API í˜¸ì¶œ ì„±ê³µ (ì´ {self.metrics['api_calls_successful']}/{self.metrics['api_calls_attempted']})")
        else:
            self.metrics['api_calls_failed'] += 1
            print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨ (ì´ {self.metrics['api_calls_failed']}/{self.metrics['api_calls_attempted']})")
            
        if rate_limited:
            self.metrics['api_rate_limited'] += 1
            print(f"â±ï¸ Rate Limit ë°œìƒ (ì´ {self.metrics['api_rate_limited']}íšŒ)")
    
    def log_data_processing(self, fetched=0, processed=0, inserted=0, updated=0, skipped=0, duplicates=0, invalid=0):
        """ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ ê¸°ë¡"""
        self.metrics['records_fetched'] += fetched
        self.metrics['records_processed'] += processed
        self.metrics['records_inserted'] += inserted
        self.metrics['records_updated'] += updated
        self.metrics['records_skipped'] += skipped
        self.metrics['duplicate_records'] += duplicates
        self.metrics['invalid_records'] += invalid
        
        print(f"ğŸ“¦ ë°ì´í„° ì²˜ë¦¬: ê°€ì ¸ì˜´({fetched}), ì²˜ë¦¬({processed}), ì‚½ì…({inserted}), ìŠ¤í‚µ({skipped})")
    
    def set_error(self, error_message):
        """ì—ëŸ¬ ë©”ì‹œì§€ ì„¤ì •"""
        self.metrics['error_message'] = str(error_message)[:1000]
        print(f"ğŸš¨ ì—ëŸ¬ ê¸°ë¡: {str(error_message)[:100]}...")
    
    def set_warning(self, warning_message):
        """ê²½ê³  ë©”ì‹œì§€ ì„¤ì •"""
        self.metrics['warning_message'] = str(warning_message)[:1000]
        print(f"âš ï¸ ê²½ê³  ê¸°ë¡: {str(warning_message)[:100]}...")
    
    def determine_task_status(self):
        """íƒœìŠ¤í¬ ìƒíƒœ ìë™ ê²°ì •"""
        if self.metrics['error_message']:
            return 'failed'
        elif self.metrics['records_inserted'] == 0 and self.metrics['records_updated'] == 0:
            if self.metrics['api_calls_attempted'] > 0:
                return 'no_data_collected'
            else:
                return 'no_api_calls'
        elif self.metrics['api_calls_failed'] > 0 or self.metrics['api_rate_limited'] > 0:
            return 'partial_success'
        else:
            return 'success'
    
    def finalize_and_save(self, latest_data_timestamp=None):
        """ë©”íŠ¸ë¦­ ìµœì¢… ì €ì¥"""
        execution_duration = int(time.time() - self.start_time)
        task_status = self.determine_task_status()
        
        # ë°ì´í„° ì‹ ì„ ë„ ê³„ì‚°
        data_freshness = None
        if latest_data_timestamp:
            try:
                if isinstance(latest_data_timestamp, str):
                    latest_data_timestamp = datetime.fromisoformat(latest_data_timestamp.replace('Z', '+00:00'))
                now = datetime.now()
                freshness = (now - latest_data_timestamp.replace(tzinfo=None)).total_seconds() / 60
                data_freshness = int(freshness)
            except:
                pass
        
        # DBì— ì €ì¥
        insert_sql = """
        INSERT INTO data_collection_metrics (
            dag_id, task_id, run_id, execution_date,
            api_calls_attempted, api_calls_successful, api_calls_failed, api_rate_limited,
            records_fetched, records_processed, records_inserted, records_updated, records_skipped,
            task_status, execution_duration_seconds, error_message, warning_message,
            data_freshness_minutes, duplicate_records, invalid_records
        ) VALUES (
            %(dag_id)s, %(task_id)s, %(run_id)s, %(execution_date)s,
            %(api_calls_attempted)s, %(api_calls_successful)s, %(api_calls_failed)s, %(api_rate_limited)s,
            %(records_fetched)s, %(records_processed)s, %(records_inserted)s, %(records_updated)s, %(records_skipped)s,
            %(task_status)s, %(execution_duration_seconds)s, %(error_message)s, %(warning_message)s,
            %(data_freshness_minutes)s, %(duplicate_records)s, %(invalid_records)s
        )
        ON CONFLICT (dag_id, task_id, run_id, execution_date) 
        DO UPDATE SET
            api_calls_attempted = EXCLUDED.api_calls_attempted,
            api_calls_successful = EXCLUDED.api_calls_successful,
            api_calls_failed = EXCLUDED.api_calls_failed,
            api_rate_limited = EXCLUDED.api_rate_limited,
            records_fetched = EXCLUDED.records_fetched,
            records_processed = EXCLUDED.records_processed,
            records_inserted = EXCLUDED.records_inserted,
            records_updated = EXCLUDED.records_updated,
            records_skipped = EXCLUDED.records_skipped,
            task_status = EXCLUDED.task_status,
            execution_duration_seconds = EXCLUDED.execution_duration_seconds,
            error_message = EXCLUDED.error_message,
            warning_message = EXCLUDED.warning_message,
            data_freshness_minutes = EXCLUDED.data_freshness_minutes,
            duplicate_records = EXCLUDED.duplicate_records,
            invalid_records = EXCLUDED.invalid_records,
            updated_at = NOW()
        """
        
        params = {
            'dag_id': self.dag_id,
            'task_id': self.task_id,
            'run_id': self.run_id,
            'execution_date': self.execution_date,
            'task_status': task_status,
            'execution_duration_seconds': execution_duration,
            'data_freshness_minutes': data_freshness,
            **self.metrics
        }
        
        try:
            self.hook.run(insert_sql, parameters=params)
            print(f"ğŸ“Š ëª¨ë‹ˆí„°ë§ ì™„ë£Œ ì €ì¥: {task_status}")
            print(f"   ğŸ“ˆ ì‹¤í–‰ì‹œê°„: {execution_duration}ì´ˆ")
            print(f"   ğŸ”¢ API: {self.metrics['api_calls_successful']}/{self.metrics['api_calls_attempted']}")
            print(f"   ğŸ’¾ ë°ì´í„°: {self.metrics['records_inserted']}ê°œ ì‚½ì…")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    @contextmanager
    def monitor_api_call(self):
        """API í˜¸ì¶œ ìë™ ëª¨ë‹ˆí„°ë§"""
        try:
            yield self
            self.log_api_call(success=True)
        except Exception as e:
            error_str = str(e).lower()
            if "429" in error_str or "rate limit" in error_str or "too many requests" in error_str:
                self.log_api_call(success=False, rate_limited=True)
            else:
                self.log_api_call(success=False)
            raise

def create_monitor(context):
    """Contextì—ì„œ ëª¨ë‹ˆí„° ìƒì„± (DAGì—ì„œ ì‚¬ìš©)"""
    return DataCollectionMonitor(
        dag_id=context['dag'].dag_id,
        task_id=context['task'].task_id,
        run_id=context['dag_run'].run_id,
        execution_date=context['execution_date']
    )