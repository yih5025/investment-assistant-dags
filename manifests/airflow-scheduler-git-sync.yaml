apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: airflow-scheduler
  namespace: investment-assistant
  labels:
    app: airflow
    component: scheduler
    tier: airflow
spec:
  serviceName: airflow-scheduler
  replicas: 1
  selector:
    matchLabels:
      app: airflow
      component: scheduler
      tier: airflow
  template:
    metadata:
      labels:
        app: airflow
        component: scheduler
        tier: airflow
    spec:
      securityContext:
        runAsUser: 50000
        fsGroup: 0
        
      initContainers:
      # Git-Sync Init Container
      - name: git-sync-init
        image: registry.k8s.io/git-sync/git-sync:v4.2.1
        args:
        - --repo=https://github.com/yih5025/investment-assistant-dags.git
        - --branch=main
        - --root=/git
        - --dest=dags
        - --one-time
        - --max-failures=3
        volumeMounts:
        - name: dags-volume
          mountPath: /git
        securityContext:
          runAsUser: 50000
          runAsGroup: 0
          
      # Wait for DB Init Container  
      - name: wait-for-airflow-migrations
        image: apache/airflow:2.8.1-python3.11
        command:
        - /bin/bash
        - -c
        - |
          while ! airflow db check; do
            echo "Waiting for database to be ready..."
            sleep 5
          done
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: postgresql+psycopg2://airflow:airflow123@postgresql.investment-assistant.svc.cluster.local:5432/airflow
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
              
      containers:
      # Main Airflow Scheduler Container
      - name: scheduler
        image: apache/airflow:2.8.1-python3.11
        command:
        - /bin/bash
        - -c
        - |
          # DAG 파일 권한 설정
          find /opt/airflow/dags -type f -name "*.py" -exec chmod 644 {} \; 2>/dev/null || true
          find /opt/airflow/dags -type f -name "*.sql" -exec chmod 644 {} \; 2>/dev/null || true
          
          # Airflow 스케줄러 시작
          airflow scheduler
          
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: postgresql+psycopg2://airflow:airflow123@postgresql.investment-assistant.svc.cluster.local:5432/airflow
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        - name: AIRFLOW__CORE__DAGS_FOLDER
          value: /opt/airflow/dags
        - name: AIRFLOW__CORE__LOAD_EXAMPLES
          value: "false"
        - name: AIRFLOW__CORE__EXECUTOR
          value: LocalExecutor
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: redis://redis.investment-assistant.svc.cluster.local:6379/0
        - name: AIRFLOW__CELERY__BROKER_URL
          value: redis://redis.investment-assistant.svc.cluster.local:6379/0
          
        ports:
        - containerPort: 8793
          name: task-logs
          
        volumeMounts:
        # Git-Sync로 가져온 DAG 파일들
        - name: dags-volume
          mountPath: /opt/airflow/dags
          subPath: dags
        # 로그 저장용 PVC
        - name: logs
          mountPath: /opt/airflow/logs
        # Airflow 설정 파일
        - name: config
          mountPath: /opt/airflow/airflow.cfg
          subPath: airflow.cfg
          readOnly: true
        - name: config
          mountPath: /opt/airflow/config/airflow_local_settings.py
          subPath: airflow_local_settings.py
          readOnly: true
          
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
            
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - |
              CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
              airflow jobs check --job-type SchedulerJob --local
          initialDelaySeconds: 10
          periodSeconds: 60
          timeoutSeconds: 20
          failureThreshold: 5
          
        startupProbe:
          exec:
            command:
            - sh
            - -c
            - |
              CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
              airflow jobs check --job-type SchedulerJob --local
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 20
          failureThreshold: 6
          
      # Git-Sync Sidecar Container (지속적 동기화)
      - name: git-sync-sidecar
        image: registry.k8s.io/git-sync/git-sync:v4.2.1
        args:
        - --repo=https://github.com/YOUR_USERNAME/investment-assistant-dags.git
        - --branch=main
        - --root=/git
        - --dest=dags
        - --wait=60  # 60초마다 Git pull
        - --max-failures=3
        volumeMounts:
        - name: dags-volume
          mountPath: /git
        securityContext:
          runAsUser: 50000
          runAsGroup: 0
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
            
      # Log Groomer Container
      - name: scheduler-log-groomer
        image: apache/airflow:2.8.1-python3.11
        command:
        - /bin/bash
        - -c
        - |
          while true; do
            airflow tasks clear --yes --only-failed --dag-id ALL --start-date $(date -d '7 days ago' +%Y-%m-%d) 2>/dev/null || true
            find /opt/airflow/logs -type f -mtime +7 -delete 2>/dev/null || true
            sleep 86400  # 24시간마다 실행
          done
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: postgresql+psycopg2://airflow:airflow123@postgresql.investment-assistant.svc.cluster.local:5432/airflow
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        volumeMounts:
        - name: dags-volume
          mountPath: /opt/airflow/dags
          readOnly: true
        - name: logs
          mountPath: /opt/airflow/logs
        resources:
          requests:
            memory: "64Mi"
            cpu: "25m"
          limits:
            memory: "128Mi"
            cpu: "50m"
            
      volumes:
      # Git-Sync용 임시 볼륨
      - name: dags-volume
        emptyDir: {}
      # ConfigMap 볼륨
      - name: config
        configMap:
          name: airflow-config
          
  volumeClaimTemplates:
  # 로그 저장용 PVC
  - metadata:
      name: logs
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: local-path
      resources:
        requests:
          storage: 10Gi