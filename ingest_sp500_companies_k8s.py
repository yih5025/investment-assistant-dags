from datetime import datetime, timedelta
import requests
import pandas as pd
from bs4 import BeautifulSoup
import os
import json

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

# ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_sp500_companies.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': 2,  # Wikipedia ì ‘ì† ì‹¤íŒ¨ ëŒ€ë¹„
    'retry_delay': timedelta(minutes=5),
}

def scrape_sp500_from_wikipedia(**context):
    """Wikipediaì—ì„œ S&P 500 ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘"""
    
    print("ğŸ” Wikipedia S&P 500 ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
    
    try:
        # User-Agent ì„¤ì • (Wikipedia ì°¨ë‹¨ ë°©ì§€)
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # Wikipedia S&P 500 í˜ì´ì§€ ìš”ì²­
        url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
        print(f"ğŸ“¡ ìš”ì²­ URL: {url}")
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        print("âœ… Wikipedia í˜ì´ì§€ ì ‘ì† ì„±ê³µ")
        
        # Pandasë¡œ í…Œì´ë¸” ì§ì ‘ ì½ê¸° (ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•)
        tables = pd.read_html(url, header=0)
        
        # ì²« ë²ˆì§¸ í…Œì´ë¸”ì´ S&P 500 êµ¬ì„± ìš”ì†Œ í…Œì´ë¸”
        df = tables[0]
        
        print(f"ğŸ“Š ìŠ¤í¬ë˜í•‘ëœ ê¸°ì—… ìˆ˜: {len(df)}ê°œ")
        print(f"ğŸ“‹ ì»¬ëŸ¼: {list(df.columns)}")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        companies_data = []
        
        for index, row in df.iterrows():
            try:
                # Wikipedia í…Œì´ë¸” êµ¬ì¡°ì— ë§ì¶° ë°ì´í„° ì¶”ì¶œ
                company_data = {
                    'symbol': str(row.get('Symbol', '')).strip(),
                    'company_name': str(row.get('Security', '')).strip(),
                    'gics_sector': str(row.get('GICS Sector', '')).strip(),
                    'gics_sub_industry': str(row.get('GICS Sub-Industry', '')).strip(),
                    'headquarters': str(row.get('Headquarters Location', '')).strip(),
                    'date_added': str(row.get('Date first added', '')).strip() if pd.notna(row.get('Date first added')) else None,
                    'cik': str(row.get('CIK', '')).strip() if pd.notna(row.get('CIK')) else None,
                    'founded': str(row.get('Founded', '')).strip() if pd.notna(row.get('Founded')) else None,
                }
                
                # ë¹ˆ ê°’ ì²˜ë¦¬
                for key, value in company_data.items():
                    if value == '' or value == 'nan':
                        company_data[key] = None
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                if company_data['symbol'] and company_data['company_name']:
                    companies_data.append(company_data)
                else:
                    print(f"âš ï¸ í•„ìˆ˜ í•„ë“œ ëˆ„ë½ìœ¼ë¡œ ê±´ë„ˆëœ€: {row}")
                    
            except Exception as e:
                print(f"âŒ í–‰ ì²˜ë¦¬ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {index}): {e}")
                continue
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(companies_data)}ê°œ ê¸°ì—…")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        if companies_data:
            print("ğŸ“‹ ì²« ë²ˆì§¸ ê¸°ì—… ìƒ˜í”Œ:")
            for key, value in companies_data[0].items():
                print(f"   {key}: {value}")
        
        # XComì— ê²°ê³¼ ì €ì¥
        context['ti'].xcom_push(key='sp500_companies', value=companies_data)
        
        return len(companies_data)
        
    except Exception as e:
        print(f"âŒ Wikipedia ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
        
        # ë°±ì—… ë°©ë²•: BeautifulSoup ì‚¬ìš©
        print("ğŸ”„ ë°±ì—… ë°©ë²• ì‹œë„: BeautifulSoup ìŠ¤í¬ë˜í•‘...")
        return scrape_with_beautifulsoup(**context)

def scrape_with_beautifulsoup(**context):
    """ë°±ì—… ë°©ë²•: BeautifulSoupì„ ì‚¬ìš©í•œ ìŠ¤í¬ë˜í•‘"""
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # S&P 500 êµ¬ì„± ìš”ì†Œ í…Œì´ë¸” ì°¾ê¸°
        table = soup.find('table', {'class': 'wikitable sortable'})
        
        if not table:
            raise Exception("S&P 500 í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        companies_data = []
        rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
        
        print(f"ğŸ“Š í…Œì´ë¸”ì—ì„œ {len(rows)}ê°œ í–‰ ë°œê²¬")
        
        for i, row in enumerate(rows):
            try:
                cells = row.find_all('td')
                
                if len(cells) >= 7:  # ìµœì†Œ í•„ìš”í•œ ì…€ ìˆ˜
                    company_data = {
                        'symbol': cells[0].get_text().strip(),
                        'company_name': cells[1].get_text().strip(),
                        'gics_sector': cells[3].get_text().strip(),
                        'gics_sub_industry': cells[4].get_text().strip(),
                        'headquarters': cells[5].get_text().strip(),
                        'date_added': cells[6].get_text().strip() if len(cells) > 6 else None,
                        'cik': cells[7].get_text().strip() if len(cells) > 7 else None,
                        'founded': cells[8].get_text().strip() if len(cells) > 8 else None,
                    }
                    
                    # ë°ì´í„° ì •ë¦¬
                    for key, value in company_data.items():
                        if value == '' or value == 'â€”' or value == 'N/A':
                            company_data[key] = None
                    
                    if company_data['symbol'] and company_data['company_name']:
                        companies_data.append(company_data)
                
            except Exception as e:
                print(f"âš ï¸ í–‰ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"âœ… BeautifulSoup ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(companies_data)}ê°œ ê¸°ì—…")
        
        # XComì— ê²°ê³¼ ì €ì¥
        context['ti'].xcom_push(key='sp500_companies', value=companies_data)
        
        return len(companies_data)
        
    except Exception as e:
        print(f"âŒ BeautifulSoup ìŠ¤í¬ë˜í•‘ë„ ì‹¤íŒ¨: {e}")
        raise

def store_sp500_companies(**context):
    """S&P 500 ê¸°ì—… ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    
    # XComì—ì„œ ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    companies_data = context['ti'].xcom_pull(key='sp500_companies') or []
    
    if not companies_data:
        print("âš ï¸ ì €ì¥í•  ê¸°ì—… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return 0
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    success_count = 0
    error_count = 0
    update_count = 0
    
    print(f"ğŸ’¾ DB ì €ì¥ ì‹œì‘: {len(companies_data)}ê°œ ê¸°ì—…")
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµë¥¼ ìœ„í•´ í˜„ì¬ DB ìƒíƒœ í™•ì¸
    try:
        existing_companies = hook.get_first("SELECT COUNT(*) FROM sp500_companies")
        existing_count = existing_companies[0] if existing_companies else 0
        print(f"ğŸ“Š ê¸°ì¡´ DB ê¸°ì—… ìˆ˜: {existing_count}ê°œ")
    except Exception as e:
        print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")
        existing_count = 0
    
    for company_data in companies_data:
        try:
            # UPSERT ì‹¤í–‰
            hook.run(UPSERT_SQL, parameters=company_data)
            success_count += 1
            
            # ì§„í–‰ë¥  í‘œì‹œ (100ê°œë§ˆë‹¤)
            if success_count % 100 == 0:
                print(f"ğŸ“Š ì €ì¥ ì§„í–‰ë¥ : {success_count}/{len(companies_data)}")
                
        except Exception as e:
            print(f"âŒ ê¸°ì—… ì €ì¥ ì‹¤íŒ¨: {company_data.get('symbol', 'Unknown')} - {e}")
            error_count += 1
            continue
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
    
    # ìµœì¢… í†µê³„
    try:
        # ì €ì¥ í›„ ì´ ê¸°ì—… ìˆ˜
        result = hook.get_first("SELECT COUNT(*) FROM sp500_companies")
        total_companies = result[0] if result else 0
        
        # ìƒˆë¡œ ì¶”ê°€ëœ ê¸°ì—… ìˆ˜ ê³„ì‚°
        new_companies = total_companies - existing_count
        update_count = success_count - max(0, new_companies)
        
        print(f"\nğŸ“ˆ ìµœì¢… í†µê³„:")
        print(f"   ğŸ“Š ì´ S&P 500 ê¸°ì—…: {total_companies}ê°œ")
        print(f"   ğŸ†• ì‹ ê·œ ì¶”ê°€: {max(0, new_companies)}ê°œ")
        print(f"   ğŸ”„ ì—…ë°ì´íŠ¸: {update_count}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨: {error_count}ê°œ")
        
        # ì„¹í„°ë³„ ë¶„í¬ í™•ì¸
        sector_stats = hook.get_records("""
            SELECT gics_sector, COUNT(*) as count 
            FROM sp500_companies 
            WHERE gics_sector IS NOT NULL
            GROUP BY gics_sector 
            ORDER BY count DESC
        """)
        
        print(f"\nğŸ“Š ì„¹í„°ë³„ ë¶„í¬:")
        for sector, count in sector_stats:
            print(f"   - {sector}: {count}ê°œ")
            
    except Exception as e:
        print(f"âš ï¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    return success_count

def validate_sp500_data(**context):
    """S&P 500 ë°ì´í„° ê²€ì¦ ë° í’ˆì§ˆ ì²´í¬"""
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    try:
        print("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œì‘...")
        
        # 1. ì´ ê¸°ì—… ìˆ˜ í™•ì¸
        total_count = hook.get_first("SELECT COUNT(*) FROM sp500_companies")[0]
        print(f"ğŸ“Š ì´ ê¸°ì—… ìˆ˜: {total_count}ê°œ")
        
        # S&P 500ì€ ëŒ€ëµ 500ê°œ ê¸°ì—…ì´ì–´ì•¼ í•¨ (ì¼ë¶€ ë‹¤ì¤‘ í´ë˜ìŠ¤ ì£¼ì‹ìœ¼ë¡œ 503ê°œ ì •ë„)
        if total_count < 480 or total_count > 520:
            print(f"âš ï¸ ê²½ê³ : ê¸°ì—… ìˆ˜ê°€ ì˜ˆìƒ ë²”ìœ„(480-520)ë¥¼ ë²—ì–´ë‚¨: {total_count}")
        
        # 2. í•„ìˆ˜ í•„ë“œ ëˆ„ë½ í™•ì¸
        missing_symbol = hook.get_first("SELECT COUNT(*) FROM sp500_companies WHERE symbol IS NULL OR symbol = ''")[0]
        missing_name = hook.get_first("SELECT COUNT(*) FROM sp500_companies WHERE company_name IS NULL OR company_name = ''")[0]
        
        print(f"ğŸ“‹ ë°ì´í„° í’ˆì§ˆ:")
        print(f"   - ì‹¬ë³¼ ëˆ„ë½: {missing_symbol}ê°œ")
        print(f"   - íšŒì‚¬ëª… ëˆ„ë½: {missing_name}ê°œ")
        
        if missing_symbol > 0 or missing_name > 0:
            print("âš ï¸ ê²½ê³ : í•„ìˆ˜ í•„ë“œì— ëˆ„ë½ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤")
        
        # 3. ì¤‘ë³µ ì‹¬ë³¼ í™•ì¸
        duplicates = hook.get_records("""
            SELECT symbol, COUNT(*) as count 
            FROM sp500_companies 
            GROUP BY symbol 
            HAVING COUNT(*) > 1
        """)
        
        if duplicates:
            print(f"âš ï¸ ì¤‘ë³µ ì‹¬ë³¼ ë°œê²¬: {len(duplicates)}ê°œ")
            for symbol, count in duplicates:
                print(f"   - {symbol}: {count}ë²ˆ ì¤‘ë³µ")
        else:
            print("âœ… ì¤‘ë³µ ì‹¬ë³¼ ì—†ìŒ")
        
        # 4. ìµœê·¼ ì—…ë°ì´íŠ¸ëœ ê¸°ì—…ë“¤ í™•ì¸
        recent_updates = hook.get_records("""
            SELECT symbol, company_name, updated_at 
            FROM sp500_companies 
            WHERE updated_at >= NOW() - INTERVAL '1 day'
            ORDER BY updated_at DESC
            LIMIT 10
        """)
        
        print(f"\nğŸ”„ ìµœê·¼ ì—…ë°ì´íŠ¸ëœ ê¸°ì—… (ìµœëŒ€ 10ê°œ):")
        for symbol, name, updated_at in recent_updates:
            print(f"   - {symbol}: {name} ({updated_at})")
        
        # 5. ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        sample_companies = hook.get_records("""
            SELECT symbol, company_name, gics_sector 
            FROM sp500_companies 
            ORDER BY symbol 
            LIMIT 5
        """)
        
        print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„°:")
        for symbol, name, sector in sample_companies:
            print(f"   - {symbol}: {name} ({sector})")
        
        print("âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
        
        return {
            'total_companies': total_count,
            'missing_data': missing_symbol + missing_name,
            'duplicates': len(duplicates),
            'recent_updates': len(recent_updates)
        }
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise

# DAG ì •ì˜
with DAG(
    dag_id='ingest_sp500_companies_wikipedia_k8s',
    default_args=default_args,
    schedule_interval='0 6 * * 1',  # ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 6ì‹œ ì‹¤í–‰
    catchup=False,
    description='Wikipediaì—ì„œ S&P 500 ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° DB ì €ì¥ (ì™„ì „ ë¬´ë£Œ)',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['sp500', 'wikipedia', 'companies', 'scraping', 'free', 'k8s'],
) as dag:
    
    # í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_sp500_companies_table',
        postgres_conn_id='postgres_default',
        sql='create_sp500_companies.sql',
    )
    
    # Wikipediaì—ì„œ S&P 500 ê¸°ì—… ë¦¬ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘
    scrape_companies = PythonOperator(
        task_id='scrape_sp500_from_wikipedia',
        python_callable=scrape_sp500_from_wikipedia,
    )
    
    # DBì— ì €ì¥
    store_companies = PythonOperator(
        task_id='store_sp500_companies',
        python_callable=store_sp500_companies,
    )
    
    # ë°ì´í„° ê²€ì¦
    validate_data = PythonOperator(
        task_id='validate_sp500_data',
        python_callable=validate_sp500_data,
    )
    
    # Task ì˜ì¡´ì„±
    create_table >> scrape_companies >> store_companies >> validate_data