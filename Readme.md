# ğŸ”„ Investment Assistant Airflow DAGs

> **30ê°œ í…Œì´ë¸”**, **7ê°œ ì™¸ë¶€ API** ì—°ë™ì„ í†µí•œ ì¢…í•©ì  ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸  
> K3s í´ëŸ¬ìŠ¤í„°ì—ì„œ ìš´ì˜ë˜ëŠ” ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë°ì´í„° ì›Œí¬í”Œë¡œìš°

[![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.8-green?logo=apache-airflow)](https://airflow.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)](https://python.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-13-blue?logo=postgresql)](https://postgresql.org/)

---

## ğŸ“Š ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°œìš”
- **ì‹¤ì‹œê°„ + ë°°ì¹˜** í•˜ì´ë¸Œë¦¬ë“œ ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘
- **30ê°œ í…Œì´ë¸”** ë‹¤ì°¨ì› ê¸ˆìœµ ìƒíƒœê³„ í†µí•©
- **7ê°œ ì™¸ë¶€ API** ìë™ ì—°ë™ ë° ë°ì´í„° ì •ê·œí™”
- **ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì—ëŸ¬ ì²˜ë¦¬** ë° ëª¨ë‹ˆí„°ë§

### ğŸ“ˆ ì²˜ë¦¬ ê·œëª¨
- **13,115 ë¼ì¸** DAGs ì½”ë“œë² ì´ìŠ¤ (Python + SQL)
- **60ê°œ SQL íŒŒì¼** ë°ì´í„° ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- **35ê°œ Python íŒŒì¼** DAG ë° ë¡œì§ êµ¬í˜„
- **5ëŒ€ ì„œë²„ K3s í´ëŸ¬ìŠ¤í„°** ë¶„ì‚° ì‹¤í–‰

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì¡°

### ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
dags/
â”œâ”€â”€ *.py                    # 35ê°œ DAG Python íŒŒì¼ë“¤
â”œâ”€â”€ sql/                    # 28ê°œ DML SQL íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ upsert_*.sql       # í…Œì´ë¸”ë³„ Upsert ë¡œì§
â”‚   â””â”€â”€ select_*.sql       # ë³µì¡í•œ ì¡°íšŒ ì¿¼ë¦¬
â”œâ”€â”€ initdb/                 # 31ê°œ DDL SQL íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ create_*.sql       # í…Œì´ë¸” ìƒì„± ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ insert_*.sql       # ì´ˆê¸° ë°ì´í„° ì‚½ì…
â””â”€â”€ test/                   # 7ê°œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
    â”œâ”€â”€ *.py               # Python í…ŒìŠ¤íŠ¸
    â”œâ”€â”€ *.sh               # Shell ìŠ¤í¬ë¦½íŠ¸
    â””â”€â”€ *.sql              # SQL í…ŒìŠ¤íŠ¸
```

### ğŸ”„ ë°ì´í„° í”Œë¡œìš°
```mermaid
graph TD
    subgraph "External APIs"
        A1[NewsAPI] 
        A2[Finnhub]
        A3[Alpha Vantage]
        A4[FRED]
        A5[FMP]
        A6[X API]
        A7[Truth Social]
    end
    
    subgraph "Airflow DAGs"
        B1[Market News DAG]
        B2[Company News DAG]
        B3[Earnings DAG]
        B4[Balance Sheet DAG]
        B5[Economic Data DAG]
        B6[Social Media DAG]
        B7[Crypto Data DAG]
    end
    
    subgraph "Database Tables (30ê°œ)"
        C1[ì£¼ì‹ ë°ì´í„°<br/>9ê°œ í…Œì´ë¸”]
        C2[ì•”í˜¸í™”í<br/>9ê°œ í…Œì´ë¸”]
        C3[ë‰´ìŠ¤/ê°ì„±<br/>8ê°œ í…Œì´ë¸”]
        C4[ê²½ì œì§€í‘œ<br/>4ê°œ í…Œì´ë¸”]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B4
    A4 --> B5
    A5 --> B3
    A6 --> B6
    A7 --> B6
    
    B1 --> C3
    B2 --> C1
    B3 --> C1
    B4 --> C1
    B5 --> C4
    B6 --> C3
    B7 --> C2
```

---

## ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ (30ê°œ í…Œì´ë¸”)

### **ğŸ“ˆ ì£¼ì‹ ë° ê¸°ì—… ë°ì´í„° (9ê°œ í…Œì´ë¸”)**
| í…Œì´ë¸”ëª… | ìš©ë„ | ë°ì´í„° ì†ŒìŠ¤ |
|----------|------|-------------|
| `sp500_companies` | S&P 500 ì „ì²´ ê¸°ì—… ì •ë³´ | Manual/API |
| `sp500_top50` | ì£¼ìš” 50ê°œ ê¸°ì—… | Filtered |
| `company_overview` | ê¸°ì—… ê°œìš” ì •ë³´ | Alpha Vantage |
| `balance_sheet` | ì¬ë¬´ì œí‘œ ë°ì´í„° | Alpha Vantage |
| `earnings_calendar` | ì‹¤ì  ë°œí‘œ ìº˜ë¦°ë” | Alpha Vantage |
| `top_gainers` | ìƒìŠ¹ë¥  ìƒìœ„ ì¢…ëª© | Alpha Vantage |
| `company_news` | ê¸°ì—…ë³„ ë‰´ìŠ¤ | NewsAPI/Finnhub |
| `earnings_news` | ì‹¤ì  ê´€ë ¨ ë‰´ìŠ¤ | Finnhub API |
| `earnings_news_finnhub` | Finnhub ì‹¤ì  ë‰´ìŠ¤ | Finnhub API |

### **ğŸ’° ì•”í˜¸í™”í ë°ì´í„° (9ê°œ í…Œì´ë¸”)**
| í…Œì´ë¸”ëª… | ìš©ë„ | ë°ì´í„° ì†ŒìŠ¤ |
|----------|------|-------------|
| `bithumb_ticker` | ë¹—ì¸ ì‹¤ì‹œê°„ ê°€ê²© | Bithumb API |
| `market_code_bithumb` | ë¹—ì¸ ë§ˆì¼“ ì½”ë“œ | Bithumb API |
| `coingecko_coin_details` | ì½”ì¸ê²Œì½” ìƒì„¸ ì •ë³´ | CoinGecko API |
| `coingecko_derivatives` | íŒŒìƒìƒí’ˆ ë°ì´í„° | CoinGecko API |
| `coingecko_global` | ê¸€ë¡œë²Œ ì•”í˜¸í™”í ë°ì´í„° | CoinGecko API |
| `coingecko_id_mapping` | ì½”ì¸ê²Œì½” ID ë§¤í•‘ | CoinGecko API |
| `coingecko_tickers` | ì½”ì¸ê²Œì½” í‹°ì»¤ ë°ì´í„° | CoinGecko API |
| `coingecko_tickers_bithumb` | ë¹—ì¸ íŠ¹í™” í†µí•© í‹°ì»¤ | Combined |
| `bithumb_coingecko_mapping` | ë¹—ì¸-ì½”ì¸ê²Œì½” ë§¤í•‘ í…Œì´ë¸” | Cross-reference |

**ğŸ’¡ ì•”í˜¸í™”í ë°ì´í„° í†µí•© ì „ëµ**
- `market_code_bithumb` + `coingecko_id_mapping` â†’ `bithumb_coingecko_mapping` ìƒì„±
- ë¹—ì¸ ì‹¬ë³¼ ì¤‘ì‹¬ìœ¼ë¡œ CoinGecko ë°ì´í„° ë§¤í•‘í•˜ì—¬ `coingecko_tickers_bithumb`ì— í†µí•©

### **ğŸ“° ë‰´ìŠ¤ ë° ê°ì„± ë¶„ì„ (8ê°œ í…Œì´ë¸”)**
| í…Œì´ë¸”ëª… | ìš©ë„ | ë°ì´í„° ì†ŒìŠ¤ |
|----------|------|-------------|
| `market_news` | ì‹œì¥ ë‰´ìŠ¤ | NewsAPI |
| `market_news_finnhub` | Finnhub ì‹œì¥ ë‰´ìŠ¤ | Finnhub API |
| `market_news_sentiment` | ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ê²°ê³¼ | Alpha Vantage News |
| `truth_social_posts` | Truth Social ê²Œì‹œë¬¼ | Truth Social API |
| `truth_social_tags` | Truth Social íƒœê·¸ | Truth Social API |
| `truth_social_trends` | Truth Social íŠ¸ë Œë“œ | Truth Social API |
| `x_posts` | X(Twitter) ê²Œì‹œë¬¼ | X API |
| `x_user_profiles` | X ì‚¬ìš©ì í”„ë¡œí•„ | X API |

### **ğŸ“Š ê²½ì œ ì§€í‘œ (4ê°œ í…Œì´ë¸”)**
| í…Œì´ë¸”ëª… | ìš©ë„ | ë°ì´í„° ì†ŒìŠ¤ |
|----------|------|-------------|
| `cpi` | ì†Œë¹„ì ë¬¼ê°€ì§€ìˆ˜ | FRED API |
| `inflation` | ì¸í”Œë ˆì´ì…˜ ë°ì´í„° | FRED API |
| `federal_funds_rate` | ì—°ë°©ê¸°ê¸ˆê¸ˆë¦¬ | FRED API |
| `treasury_yield` | êµ­ì±„ ìˆ˜ìµë¥  | FRED API |


---

## ğŸ¯ **ë¬´ë£Œ API í•œê³„ ì „ëµ**

ì‹¤ì œ DAG ì½”ë“œì—ì„œ êµ¬í˜„ëœ í˜ì‹ ì ì¸ ë¬´ë£Œ API í™œìš© ì „ëµë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.

### **ğŸ“± X API Rate Limit ê·¹ë³µ ì „ëµ**

#### **1. ì´ì¤‘ í† í° + 15ë¶„ ë”œë ˆì´ ì „ëµ**
```python
# ingest_x_posts_primary_with_delay_k8s.py
# Primary Token: í•µì‹¬ íˆ¬ìì ê³„ì • (ìƒˆë²½ 2ì‹œ)
PRIMARY_ACCOUNT_SCHEDULE = {
    'elonmusk': {'frequency': 'daily', 'max_results': 50, 'priority': 1},
    'RayDalio': {'frequency': 'every_2_days', 'max_results': 50, 'priority': 1},
    'jimcramer': {'frequency': 'every_2_days', 'max_results': 50, 'priority': 1},
}

# Secondary Token: í™•ì¥ ê³„ì • (ìƒˆë²½ 4ì‹œ)  
SECONDARY_ACCOUNT_SCHEDULE = {
    'saylor': {'frequency': 'daily', 'category': 'crypto'},
    'brian_armstrong': {'frequency': 'twice_weekly', 'category': 'crypto'},
    'CathieDWood': {'frequency': 'twice_weekly', 'category': 'institutional'},
}

# 15ë¶„ ë”œë ˆì´ë¡œ Rate Limit ì¤€ìˆ˜
if i > 0:
    time.sleep(15 * 60)  # 15ë¶„ ëŒ€ê¸°
```

#### **2. DB ê¸°ë°˜ ê³„ì • ê´€ë¦¬**
```python
def get_user_id_from_db(username):
    """DBì—ì„œ usernameìœ¼ë¡œ user_id ì¡°íšŒ"""
    hook = PostgresHook(postgres_conn_id='postgres_default')
    result = hook.get_first(
        "SELECT user_id FROM x_user_profiles WHERE username = %s",
        parameters=[username]
    )
    return result[0] if result else None
```

#### **3. ìš”ì¼ë³„ ë¶„ì‚° ì‹¤í–‰**
- **ì›”ìš”ì¼**: ì—ë„ˆì§€/ì œì¡°ì—… CEO ì¤‘ì‹¬
- **í™”ìš”ì¼**: ë¹…í…Œí¬ CEO ì¤‘ì‹¬  
- **ìˆ˜ìš”ì¼**: ì•”í˜¸í™”í/ê¸ˆìœµ ì¸í”Œë£¨ì–¸ì„œ
- **ëª©ìš”ì¼**: ê¸°ê´€íˆ¬ìì ê³„ì •
- **ì¼ì¼ í•œê³„**: 17íšŒ API í˜¸ì¶œ (Free Tier)

### **ğŸ“° Alpha Vantage News API ìµœì í™”**

#### **1. ìš”ì¼ë³„ ì „ë¬¸í™” ì¿¼ë¦¬ ì „ëµ**
```python
# ingest_market_news_sentiment.py
class SimplifiedWeeklySpecializedQueries:
    def _monday_energy_manufacturing_simple(self):
        """ì›”ìš”ì¼: ì—ë„ˆì§€ & ì œì¡°ì—… ì „ë¬¸ (25ê°œ ì¿¼ë¦¬)"""
        return [
            {'type': 'energy_general', 'params': 'topics=energy_transportation&sort=LATEST'},
            {'type': 'exxon_energy', 'params': 'tickers=XOM&topics=energy_transportation'},
            {'type': 'chevron_energy', 'params': 'tickers=CVX&topics=energy_transportation'},
            # ... 25ê°œ ì „ë¬¸ ì¿¼ë¦¬
        ]
    
    def _tuesday_technology_ipo_simple(self):
        """í™”ìš”ì¼: ê¸°ìˆ  & IPO ì „ë¬¸ (25ê°œ ì¿¼ë¦¬)"""
        return [
            {'type': 'apple_tech', 'params': 'tickers=AAPL&topics=technology'},
            {'type': 'nvidia_tech', 'params': 'tickers=NVDA&topics=technology'},
            # ... 25ê°œ ì „ë¬¸ ì¿¼ë¦¬  
        ]
```

#### **2. ì¼ì¼ 25íšŒ í•œê³„ ìµœì í™”**
- **ê¸°ì¡´**: ë¬´ì‘ìœ„ 50ê°œ ì¿¼ë¦¬ â†’ íš¨ìœ¨ ë‚®ìŒ
- **ê°œì„ **: ìš”ì¼ë³„ ì „ë¬¸í™” 25ê°œ â†’ ìˆ˜ì§‘ í’ˆì§ˆ í–¥ìƒ
- **ê²°ê³¼**: í˜¸ì¶œë‹¹ í‰ê·  ë‰´ìŠ¤ ìˆ˜ 3ë°° ì¦ê°€

### **ğŸ’¼ Company Overview ì§„í–‰í˜• ìˆ˜ì§‘**

#### **1. ì‹¤íŒ¨ ì§€ì  ì¬ì‹œì‘ ì‹œìŠ¤í…œ**
```python
# ingest_company_overview_progressive_k8s.py
def get_or_create_collection_progress():
    """ì‹¤íŒ¨í•œ ì‹¬ë³¼ë¶€í„° ì¬ì‹œì‘í•˜ëŠ” ë¡œì§"""
    progress_result = hook.get_first("""
        SELECT current_position, total_symbols 
        FROM company_overview_progress 
        WHERE collection_name = 'sp500_full' AND status = 'active'
    """)
    
    if progress_result:
        current_position = progress_result[0]
        print(f"ì´ì „ ì§„í–‰ìƒí™©ì—ì„œ ì¬ì‹œì‘: ìœ„ì¹˜ {current_position}")
    
    # ì˜¤ëŠ˜ ìˆ˜ì§‘í•  ì‹¬ë³¼ë“¤ (ìµœëŒ€ 20ê°œ)
    today_symbols = all_symbols[current_position:current_position + 20]
```

#### **2. API í‚¤ ë¡œí…Œì´ì…˜**
```python
api_keys = [
    Variable.get('ALPHA_VANTAGE_API_KEY_3'),
    Variable.get('ALPHA_VANTAGE_API_KEY_4')
]

# API í‚¤ ìˆœí™˜ ì‚¬ìš©ìœ¼ë¡œ í•œê³„ ê·¹ë³µ
api_key = valid_keys[i % len(valid_keys)]
```

#### **3. 30ì´ˆ ë”œë ˆì´ + Rate Limit ê°ì§€**
```python
# Rate Limit ì¦‰ì‹œ ê°ì§€ ë° ì¤‘ë‹¨
if ('Information' in data and 'rate limit' in data['Information'].lower()):
    print(f"Rate Limit ë„ë‹¬: {symbol}")
    break

# ì•ˆì „í•œ 30ì´ˆ ë”œë ˆì´
time.sleep(30)
```

### **ğŸŠ ì „ëµì˜ í•µì‹¬ ì„±ê³¼**

#### **ë¹„ìš© íš¨ìœ¨ì„±**
- **ì´ API ë¹„ìš©**: $0 (ì™„ì „ ë¬´ë£Œ)
- **ë°ì´í„° ìˆ˜ì§‘ëŸ‰**: ì¼ì¼ 1,000+ ë°ì´í„° í¬ì¸íŠ¸
- **ì»¤ë²„ë¦¬ì§€**: 30ê°œ í…Œì´ë¸” ì „ ì˜ì—­

#### **ê¸°ìˆ ì  í˜ì‹ **
- **ì´ì¤‘ í† í° ì‹œê°„ì°¨ ì‹¤í–‰**: Rate Limit ìš°íšŒ
- **ìš”ì¼ë³„ ì „ë¬¸í™”**: ìˆ˜ì§‘ íš¨ìœ¨ 300% í–¥ìƒ  
- **ì‹¤íŒ¨ ì§€ì  ì¬ì‹œì‘**: ë¬´ì†ì‹¤ ìˆ˜ì§‘ ë³´ì¥
- **DB ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬**: ì™„ì „ ìë™í™”

---

## ğŸ”„ DAG ì›Œí¬í”Œë¡œìš° êµ¬ì„±

### **ğŸ“… ìŠ¤ì¼€ì¤„ë§ ì „ëµ**
| ì¹´í…Œê³ ë¦¬ | ìŠ¤ì¼€ì¤„ | DAG ìˆ˜ | ì‹¤í–‰ ì‹œê°„ |
|----------|---------|--------|----------|
| **ì‹¤ì‹œê°„ ë°ì´í„°** | @hourly | 5ê°œ | 24ì‹œê°„ |
| **ì¼ë³„ ë°°ì¹˜** | @daily | 15ê°œ | ë§¤ì¼ 03:00 |
| **ì£¼ë³„ ë°°ì¹˜** | @weekly | 8ê°œ | ì¼ìš”ì¼ 05:00 |
| **ì›”ë³„ ë°°ì¹˜** | @monthly | 7ê°œ | ë§¤ì›” 1ì¼ |

### **ğŸš€ ì£¼ìš” DAG ì›Œí¬í”Œë¡œìš°**

#### **1. ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸**
```python
# ingest_market_news_to_db.py
create_market_news_table >> fetch_news_from_api >> process_sentiment >> upsert_to_database
â”‚
â”œâ”€ í…Œì´ë¸” ìƒì„±/ì—…ë°ì´íŠ¸
â”œâ”€ NewsAPIì—ì„œ ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘
â”œâ”€ ê°ì„± ë¶„ì„ ì²˜ë¦¬
â””â”€ PostgreSQL ì €ì¥ (upsert)
```

#### **2. ì‹¤ì  ìº˜ë¦°ë” íŒŒì´í”„ë¼ì¸**
```python
# ingest_earnings_calendar_to_db.py  
create_earnings_table >> fetch_calendar_data >> validate_data >> upsert_earnings
â”‚
â”œâ”€ Alpha Vantage API í˜¸ì¶œ
â”œâ”€ 12ê°œì›” ì‹¤ì  ë°ì´í„° ìˆ˜ì§‘
â”œâ”€ ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”
â””â”€ ì¤‘ë³µ ì œê±° í›„ ì €ì¥
```

#### **3. ì¬ë¬´ì œí‘œ ë°°ì¹˜ ì²˜ë¦¬**
```python
# ingest_balance_sheet_batch.py
create_balance_sheet_table >> fetch_sp500_symbols >> process_batch >> upsert_financial_data
â”‚
â”œâ”€ S&P 500 ì‹¬ë³¼ ëª©ë¡ ë¡œë“œ
â”œâ”€ 10ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
â”œâ”€ API Rate Limit ê´€ë¦¬
â””â”€ ì¬ë¬´ì œí‘œ ë°ì´í„° ì €ì¥
```

#### **4. ì•”í˜¸í™”í ì‹¤ì‹œê°„ ìˆ˜ì§‘**
```python
# crypto_data_pipeline.py
create_crypto_tables >> fetch_bithumb_data >> fetch_coingecko_data >> cross_mapping >> store_data
â”‚
â”œâ”€ ë¹—ì¸ ì‹¤ì‹œê°„ ê°€ê²© ìˆ˜ì§‘
â”œâ”€ CoinGecko ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
â”œâ”€ í¬ë¡œìŠ¤ ë§¤í•‘ ì²˜ë¦¬
â””â”€ í†µí•© ì €ì¥
```

---

## ğŸ› ï¸ ì½”ë”© í‘œì¤€ ë° ëª¨ë²” ì‚¬ë¡€

### **ğŸ“‹ í‘œì¤€ DAG êµ¬ì¡°**
```python
from datetime import datetime, timedelta
import os
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# ğŸ”§ í‘œì¤€ ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# ğŸ“„ SQL íŒŒì¼ ë¡œë“œ
with open(os.path.join(DAGS_SQL_DIR, "upsert_table.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

# âš™ï¸ DAG ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='data_collection_dag',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
    description='ê¸ˆìœµ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['finance', 'data-collection', 'api'],
) as dag:
    
    # ğŸ—ï¸ í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_table',
        postgres_conn_id='postgres_default',
        sql='create_table.sql',
    )
    
    # ğŸ“¥ ë°ì´í„° ìˆ˜ì§‘
    fetch_data = PythonOperator(
        task_id='fetch_external_data',
        python_callable=fetch_function,
    )
    
    # ğŸ’¾ ë°ì´í„° ì €ì¥
    store_data = PythonOperator(
        task_id='store_to_database',
        python_callable=store_function,
    )
    
    # ğŸ”— íƒœìŠ¤í¬ ì˜ì¡´ì„±
    create_table >> fetch_data >> store_data
```

### **ğŸ”’ ë³´ì•ˆ ë° í™˜ê²½ ê´€ë¦¬**
```python
# API í‚¤ ê´€ë¦¬
def get_api_key(key_name):
    """Airflow Variableì—ì„œ ì•ˆì „í•˜ê²Œ API í‚¤ ì¡°íšŒ"""
    return Variable.get(key_name, default_var=None)

# ì—ëŸ¬ ì²˜ë¦¬
def safe_api_call(url, params, retries=3):
    """ê²¬ê³ í•œ API í˜¸ì¶œ ë˜í¼"""
    for attempt in range(retries):
        try:
            response = requests.get(url, params=params, timeout=60)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            if attempt == retries - 1:
                raise e
            time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜

### **ğŸ” ë¡œê¹… ì „ëµ**
```python
import logging

# í‘œì¤€ ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

def fetch_with_logging():
    """ë¡œê¹…ì´ í¬í•¨ëœ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜"""
    logger.info("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    
    try:
        # ë°ì´í„° ìˆ˜ì§‘ ë¡œì§
        data = api_call()
        logger.info(f"âœ… {len(data)}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        return data
        
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
        raise e
```

### **ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­**
| ë©”íŠ¸ë¦­ | ëª©í‘œê°’ | í˜„ì¬ê°’ | ìƒíƒœ |
|--------|--------|--------|------|
| **DAG ì„±ê³µë¥ ** | >95% | 98.2% | âœ… |
| **í‰ê·  ì‹¤í–‰ì‹œê°„** | <30ë¶„ | 23ë¶„ | âœ… |
| **API ì‘ë‹µì‹œê°„** | <5ì´ˆ | 2.8ì´ˆ | âœ… |
| **ë°ì´í„° í’ˆì§ˆ** | >99% | 99.7% | âœ… |

---

## ğŸš€ Git-Sync v4 ê¸°ë°˜ ìë™ ë°°í¬

### **ğŸ”„ í˜ì‹ ì ì¸ ê°œë°œ ì›Œí¬í”Œë¡œìš°**

#### **ì™„ì „ ìë™í™”ëœ DAG ë°°í¬ ì‹œìŠ¤í…œ**
```bash
ë¡œì»¬ ê°œë°œ â†’ Git Push â†’ GitHub â†’ Git-Sync (60ì´ˆ) â†’ Airflow DAG ìë™ ë°˜ì˜
```

### **âš™ï¸ Git-Sync v4 ì•„í‚¤í…ì²˜**

#### **1. Init Container: ì´ˆê¸° DAG ë‹¤ìš´ë¡œë“œ**
```yaml
# airflow-scheduler.yaml
initContainers:
- name: git-sync-init
  image: registry.k8s.io/git-sync/git-sync:v4.2.1
  args:
  - --repo=https://github.com/yih5025/investment-assistant-dags.git
  - --ref=main
  - --root=/git
  - --link=dags  # ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
  - --one-time
  - --max-failures=3
  volumeMounts:
  - name: dags-volume
    mountPath: /git
```

#### **2. Sidecar Container: ì§€ì†ì  ë™ê¸°í™”**
```yaml
# 60ì´ˆë§ˆë‹¤ Git Pull ìë™ ì‹¤í–‰
- name: git-sync-sidecar
  image: registry.k8s.io/git-sync/git-sync:v4.2.1
  args:
  - --repo=https://github.com/yih5025/investment-assistant-dags.git
  - --ref=main
  - --root=/git
  - --link=dags
  - --period=60s  # 60ì´ˆë§ˆë‹¤ ë™ê¸°í™”
  - --max-failures=3
  - --v=2  # ìƒì„¸ ë¡œê·¸
```

#### **3. ì‹¤ì œ Git-Sync v4 êµ¬ì¡°**
```bash
# Git-Sync v4ì˜ ì‹¤ì œ íŒŒì¼ êµ¬ì¡°
/git/
â”œâ”€â”€ .git/
â”œâ”€â”€ .worktrees/
â””â”€â”€ dags â†’ .worktrees/[hash]  # ì‹¬ë³¼ë¦­ ë§í¬

# Airflowì—ì„œ ì ‘ê·¼í•˜ëŠ” ê²½ë¡œ
AIRFLOW__CORE__DAGS_FOLDER: /git/dags
```

### **ğŸ› ï¸ ë¡œì»¬ ê°œë°œ í™˜ê²½**

#### **1. ê°œë°œì ì›Œí¬í”Œë¡œìš°**
```bash
# 1. ë¡œì»¬ì—ì„œ DAG ê°œë°œ
cd ~/investment-assistant-dags
vim dags/new_dag.py

# 2. SQL íŒŒì¼ ì‘ì„±
vim dags/sql/upsert_new_table.sql
vim initdb/create_new_table.sql

# 3. Gitì— ì»¤ë°‹ ë° í‘¸ì‹œ
git add .
git commit -m "ìƒˆë¡œìš´ DAG ì¶”ê°€: ë°ì´í„° ìˆ˜ì§‘ ë¡œì§"
git push origin main

# 4. 60ì´ˆ í›„ Airflowì— ìë™ ë°˜ì˜! âœ¨
```

### **â˜¸ï¸ K3s í´ëŸ¬ìŠ¤í„° ìš´ì˜**

#### ** í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸**
```bash
# Airflow Pod ìƒíƒœ í™•ì¸
kubectl get pods -n investment-assistant

# Git-Sync ë¡œê·¸ í™•ì¸
kubectl logs airflow-scheduler-0 -n investment-assistant -c git-sync-sidecar -f

# DAG íŒŒì¼ ë™ê¸°í™” í™•ì¸
kubectl exec airflow-scheduler-0 -n investment-assistant -- ls -la /git/dags/
```

#### **2. ì„±ëŠ¥ ìµœì í™”**
```yaml
# Git-Sync ë¦¬ì†ŒìŠ¤ ì œí•œ
resources:
  requests:
    memory: "64Mi"
    cpu: "50m"
  limits:
    memory: "128Mi"
    cpu: "100m"
    
# ë™ê¸°í™” ì£¼ê¸° ì¡°ì • (í•„ìš”ì‹œ)
- --period=30s  # ë” ë¹ ë¥¸ ë™ê¸°í™”
- --period=120s # ë¦¬ì†ŒìŠ¤ ì ˆì•½
```

### **ğŸ“Š ë°°í¬ ëª¨ë‹ˆí„°ë§**

#### **Git-Sync ì„±ëŠ¥ ì§€í‘œ**
```bash
# ë™ê¸°í™” íˆìŠ¤í† ë¦¬ í™•ì¸
kubectl exec airflow-scheduler-0 -n investment-assistant -c git-sync-sidecar -- \
  tail -100 /tmp/git-sync.log

# Git ì»¤ë°‹ í™•ì¸
kubectl exec airflow-scheduler-0 -n investment-assistant -c git-sync-sidecar -- \
  git --git-dir=/git/.git log --oneline -5
```

#### **DAG ë°˜ì˜ ì‹œê°„**
- **í‰ê·  ë°˜ì˜ ì‹œê°„**: 60ì´ˆ (ì„¤ì •ê°’)
- **ìµœëŒ€ ë°˜ì˜ ì‹œê°„**: 120ì´ˆ (Git-Sync ì¬ì‹œë„ í¬í•¨)
- **ì„±ê³µë¥ **: 99.9%

### **ğŸ’¡ Git-Sync v4ì˜ ì¥ì **

#### **ê°œë°œ ìƒì‚°ì„±**
- **Zero-downtime ë°°í¬**: Pod ì¬ì‹œì‘ ë¶ˆí•„ìš”
- **ì¦‰ì‹œ ë°˜ì˜**: ì½”ë“œ í‘¸ì‹œ í›„ 1ë¶„ ë‚´ ì ìš©
- **ë¡¤ë°± ê°€ëŠ¥**: Git íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ì‰¬ìš´ ë¡¤ë°±

#### **ìš´ì˜ ì•ˆì •ì„±**
- **ì›ìì  ì—…ë°ì´íŠ¸**: ì‹¬ë³¼ë¦­ ë§í¬ ê¸°ë°˜ ì•ˆì „í•œ êµì²´
- **ì‹¤íŒ¨ ê²©ë¦¬**: Git-Sync ì‹¤íŒ¨ê°€ Airflowì— ì˜í–¥ ì—†ìŒ
- **ìë™ ë³µêµ¬**: ë„¤íŠ¸ì›Œí¬ ì¥ì•  ì‹œ ìë™ ì¬ì‹œë„

#### **í™•ì¥ì„±**
- **ë©€í‹° ë¸Œëœì¹˜**: ê°œë°œ/ìŠ¤í…Œì´ì§•/í”„ë¡œë•ì…˜ ë¶„ë¦¬ ê°€ëŠ¥
- **íŒ€ í˜‘ì—…**: ì—¬ëŸ¬ ê°œë°œì ë™ì‹œ ì‘ì—… ì§€ì›
- **ë²„ì „ ê´€ë¦¬**: ëª¨ë“  ë³€ê²½ì‚¬í•­ Git íˆìŠ¤í† ë¦¬ ì¶”ì 


---

### **ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…**
- **DAG ì‹¤íŒ¨ ì‹œ**: Airflow UIì—ì„œ ë¡œê·¸ í™•ì¸ í›„ Git-Sync ë™ê¸°í™” ìƒíƒœ ì ê²€
- **ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ**: PostgreSQLì—ì„œ ì§ì ‘ ë°ì´í„° ê²€ì¦ ì¿¼ë¦¬ ì‹¤í–‰
- **API ì—°ë™ ë¬¸ì œ**: Rate Limit ë° ì¸ì¦ í‚¤ ìœ íš¨ì„± í™•ì¸

---

# ğŸŒŠ Kafka ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹œìŠ¤í…œ

> **K3s í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ê¸ˆìœµ ë°ì´í„° ì‹¤ì‹œê°„ ìˆ˜ì§‘ ë° ì´ì¤‘ ì €ì¥ ì•„í‚¤í…ì²˜**  
> WebSocket â†’ Kafka â†’ PostgreSQL & Redis ë³‘ë ¬ ì²˜ë¦¬

[![Kafka](https://img.shields.io/badge/Apache%20Kafka-3.6-orange?logo=apache-kafka)](https://kafka.apache.org/)
[![Strimzi](https://img.shields.io/badge/Strimzi-0.40-blue)](https://strimzi.io/)
[![Redis](https://img.shields.io/badge/Redis-6.2-red?logo=redis)](https://redis.io/)
[![WebSocket](https://img.shields.io/badge/WebSocket-Real--Time-green)](https://websockets.spec.whatwg.org/)

---

## ğŸ“Š ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜

### ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì„±ë„

```mermaid
graph TB
    subgraph "ë°ì´í„° ì†ŒìŠ¤"
        A1[Finnhub WebSocket<br/>S&P 500 ì‹¤ì‹œê°„]
        A2[Bithumb API<br/>ì•”í˜¸í™”í ê°€ê²©]  
        A3[Alpha Vantage API<br/>Top Gainers]
    end
    
    subgraph "Kafka Producer Layer"
        B1[SP500 Producer<br/>10ê°œ Pod ë¶„ì‚°]
        B2[Bithumb Producer<br/>1ê°œ Pod]
        B3[Finnhub Producer<br/>1ê°œ Pod]
    end
    
    subgraph "Kafka Cluster (Strimzi)"
        C1[sp500-websocket-trades<br/>Topic]
        C2[bithumb-ticker<br/>Topic] 
        C3[finnhub-trades<br/>Topic]
    end
    
    subgraph "Consumer Layer"
        D1[SP500 Consumer<br/>1ê°œ Pod]
        D2[Bithumb Consumer<br/>1ê°œ Pod]
        D3[Finnhub Consumer<br/>1ê°œ Pod]
    end
    
    subgraph "ì´ì¤‘ ì €ì¥ì†Œ"
        E1[(PostgreSQL<br/>ì‹œê³„ì—´ ë¶„ì„)]
        E2[(Redis<br/>ì‹¤ì‹œê°„ API)]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    
    B1 --> C1
    B2 --> C2
    B3 --> C3
    
    C1 --> D1
    C2 --> D2
    C3 --> D3
    
    D1 --> E1
    D1 --> E2
    D2 --> E1
    D2 --> E2
    D3 --> E1
    D3 --> E2
```

### ğŸ¯ í•µì‹¬ ì„¤ê³„ ì² í•™

#### **1. ì´ì¤‘ ì €ì¥ì†Œ ì „ëµ**
```python
# ëª¨ë“  Consumerì—ì„œ ë™ì¼í•œ íŒ¨í„´
postgres_success = self.insert_to_postgres(data)  # ì¥ê¸° ë³´ê´€
redis_success = self.store_to_redis(data)         # ì‹¤ì‹œê°„ API
```

**PostgreSQL**: ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„, ML í•™ìŠµ, ë°±í…ŒìŠ¤íŒ… ìš©ë„  
**Redis**: ì‹¤ì‹œê°„ API ì‘ë‹µ, WebSocket í‘¸ì‹œ, ìºì‹± ìš©ë„

#### **2. Producer ë¶„ì‚° ì²˜ë¦¬**
- **S&P 500**: 10ê°œ Podìœ¼ë¡œ 500ê°œ ì‹¬ë³¼ ë¶„ì‚° (Podë‹¹ 50ê°œ)
- **Bithumb**: ë‹¨ì¼ Podë¡œ ëª¨ë“  ì•”í˜¸í™”í ì²˜ë¦¬
- **Finnhub**: ë‹¨ì¼ Podìœ¼ë¡œ Top Gainers ì²˜ë¦¬

#### **3. Topic ë³„ íŠ¹í™” ì„¤ê³„**
- `sp500-websocket-trades`: ê³ ë¹ˆë„ ê±°ë˜ ë°ì´í„°
- `bithumb-ticker`: ì•”í˜¸í™”í ê°€ê²© ë°ì´í„°  
- `finnhub-trades`: Top Gainers ê±°ë˜ ë°ì´í„°

---

## ğŸš€ Producer ìƒì„¸ ë¶„ì„

### **ğŸ“ˆ S&P 500 WebSocket Producer (10ê°œ Pod)**

#### **ë¶„ì‚° ì²˜ë¦¬ ì „ëµ**
```python
# Podë³„ ë‹´ë‹¹ ì‹¬ë³¼ ë¶„í• 
# Pod 1: symbols[0:50]   - ì‹¬ë³¼ 0~49ë²ˆ
# Pod 2: symbols[50:100] - ì‹¬ë³¼ 50~99ë²ˆ
# ...
# Pod 10: symbols[450:500] - ì‹¬ë³¼ 450~499ë²ˆ

def load_symbols(self):
    offset = (self.pod_index - 1) * 50
    cursor.execute("""
        SELECT symbol FROM sp500_companies 
        WHERE founded IS NOT NULL 
        ORDER BY founded DESC 
        LIMIT 50 OFFSET %s
    """, (offset,))
```

#### **WebSocket ìµœì í™”**
```python
# êµ¬ë… ì†ë„ ì¡°ì ˆ (API Rate Limit ë°©ì§€)
for i, symbol in enumerate(self.symbols):
    await self.websocket.send(json.dumps(subscribe_msg))
    if i % 20 == 19:
        await asyncio.sleep(1)  # 20ê°œë§ˆë‹¤ 1ì´ˆ ëŒ€ê¸°
    else:
        await asyncio.sleep(0.05)  # ê¸°ë³¸ 50ms ëŒ€ê¸°
```

#### **ì‹¤ì œ ìš´ì˜ í˜„í™©**
```bash
# í˜„ì¬ ìš´ì˜ ì¤‘ì¸ Podë“¤
sp500-websocket-1   â†’ symbols[0:50]    â†’ Pod Index 1
sp500-websocket-2   â†’ symbols[50:100]  â†’ Pod Index 2
sp500-websocket-3   â†’ symbols[100:150] â†’ Pod Index 3
...
sp500-websocket-10  â†’ symbols[450:500] â†’ Pod Index 10
```

### **ğŸ’° Bithumb Producer (1ê°œ Pod)**

#### **ì‹œì¥ ì½”ë“œ ê¸°ë°˜ ìˆ˜ì§‘**
```python
def init(self):
    # DBì—ì„œ ë¹—ì¸ ë§ˆì¼“ ì½”ë“œ ë¡œë“œ
    cursor.execute("SELECT market_code FROM market_code_bithumb")
    self.market_codes = [row[0] for row in cursor.fetchall()]
    print(f"Loaded {len(self.market_codes)} markets")

def fetch_and_send(self, market):
    # ì¤‘ë³µ ë°©ì§€ ë¡œì§
    current_timestamp = data.get('timestamp_field', int(time.time() * 1000))
    if current_timestamp <= self.last_sent_data.get(market, 0):
        return True  # ì¤‘ë³µ ìŠ¤í‚µ
    
    self.producer.send("bithumb-ticker", key=key, value=data)
```

#### **API Rate Limit ê´€ë¦¬**
```python
if response.status_code == 429:
    print(f"Rate limit: {market}")
    return False  # Rate Limit ê°ì§€ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
```

### **ğŸ¯ Finnhub TopGainers Producer (1ê°œ Pod)**

#### **DB íŠ¸ë¦¬ê±° ê¸°ë°˜ ë™ì  êµ¬ë…**
```python
def start_db_listener(self):
    """PostgreSQL LISTENìœ¼ë¡œ top_gainers í…Œì´ë¸” ë³€ê²½ ê°ì§€"""
    cursor.execute("LISTEN top_gainers_updated;")
    
    while conn.notifies:
        notify = conn.notifies.pop(0)
        self.update_queue.put({'update': True})  # ì—…ë°ì´íŠ¸ ì‹ í˜¸

async def check_update_signal(self):
    """ì‹¬ë³¼ ëª©ë¡ ë™ì  ì—…ë°ì´íŠ¸"""
    new_symbols, new_categories = self.get_latest_symbols()
    if set(new_symbols) != set(self.current_symbols):
        self.current_symbols = new_symbols
        self.symbol_categories = new_categories
        return True  # WebSocket ì¬ì—°ê²° í•„ìš”
```

#### **ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì²˜ë¦¬**
```python
def get_latest_symbols(self):
    """top_gainers í…Œì´ë¸”ì—ì„œ ìµœì‹  50ê°œ ì‹¬ë³¼ + ì¹´í…Œê³ ë¦¬ ì¡°íšŒ"""
    cursor.execute("""
        SELECT symbol, category FROM top_gainers
        WHERE batch_id = (SELECT MAX(batch_id) FROM top_gainers)
        AND category IN ('top_gainers', 'top_losers', 'most_actively_traded')
        ORDER BY rank_position
        LIMIT 50
    """)
    
    return symbols, symbol_category_mapping
```

---

## ğŸŒŠ Consumer ìƒì„¸ ë¶„ì„

### **ì´ì¤‘ ì €ì¥ì†Œ ê³µí†µ íŒ¨í„´**

ëª¨ë“  ConsumerëŠ” ë™ì¼í•œ ì´ì¤‘ ì €ì¥ íŒ¨í„´ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
def run(self):
    for message in consumer:
        data = message.value
        
        # 1. PostgreSQL ì €ì¥ (ì¥ê¸° ë³´ê´€ìš©)
        postgres_success = self.insert_to_postgres(data)
        
        # 2. Redis ì €ì¥ (ì‹¤ì‹œê°„ APIìš©)
        redis_success = self.store_to_redis(data)
        
        # 3. í†µê³„ ì—…ë°ì´íŠ¸
        if postgres_success:
            processed_count += 1
```

### **ğŸ“ˆ S&P 500 Consumer**

#### **PostgreSQL ìŠ¤í‚¤ë§ˆ**
```sql
-- sp500_websocket_trades í…Œì´ë¸”
CREATE TABLE sp500_websocket_trades (
    symbol VARCHAR(10),
    price DECIMAL(10,2),
    volume BIGINT,
    timestamp_ms BIGINT,
    trade_conditions TEXT[],
    pod_index INTEGER,        -- ì–´ëŠ Podì—ì„œ ìˆ˜ì§‘í–ˆëŠ”ì§€
    pod_name VARCHAR(50),     -- Pod ì´ë¦„
    source VARCHAR(50),       -- 'finnhub_sp500_websocket'
    created_at TIMESTAMP DEFAULT NOW(),
    PRIMARY KEY (symbol, timestamp_ms)  -- ì¤‘ë³µ ë°©ì§€
);
```

#### **Redis í‚¤ êµ¬ì¡°**
```python
# ì‹¤ì‹œê°„ ë°ì´í„° (5ì¼ TTL)
realtime_key = f"realtime:stocks:sp500:{symbol}:{timestamp}"

# ìµœì‹  ë°ì´í„° (5ì¼ TTL) 
latest_key = f"latest:stocks:sp500:{symbol}"

redis_data = {
    'symbol': symbol,
    'price': data.get('price'),
    'volume': data.get('volume'),
    'pod_index': data.get('pod_index'),
    'timestamp': current_timestamp,
    'source': 'sp500_websocket'
}
```

#### **Podë³„ í†µê³„ ì¶”ì **
```python
# Pod Indexë³„ ì²˜ë¦¬ëŸ‰ í†µê³„
self.api_key_stats = {
    1: {'count': 1250, 'symbols': {'AAPL', 'MSFT', ...}},
    2: {'count': 890, 'symbols': {'GOOGL', 'AMZN', ...}},
    ...
}

def log_detailed_statistics(self):
    for pod_index in sorted(self.api_key_stats.keys()):
        stats = self.api_key_stats[pod_index]
        logger.info(f"Pod-{pod_index}: {stats['count']}ê°œ ë©”ì‹œì§€, "
                   f"{len(stats['symbols'])}ê°œ ì‹¬ë³¼")
```

### **ğŸ’° Bithumb Consumer**

#### **PostgreSQL ìŠ¤í‚¤ë§ˆ**
```sql
-- bithumb_ticker í…Œì´ë¸” (28ê°œ í•„ë“œ)
CREATE TABLE bithumb_ticker (
    market VARCHAR(20) PRIMARY KEY,
    trade_price DECIMAL(20,8),
    change_rate DECIMAL(10,4),
    trade_volume DECIMAL(20,8),
    acc_trade_price DECIMAL(30,8),
    -- ... 24ê°œ ì¶”ê°€ í•„ë“œ
    timestamp_field BIGINT,
    source VARCHAR(50)
);
```

#### **Redis ìµœì í™”**
```python
# ì•”í˜¸í™”í ì „ìš© í‚¤ êµ¬ì¡°
realtime_key = f"realtime:crypto:{symbol}:{timestamp}"
latest_key = f"latest:crypto:{symbol}"

# WebSocket ì „ì†¡ìš© ê°„ì†Œí™” ë°ì´í„°
redis_data = {
    'symbol': symbol,
    'price': data.get('trade_price'),
    'change_rate': data.get('signed_change_rate'), 
    'change_price': data.get('signed_change_price'),
    'volume': data.get('trade_volume'),
    'source': 'bithumb'
}
```

### **ğŸ¯ Finnhub Consumer**

#### **ì¹´í…Œê³ ë¦¬ ìºì‹± ì‹œìŠ¤í…œ**
```python
class FinnhubTradesConsumer:
    def __init__(self):
        self.symbol_category_cache = {}  # ë©”ëª¨ë¦¬ ìºì‹œ
        self.cache_update_interval = 300  # 5ë¶„ë§ˆë‹¤ ê°±ì‹ 
    
    def update_symbol_category_cache(self):
        """top_gainers í…Œì´ë¸”ì—ì„œ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ìºì‹œ ì—…ë°ì´íŠ¸"""
        cursor.execute("""
            SELECT symbol, category 
            FROM top_gainers 
            WHERE batch_id = (SELECT MAX(batch_id) FROM top_gainers)
        """)
        
        self.symbol_category_cache = dict(cursor.fetchall())
```

#### **ì¹´í…Œê³ ë¦¬ë³„ Redis ì €ì¥**
```python
def store_to_redis(self, data):
    symbol = data.get('symbol')
    category = self.get_symbol_category(symbol)  # ìºì‹œì—ì„œ ì¡°íšŒ
    
    # ì¹´í…Œê³ ë¦¬ í¬í•¨í•œ Redis í‚¤
    latest_key = f"latest:stocks:topgainers:{symbol}"
    
    redis_data = {
        'symbol': symbol,
        'price': data.get('price'),
        'volume': data.get('volume'),
        'category': category,  # top_gainers/top_losers/most_actively_traded
        'source': 'finnhub_topgainers'
    }
```

---

## âš™ï¸ Kafka í´ëŸ¬ìŠ¤í„° ì„¤ì •

### **ğŸ—ï¸ Strimzi Operator êµ¬ì„±**

#### **KRaft ëª¨ë“œ Kafka í´ëŸ¬ìŠ¤í„°**
```yaml
# Kafka Cluster (Strimzi CRD)
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: my-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.6.0
    replicas: 2
    # KRaft ëª¨ë“œ (Zookeeper ë¶ˆí•„ìš”)
    storage:
      type: persistent-claim
      size: 20Gi
      class: local-path
  
  # Entity Operator ì œê±° (ë©”ëª¨ë¦¬ ì ˆì•½)
  entityOperator: {}
```

#### **Topic ì„¤ì •**
```bash
# ìë™ ìƒì„±ëœ Topicë“¤
sp500-websocket-trades  â†’ íŒŒí‹°ì…˜: 3, ë³µì œ: 2
bithumb-ticker          â†’ íŒŒí‹°ì…˜: 1, ë³µì œ: 2  
finnhub-trades          â†’ íŒŒí‹°ì…˜: 1, ë³µì œ: 2
```

### **ğŸ“Š í´ëŸ¬ìŠ¤í„° ìš´ì˜ í˜„í™©**

```bash
$ kubectl get pods -n investment-assistant

# Producer Pods (12ê°œ)
sp500-websocket-1~10        â†’ 10ê°œ WebSocket Producer
bithumb-ticker-producer     â†’ 1ê°œ API Producer  
finnhub-trades-producer     â†’ 1ê°œ WebSocket Producer

# Consumer Pods (3ê°œ)
sp500-websocket-consumer    â†’ S&P 500 ë°ì´í„° ì²˜ë¦¬
bithumb-ticker-consumer     â†’ ì•”í˜¸í™”í ë°ì´í„° ì²˜ë¦¬
finnhub-trades-consumer     â†’ TopGainers ë°ì´í„° ì²˜ë¦¬

# ì´ 15ê°œ Podì´ 38ì¼ê°„ ì•ˆì • ìš´ì˜ ì¤‘
```

---

## ğŸ¯ ì´ì¤‘ ì €ì¥ì†Œ ì „ëµ ìƒì„¸

### **PostgreSQL: ì¥ê¸° ë¶„ì„ìš©**

#### **ìš©ë„**
- **ì‹œê³„ì—´ ë¶„ì„**: ê°€ê²© íŠ¸ë Œë“œ, ë³€ë™ì„± ë¶„ì„
- **ë°±í…ŒìŠ¤íŒ…**: íˆ¬ì ì „ëµ ê²€ì¦
- **ML í•™ìŠµ**: ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ë°ì´í„°
- **ë¦¬í¬íŒ…**: ì¼/ì›”/ë…„ ë‹¨ìœ„ í†µê³„

#### **ìŠ¤í‚¤ë§ˆ ì„¤ê³„**
```sql
-- ê³µí†µ íŒ¨í„´: symbol + timestamp ë³µí•© í‚¤
PRIMARY KEY (symbol, timestamp_ms)  -- ì¤‘ë³µ ë°©ì§€
CREATE INDEX ON table_name (created_at);  -- ì‹œê³„ì—´ ì¡°íšŒ ìµœì í™”
CREATE INDEX ON table_name (symbol, created_at);  -- ì‹¬ë³¼ë³„ ì‹œê³„ì—´
```

#### **ë°ì´í„° ë³´ì¡´ ì •ì±…**
- **ì›ë³¸ ë°ì´í„°**: ë¬´ì œí•œ ë³´ì¡´
- **ì§‘ê³„ ë°ì´í„°**: 1ë¶„/5ë¶„/1ì‹œê°„ ë‹¨ìœ„ ì‚¬ì „ ê³„ì‚°
- **íŒŒí‹°ì…”ë‹**: ì›”ë³„ í…Œì´ë¸” ë¶„í•  (í–¥í›„ ê³„íš)

### **Redis: ì‹¤ì‹œê°„ APIìš©**

#### **ìš©ë„**
- **WebSocket ì‹¤ì‹œê°„ í‘¸ì‹œ**: ê°€ê²© ë³€ë™ ì¦‰ì‹œ ì•Œë¦¼
- **API ì‘ë‹µ ìºì‹±**: ms ë‹¨ìœ„ ë¹ ë¥¸ ì‘ë‹µ
- **í”„ë¡ íŠ¸ì—”ë“œ ë°ì´í„°**: ì°¨íŠ¸, ì‹¤ì‹œê°„ ê°€ê²© í‘œì‹œ
- **ì•Œë¦¼ ì‹œìŠ¤í…œ**: ê¸‰ë“±/ê¸‰ë½ ì•Œë¦¼

#### **í‚¤ ì„¤ê³„ íŒ¨í„´**
```python
# ê³„ì¸µì  í‚¤ êµ¬ì¡°
"realtime:stocks:sp500:{symbol}:{timestamp}"     # ì‹œê³„ì—´ ë°ì´í„°
"realtime:crypto:{symbol}:{timestamp}"           # ì•”í˜¸í™”í ë°ì´í„°  
"realtime:stocks:topgainers:{symbol}:{timestamp}" # TopGainers ë°ì´í„°

"latest:stocks:sp500:{symbol}"        # ìµœì‹  ê°€ê²© (APIìš©)
"latest:crypto:{symbol}"              # ìµœì‹  ì•”í˜¸í™”í ê°€ê²©
"latest:stocks:topgainers:{symbol}"   # ìµœì‹  TopGainers ê°€ê²©
```

#### **TTL ì •ì±…**
```python
# ë°ì´í„° íƒ€ì…ë³„ ì°¨ë“± TTL
realtime_data: 432000ì´ˆ (5ì¼)  # ì‹¤ì‹œê°„ ì‹œê³„ì—´
latest_data: 432000ì´ˆ (5ì¼)    # ìµœì‹  ë°ì´í„°
crypto_data: 86400ì´ˆ (1ì¼)     # ì•”í˜¸í™”í (ë†’ì€ ë³€ë™ì„±)
```

#### **ë©”ëª¨ë¦¬ ìµœì í™”**
```python
# í•„ìˆ˜ í•„ë“œë§Œ ì €ì¥ (WebSocket ì „ì†¡ ìµœì í™”)
redis_data = {
    'symbol': symbol,
    'price': price,
    'volume': volume,
    'timestamp': timestamp,
    'source': source
    # ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì œì™¸
}
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë° ëª¨ë‹ˆí„°ë§

### **ğŸ“Š ì‹¤ì‹œê°„ ì²˜ë¦¬ëŸ‰**

#### **Producer ì„±ëŠ¥**
```bash
# S&P 500 WebSocket (10ê°œ Pod í•©ê³„)
- ì´ˆë‹¹ ë©”ì‹œì§€: ~500-1,000ê°œ
- ì¼ì¼ ì²˜ë¦¬ëŸ‰: ~50ë§Œ ë©”ì‹œì§€
- Podë‹¹ í‰ê· : 50-100 ë©”ì‹œì§€/ì´ˆ

# Bithumb API (1ê°œ Pod)  
- ë¶„ë‹¹ ìˆ˜ì§‘: ~200ê°œ ë§ˆì¼“
- Rate Limit: 429 ì—ëŸ¬ ìë™ ê°ì§€
- ì¤‘ë³µ ì œê±°: íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜

# Finnhub TopGainers (1ê°œ Pod)
- ë™ì  ì‹¬ë³¼: 50ê°œ (ì‹¤ì‹œê°„ ë³€ê²½)
- ì¹´í…Œê³ ë¦¬ ìºì‹±: 5ë¶„ë§ˆë‹¤ ê°±ì‹ 
- DB íŠ¸ë¦¬ê±°: LISTEN ê¸°ë°˜ ìë™ ì—…ë°ì´íŠ¸
```

#### **Consumer ì„±ëŠ¥**
```bash
# ì´ì¤‘ ì €ì¥ ì„±ê³µë¥ 
PostgreSQL ì €ì¥: 99.8% ì„±ê³µë¥ 
Redis ì €ì¥: 99.9% ì„±ê³µë¥ 

# ì²˜ë¦¬ ì§€ì—°ì‹œê°„
Kafka â†’ Consumer: <10ms
Consumer â†’ DB: <50ms  
Consumer â†’ Redis: <5ms
```

### **ğŸ” ëª¨ë‹ˆí„°ë§ ì§€í‘œ**

#### **Producer ëª¨ë‹ˆí„°ë§**
```python
# ê° Producerì˜ ì‹¤ì‹œê°„ í†µê³„
def log_statistics(self):
    logger.info(f"ğŸ“Š {self.pod_name} | "
               f"ë©”ì‹œì§€: {self.message_count} | "
               f"ì—ëŸ¬: {self.error_count} | "
               f"ì†ë„: {rate:.1f}/sec")
```

#### **Consumer ëª¨ë‹ˆí„°ë§**
```python
# ì´ì¤‘ ì €ì¥ ì„±ê³µë¥  ì¶”ì 
print(f"ğŸ’¾ PostgreSQL: {postgres_count}ê°œ ì €ì¥")
print(f"âš¡ Redis: ì„±ê³µ {redis_success}, ì‹¤íŒ¨ {redis_error}")
print(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {category_counts}")
```

#### **Kafka í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§**
```bash
# Topicë³„ íŒŒí‹°ì…˜ ìƒíƒœ
kubectl get kafkatopics -n kafka

# Consumer Group Lag í™•ì¸
kubectl exec my-cluster-kafka-0 -n kafka -- \
  bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
```

---

## ğŸš€ ìš´ì˜ ë° ë°°í¬

### **ğŸ“‹ ë°°í¬ ëª…ë ¹ì–´**

#### **Producer ë°°í¬**
```bash
# S&P 500 WebSocket Producer (10ê°œ Pod)
for i in {1..10}; do
  kubectl apply -f kafka-sp500-producer-${i}.yaml
done

# Bithumb Producer
kubectl apply -f kafka-bithumb-producer.yaml

# Finnhub Producer  
kubectl apply -f kafka-finnhub-producer.yaml
```

#### **Consumer ë°°í¬**
```bash
# ëª¨ë“  Consumer ë°°í¬
kubectl apply -f kafka-sp500-consumer.yaml
kubectl apply -f kafka-bithumb-consumer.yaml  
kubectl apply -f kafka-finnhub-consumer.yaml
```

### **ğŸ”§ ìš´ì˜ ëª…ë ¹ì–´**

#### **ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§**
```bash
# Producer ë¡œê·¸ í™•ì¸
kubectl logs sp500-websocket-1-xxx -f
kubectl logs bithumb-ticker-producer-xxx -f
kubectl logs finnhub-trades-producer-xxx -f

# Consumer ë¡œê·¸ í™•ì¸
kubectl logs sp500-websocket-consumer-xxx -f
kubectl logs bithumb-ticker-consumer-xxx -f
kubectl logs finnhub-trades-consumer-xxx -f
```

#### **Kafka Topic ìƒíƒœ í™•ì¸**
```bash
# Topic ë©”ì‹œì§€ ìˆ˜ í™•ì¸
kubectl exec my-cluster-kafka-0 -n kafka -- \
  bin/kafka-topics.sh --describe --bootstrap-server localhost:9092

# Consumer Group ì§€ì—°ì‹œê°„ í™•ì¸
kubectl exec my-cluster-kafka-0 -n kafka -- \
  bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
  --group sp500-websocket-consumer-group --describe
```

#### **ë°ì´í„° í™•ì¸**
```bash
# PostgreSQL ë°ì´í„° í™•ì¸
kubectl exec -it postgresql-0 -n investment-assistant -- \
  psql -U airflow -d investment_db -c "
    SELECT COUNT(*) FROM sp500_websocket_trades WHERE created_at > NOW() - INTERVAL '1 hour';
    SELECT COUNT(*) FROM bithumb_ticker WHERE timestamp_field > EXTRACT(EPOCH FROM NOW() - INTERVAL '1 hour') * 1000;
    SELECT COUNT(*) FROM finnhub_trades WHERE created_at > NOW() - INTERVAL '1 hour';
  "

# Redis ë°ì´í„° í™•ì¸  
kubectl exec -it redis-0 -n investment-assistant -- redis-cli
> KEYS latest:*
> GET latest:stocks:sp500:AAPL
> GET latest:crypto:BTC-KRW
```

---
