# app/services/websocket_service.py
import asyncio
import json
import logging
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime, timedelta
import hashlib

from app.database import get_db
from app.config import settings
from app.models.top_gainers_model import TopGainers
from app.models.bithumb_ticker_model import BithumbTicker
from app.models.finnhub_trades_model import FinnhubTrades
from app.schemas.websocket_schema import (
    TopGainerData, CryptoData, SP500Data,
    db_to_topgainer_data, db_to_crypto_data, db_to_sp500_data
)

logger = logging.getLogger(__name__)

class WebSocketService:
    """
    WebSocket ì‹¤ì‹œê°„ ë°ì´í„° ì„œë¹„ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì™€ Redisì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³ ,
    ë³€ê²½ ê°ì§€ ë° ìºì‹± ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """WebSocketService ì´ˆê¸°í™”"""
        self.redis_client = None
        self.last_data_cache: Dict[str, Any] = {}
        self.data_hashes: Dict[str, str] = {}
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            "db_queries": 0,
            "redis_queries": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "changes_detected": 0,
            "last_update": None,
            "errors": 0
        }
        
        logger.info("âœ… WebSocketService ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def init_redis(self) -> bool:
        """
        Redis ì—°ê²° ì´ˆê¸°í™”
        
        Returns:
            bool: Redis ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            import redis.asyncio as redis
            
            self.redis_client = redis.Redis(
                host=settings.REDIS_HOST,
                port=settings.REDIS_PORT,
                db=settings.REDIS_DB,
                password=settings.REDIS_PASSWORD,
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5
            )
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            await self.redis_client.ping()
            logger.info("âœ… Redis ì—°ê²° ì„±ê³µ")
            return True
            
        except Exception as e:
            logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨: {e} (DB fallback ëª¨ë“œ)")
            self.redis_client = None
            return False
    
    # =========================
    # Top Gainers ë°ì´í„° ì²˜ë¦¬
    # =========================
    
    async def get_topgainers_from_db(self, category: str = None, limit: int = 50) -> List[TopGainerData]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ TopGainers ë°ì´í„° ì¡°íšŒ
        
        Args:
            category: ì¹´í…Œê³ ë¦¬ í•„í„° (top_gainers, top_losers, most_actively_traded)
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            
        Returns:
            List[TopGainerData]: TopGainers ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            db = next(get_db())
            
            # ìµœì‹  batch_id ì¡°íšŒ
            latest_batch = TopGainers.get_latest_batch_id(db)
            if not latest_batch:
                logger.warning("ğŸ“Š TopGainers ë°ì´í„° ì—†ìŒ")
                return []
            
            batch_id = latest_batch[0]
            
            if category:
                # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ì¡°íšŒ
                db_objects = TopGainers.get_by_category(db, category, batch_id, limit)
            else:
                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ
                db_objects = db.query(TopGainers).filter(
                    TopGainers.batch_id == batch_id
                ).order_by(TopGainers.rank_position).limit(limit).all()
            
            # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            data = [db_to_topgainer_data(obj) for obj in db_objects]
            
            self.stats["db_queries"] += 1
            self.stats["last_update"] = datetime.utcnow()
            
            logger.debug(f"ğŸ“Š TopGainers ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(data)}ê°œ")
            return data
            
        except Exception as e:
            logger.error(f"âŒ TopGainers DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.stats["errors"] += 1
            return []
        finally:
            db.close()
    
    async def get_topgainers_from_redis(self, category: str = None, limit: int = 50) -> List[TopGainerData]:
        """
        Redisì—ì„œ TopGainers ë°ì´í„° ì¡°íšŒ
        
        Args:
            category: ì¹´í…Œê³ ë¦¬ í•„í„°
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            
        Returns:
            List[TopGainerData]: TopGainers ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if not self.redis_client:
            return await self.get_topgainers_from_db(category, limit)
        
        try:
            # Redis í‚¤ íŒ¨í„´: latest:stocks:topgainers:{symbol}
            pattern = "latest:stocks:topgainers:*"
            keys = await self.redis_client.keys(pattern)
            
            if not keys:
                logger.debug("ğŸ“Š Redis TopGainers ë°ì´í„° ì—†ìŒ, DB fallback")
                return await self.get_topgainers_from_db(category, limit)
            
            # ëª¨ë“  í‚¤ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            pipeline = self.redis_client.pipeline()
            for key in keys:
                pipeline.get(key)
            
            results = await pipeline.execute()
            
            # JSON íŒŒì‹± ë° í•„í„°ë§
            data = []
            for result in results:
                if result:
                    try:
                        json_data = json.loads(result)
                        
                        # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                        if category and json_data.get('category') != category:
                            continue
                        
                        # TopGainerData ìƒì„±
                        data.append(TopGainerData(**json_data))
                        
                    except (json.JSONDecodeError, ValueError) as e:
                        logger.warning(f"âš ï¸ Redis ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
            
            # ìˆœìœ„ë³„ ì •ë ¬ ë° ì œí•œ
            data.sort(key=lambda x: x.rank_position or 999)
            data = data[:limit]
            
            self.stats["redis_queries"] += 1
            self.stats["last_update"] = datetime.utcnow()
            
            logger.debug(f"ğŸ“Š Redis TopGainers ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(data)}ê°œ")
            return data
            
        except Exception as e:
            logger.error(f"âŒ Redis TopGainers ì¡°íšŒ ì‹¤íŒ¨: {e}, DB fallback")
            self.stats["errors"] += 1
            return await self.get_topgainers_from_db(category, limit)
    
    # =========================
    # ì•”í˜¸í™”í ë°ì´í„° ì²˜ë¦¬
    # =========================
    
    async def get_crypto_from_db(self, limit: int = 100) -> List[CryptoData]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì•”í˜¸í™”í ë°ì´í„° ì¡°íšŒ
        
        Args:
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            
        Returns:
            List[CryptoData]: ì•”í˜¸í™”í ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            db = next(get_db())
            
            # ëª¨ë“  ë§ˆì¼“ì˜ ìµœì‹  ê°€ê²© ì¡°íšŒ
            db_objects = BithumbTicker.get_all_latest_prices(db, limit)
            
            # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            data = [db_to_crypto_data(obj) for obj in db_objects]
            
            self.stats["db_queries"] += 1
            self.stats["last_update"] = datetime.utcnow()
            
            logger.debug(f"ğŸ“Š ì•”í˜¸í™”í ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(data)}ê°œ")
            return data
            
        except Exception as e:
            logger.error(f"âŒ ì•”í˜¸í™”í DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.stats["errors"] += 1
            return []
        finally:
            db.close()
    
    async def get_crypto_from_redis(self, limit: int = 100) -> List[CryptoData]:
        """
        Redisì—ì„œ ì•”í˜¸í™”í ë°ì´í„° ì¡°íšŒ
        
        Args:
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            
        Returns:
            List[CryptoData]: ì•”í˜¸í™”í ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if not self.redis_client:
            return await self.get_crypto_from_db(limit)
        
        try:
            # Redis í‚¤ íŒ¨í„´: latest:crypto:{market}
            pattern = "latest:crypto:*"
            keys = await self.redis_client.keys(pattern)
            
            if not keys:
                logger.debug("ğŸ“Š Redis ì•”í˜¸í™”í ë°ì´í„° ì—†ìŒ, DB fallback")
                return await self.get_crypto_from_db(limit)
            
            # ëª¨ë“  í‚¤ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            pipeline = self.redis_client.pipeline()
            for key in keys:
                pipeline.get(key)
            
            results = await pipeline.execute()
            
            # JSON íŒŒì‹±
            data = []
            for i, result in enumerate(results):
                if result:
                    try:
                        json_data = json.loads(result)
                        
                        # CryptoData ìƒì„± (Redis í˜•íƒœ â†’ DB í˜•íƒœ ë³€í™˜)
                        crypto_data = CryptoData(
                            market=json_data.get('symbol', keys[i].split(':')[-1]),
                            trade_price=json_data.get('price'),
                            signed_change_rate=json_data.get('change_rate'),
                            signed_change_price=json_data.get('change_price'),
                            trade_volume=json_data.get('volume'),
                            timestamp_field=json_data.get('timestamp'),
                            source=json_data.get('source', 'bithumb')
                        )
                        data.append(crypto_data)
                        
                    except (json.JSONDecodeError, ValueError) as e:
                        logger.warning(f"âš ï¸ Redis ì•”í˜¸í™”í ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
            
            # ê±°ë˜ëŸ‰ë³„ ì •ë ¬ ë° ì œí•œ
            data.sort(key=lambda x: x.trade_volume or 0, reverse=True)
            data = data[:limit]
            
            self.stats["redis_queries"] += 1
            self.stats["last_update"] = datetime.utcnow()
            
            logger.debug(f"ğŸ“Š Redis ì•”í˜¸í™”í ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(data)}ê°œ")
            return data
            
        except Exception as e:
            logger.error(f"âŒ Redis ì•”í˜¸í™”í ì¡°íšŒ ì‹¤íŒ¨: {e}, DB fallback")
            self.stats["errors"] += 1
            return await self.get_crypto_from_db(limit)
    
    # =========================
    # SP500 ë°ì´í„° ì²˜ë¦¬
    # =========================
    
    async def get_sp500_from_db(self, category: str = None, limit: int = 100) -> List[SP500Data]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ SP500 ë°ì´í„° ì¡°íšŒ
        
        Args:
            category: ì¹´í…Œê³ ë¦¬ í•„í„° (top_gainers, most_actively_traded, top_losers)
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            
        Returns:
            List[SP500Data]: SP500 ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        try:
            db = next(get_db())
            
            if category:
                # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìµœì‹  ê°€ê²©ë“¤ ì¡°íšŒ
                db_objects = FinnhubTrades.get_latest_prices(db, category=category)
            else:
                # ëª¨ë“  ì‹¬ë³¼ì˜ ìµœì‹  ê°€ê²©ë“¤ ì¡°íšŒ
                db_objects = FinnhubTrades.get_latest_prices(db)
            
            # ê²°ê³¼ ì œí•œ
            db_objects = db_objects[:limit]
            
            # Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            data = [db_to_sp500_data(obj) for obj in db_objects]
            
            self.stats["db_queries"] += 1
            self.stats["last_update"] = datetime.utcnow()
            
            logger.debug(f"ğŸ“Š SP500 ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(data)}ê°œ")
            return data
            
        except Exception as e:
            logger.error(f"âŒ SP500 DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.stats["errors"] += 1
            return []
        finally:
            db.close()
    
    async def get_sp500_from_redis(self, category: str = None, limit: int = 100) -> List[SP500Data]:
        """
        Redisì—ì„œ SP500 ë°ì´í„° ì¡°íšŒ
        
        Args:
            category: ì¹´í…Œê³ ë¦¬ í•„í„°
            limit: ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
            
        Returns:
            List[SP500Data]: SP500 ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        if not self.redis_client:
            return await self.get_sp500_from_db(category, limit)
        
        try:
            # Redis í‚¤ íŒ¨í„´: latest:stocks:sp500:{symbol}
            pattern = "latest:stocks:sp500:*"
            keys = await self.redis_client.keys(pattern)
            
            if not keys:
                logger.debug("ğŸ“Š Redis SP500 ë°ì´í„° ì—†ìŒ, DB fallback")
                return await self.get_sp500_from_db(category, limit)
            
            # ëª¨ë“  í‚¤ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            pipeline = self.redis_client.pipeline()
            for key in keys:
                pipeline.get(key)
            
            results = await pipeline.execute()
            
            # JSON íŒŒì‹± ë° í•„í„°ë§
            data = []
            for i, result in enumerate(results):
                if result:
                    try:
                        json_data = json.loads(result)
                        
                        # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                        if category and json_data.get('category') != category:
                            continue
                        
                        # SP500Data ìƒì„±
                        sp500_data = SP500Data(
                            symbol=json_data.get('symbol', keys[i].split(':')[-1]),
                            price=json_data.get('price'),
                            volume=json_data.get('volume'),
                            timestamp_ms=json_data.get('timestamp'),
                            category=json_data.get('category'),
                            source=json_data.get('source', 'finnhub_websocket')
                        )
                        data.append(sp500_data)
                        
                    except (json.JSONDecodeError, ValueError) as e:
                        logger.warning(f"âš ï¸ Redis SP500 ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
            
            # ê±°ë˜ëŸ‰ë³„ ì •ë ¬ ë° ì œí•œ
            data.sort(key=lambda x: x.volume or 0, reverse=True)
            data = data[:limit]
            
            self.stats["redis_queries"] += 1
            self.stats["last_update"] = datetime.utcnow()
            
            logger.debug(f"ğŸ“Š Redis SP500 ë°ì´í„° ì¡°íšŒ ì™„ë£Œ: {len(data)}ê°œ")
            return data
            
        except Exception as e:
            logger.error(f"âŒ Redis SP500 ì¡°íšŒ ì‹¤íŒ¨: {e}, DB fallback")
            self.stats["errors"] += 1
            return await self.get_sp500_from_db(category, limit)
    
    # =========================
    # íŠ¹ì • ì‹¬ë³¼ ë°ì´í„° ì²˜ë¦¬
    # =========================
    
    async def get_symbol_realtime_data(self, symbol: str, data_type: str = "topgainers") -> Optional[Any]:
        """
        íŠ¹ì • ì‹¬ë³¼ì˜ ì‹¤ì‹œê°„ ë°ì´í„° ì¡°íšŒ
        
        Args:
            symbol: ì¡°íšŒí•  ì‹¬ë³¼
            data_type: ë°ì´í„° íƒ€ì… (topgainers, crypto, sp500)
            
        Returns:
            Optional[Any]: ì‹¬ë³¼ ë°ì´í„° (ë°ì´í„° íƒ€ì…ì— ë”°ë¼ ë‹¤ë¦„)
        """
        symbol = symbol.upper()
        
        try:
            if data_type == "topgainers":
                return await self._get_topgainer_symbol_data(symbol)
            elif data_type == "crypto":
                return await self._get_crypto_symbol_data(symbol)
            elif data_type == "sp500":
                return await self._get_sp500_symbol_data(symbol)
            else:
                logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° íƒ€ì…: {data_type}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ ì‹¬ë³¼ {symbol} ({data_type}) ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    async def _get_topgainer_symbol_data(self, symbol: str) -> Optional[TopGainerData]:
        """TopGainers íŠ¹ì • ì‹¬ë³¼ ë°ì´í„° ì¡°íšŒ"""
        if self.redis_client:
            try:
                key = f"latest:stocks:topgainers:{symbol}"
                result = await self.redis_client.get(key)
                
                if result:
                    json_data = json.loads(result)
                    return TopGainerData(**json_data)
            except Exception as e:
                logger.warning(f"âš ï¸ Redis TopGainer ì‹¬ë³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # DB fallback
        try:
            db = next(get_db())
            db_obj = TopGainers.get_symbol_data(db, symbol)
            return db_to_topgainer_data(db_obj) if db_obj else None
        except Exception as e:
            logger.error(f"âŒ DB TopGainer ì‹¬ë³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
        finally:
            db.close()
    
    async def _get_crypto_symbol_data(self, symbol: str) -> Optional[CryptoData]:
        """ì•”í˜¸í™”í íŠ¹ì • ì‹¬ë³¼ ë°ì´í„° ì¡°íšŒ"""
        if self.redis_client:
            try:
                key = f"latest:crypto:{symbol}"
                result = await self.redis_client.get(key)
                
                if result:
                    json_data = json.loads(result)
                    return CryptoData(
                        market=symbol,
                        trade_price=json_data.get('price'),
                        signed_change_rate=json_data.get('change_rate'),
                        signed_change_price=json_data.get('change_price'),
                        trade_volume=json_data.get('volume'),
                        timestamp_field=json_data.get('timestamp'),
                        source=json_data.get('source', 'bithumb')
                    )
            except Exception as e:
                logger.warning(f"âš ï¸ Redis ì•”í˜¸í™”í ì‹¬ë³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # DB fallback
        try:
            db = next(get_db())
            db_objects = BithumbTicker.get_latest_by_market(db, symbol, limit=1)
            return db_to_crypto_data(db_objects[0]) if db_objects else None
        except Exception as e:
            logger.error(f"âŒ DB ì•”í˜¸í™”í ì‹¬ë³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
        finally:
            db.close()
    
    async def _get_sp500_symbol_data(self, symbol: str) -> Optional[SP500Data]:
        """SP500 íŠ¹ì • ì‹¬ë³¼ ë°ì´í„° ì¡°íšŒ"""
        if self.redis_client:
            try:
                key = f"latest:stocks:sp500:{symbol}"
                result = await self.redis_client.get(key)
                
                if result:
                    json_data = json.loads(result)
                    return SP500Data(
                        symbol=symbol,
                        price=json_data.get('price'),
                        volume=json_data.get('volume'),
                        timestamp_ms=json_data.get('timestamp'),
                        category=json_data.get('category'),
                        source=json_data.get('source', 'finnhub_websocket')
                    )
            except Exception as e:
                logger.warning(f"âš ï¸ Redis SP500 ì‹¬ë³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # DB fallback
        try:
            db = next(get_db())
            db_objects = FinnhubTrades.get_latest_by_symbol(db, symbol, limit=1)
            return db_to_sp500_data(db_objects[0]) if db_objects else None
        except Exception as e:
            logger.error(f"âŒ DB SP500 ì‹¬ë³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
        finally:
            db.close()
    
    # =========================
    # ë³€ê²½ ê°ì§€ ë° ìºì‹±
    # =========================
    
    def detect_changes(self, new_data: List[Any], data_type: str = "topgainers") -> Tuple[List[Any], int]:
        """
        ë°ì´í„° ë³€ê²½ ê°ì§€
        
        Args:
            new_data: ìƒˆë¡œìš´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            data_type: ë°ì´í„° íƒ€ì…
            
        Returns:
            Tuple[List[Any], int]: (ë³€ê²½ëœ ë°ì´í„°, ë³€ê²½ ê°œìˆ˜)
        """
        cache_key = f"{data_type}_last_data"
        hash_key = f"{data_type}_hash"
        
        # ìƒˆ ë°ì´í„° í•´ì‹œ ê³„ì‚°
        new_hash = self._calculate_data_hash(new_data)
        
        # ì´ì „ í•´ì‹œì™€ ë¹„êµ
        previous_hash = self.data_hashes.get(hash_key)
        
        if previous_hash == new_hash:
            # ë³€ê²½ ì—†ìŒ
            self.stats["cache_hits"] += 1
            return [], 0
        
        # ë³€ê²½ ê°ì§€ë¨
        self.data_hashes[hash_key] = new_hash
        self.last_data_cache[cache_key] = new_data
        self.stats["cache_misses"] += 1
        self.stats["changes_detected"] += 1
        
        logger.debug(f"ğŸ“Š {data_type} ë°ì´í„° ë³€ê²½ ê°ì§€: {len(new_data)}ê°œ")
        return new_data, len(new_data)
    
    def _calculate_data_hash(self, data: List[Any]) -> str:
        """
        ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì˜ í•´ì‹œ ê³„ì‚°
        
        Args:
            data: ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            str: MD5 í•´ì‹œ ê°’
        """
        try:
            # ë°ì´í„°ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (ì •ë ¬í•˜ì—¬ ì¼ê´€ì„± ë³´ì¥)
            json_str = json.dumps([
                item.dict() if hasattr(item, 'dict') else str(item) 
                for item in data
            ], sort_keys=True)
            
            # MD5 í•´ì‹œ ê³„ì‚°
            return hashlib.md5(json_str.encode()).hexdigest()
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë°ì´í„° í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return str(hash(str(data)))
    
    # =========================
    # ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì²˜ë¦¬
    # =========================
    
    async def get_dashboard_data(self) -> Dict[str, Any]:
        """
        ëŒ€ì‹œë³´ë“œìš© í†µí•© ë°ì´í„° ì¡°íšŒ
        
        Returns:
            Dict[str, Any]: ëŒ€ì‹œë³´ë“œ ë°ì´í„°
        """
        try:
            # ë³‘ë ¬ë¡œ ë°ì´í„° ì¡°íšŒ
            tasks = [
                self.get_topgainers_from_redis("top_gainers", 10),  # ìƒìœ„ 10ê°œ ìƒìŠ¹ ì£¼ì‹
                self.get_crypto_from_redis(20),                     # ìƒìœ„ 20ê°œ ì•”í˜¸í™”í
                self.get_sp500_from_redis(None, 15)                 # SP500 15ê°œ
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            top_gainers = results[0] if not isinstance(results[0], Exception) else []
            top_crypto = results[1] if not isinstance(results[1], Exception) else []
            sp500_highlights = results[2] if not isinstance(results[2], Exception) else []
            
            # ìš”ì•½ í†µê³„ ê³„ì‚°
            summary = {
                "top_gainers_count": len(top_gainers),
                "crypto_count": len(top_crypto),
                "sp500_count": len(sp500_highlights),
                "last_updated": datetime.utcnow().isoformat(),
                "data_sources": ["topgainers", "crypto", "sp500"]
            }
            
            return {
                "top_gainers": top_gainers,
                "top_crypto": top_crypto,
                "sp500_highlights": sp500_highlights,
                "summary": summary
            }
            
        except Exception as e:
            logger.error(f"âŒ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "top_gainers": [],
                "top_crypto": [],
                "sp500_highlights": [],
                "summary": {"error": str(e)}
            }
    
    # =========================
    # í—¬ìŠ¤ ì²´í¬ ë° í†µê³„
    # =========================
    
    async def health_check(self) -> Dict[str, Any]:
        """
        ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬
        
        Returns:
            Dict[str, Any]: í—¬ìŠ¤ ì²´í¬ ê²°ê³¼
        """
        health_info = {
            "timestamp": datetime.utcnow().isoformat(),
            "status": "healthy",
            "services": {}
        }
        
        # Redis ì—°ê²° ìƒíƒœ í™•ì¸
        if self.redis_client:
            try:
                await asyncio.wait_for(self.redis_client.ping(), timeout=3.0)
                health_info["services"]["redis"] = {"status": "connected", "mode": "primary"}
            except Exception as e:
                health_info["services"]["redis"] = {"status": "disconnected", "error": str(e), "mode": "fallback"}
                health_info["status"] = "degraded"
        else:
            health_info["services"]["redis"] = {"status": "not_configured", "mode": "db_only"}
            health_info["status"] = "degraded"
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸
        try:
            db = next(get_db())
            # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ DB ì—°ê²° í…ŒìŠ¤íŠ¸
            result = db.execute("SELECT 1").fetchone()
            health_info["services"]["database"] = {"status": "connected"}
            db.close()
        except Exception as e:
            health_info["services"]["database"] = {"status": "disconnected", "error": str(e)}
            health_info["status"] = "unhealthy"
        
        # ìµœê·¼ ë°ì´í„° ì—…ë°ì´íŠ¸ í™•ì¸
        last_update = self.stats.get("last_update")
        if last_update:
            time_since_update = (datetime.utcnow() - last_update).total_seconds()
            if time_since_update > 300:  # 5ë¶„ ì´ìƒ ì—…ë°ì´íŠ¸ ì—†ìŒ
                health_info["data_freshness"] = "stale"
                health_info["status"] = "degraded"
            else:
                health_info["data_freshness"] = "fresh"
        else:
            health_info["data_freshness"] = "unknown"
        
        return health_info
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        ì„œë¹„ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: í†µê³„ ì •ë³´
        """
        total_queries = self.stats["db_queries"] + self.stats["redis_queries"]
        cache_hit_rate = (
            self.stats["cache_hits"] / max(self.stats["cache_hits"] + self.stats["cache_misses"], 1) * 100
        )
        
        return {
            "performance": {
                "total_queries": total_queries,
                "db_queries": self.stats["db_queries"],
                "redis_queries": self.stats["redis_queries"],
                "cache_hit_rate": f"{cache_hit_rate:.1f}%",
                "changes_detected": self.stats["changes_detected"],
                "errors": self.stats["errors"]
            },
            "data_status": {
                "last_update": self.stats["last_update"].isoformat() if self.stats["last_update"] else None,
                "cached_datasets": len(self.last_data_cache),
                "data_hashes": len(self.data_hashes)
            },
            "health": {
                "redis_available": self.redis_client is not None,
                "error_rate": self.stats["errors"] / max(total_queries, 1) * 100
            }
        }
    
    async def cleanup_cache(self):
        """ìºì‹œ ì •ë¦¬ (ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰)"""
        try:
            # 1ì‹œê°„ ì´ìƒ ëœ ìºì‹œ ë°ì´í„° ì •ë¦¬
            cutoff_time = datetime.utcnow() - timedelta(hours=1)
            
            if self.stats.get("last_update") and self.stats["last_update"] < cutoff_time:
                self.last_data_cache.clear()
                self.data_hashes.clear()
                logger.info("ğŸ§¹ WebSocket ì„œë¹„ìŠ¤ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def shutdown(self):
        """ì„œë¹„ìŠ¤ ì¢…ë£Œ ì²˜ë¦¬"""
        try:
            if self.redis_client:
                await self.redis_client.close()
                logger.info("âœ… Redis ì—°ê²° ì¢…ë£Œ")
            
            # ìºì‹œ ì •ë¦¬
            self.last_data_cache.clear()
            self.data_hashes.clear()
            
            logger.info("âœ… WebSocketService ì¢…ë£Œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ WebSocketService ì¢…ë£Œ ì‹¤íŒ¨: {e}")