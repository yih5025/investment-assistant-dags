from datetime import datetime, timedelta
import requests
import json
import os
import time

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_x_posts.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=30),
}

# ===== Secondary ê³„ì •ë³„ í† í° í• ë‹¹ ë° ì¤‘ìš”ë„ ì„¤ì • =====
SECONDARY_ACCOUNT_CONFIG = {
    # === ì•”í˜¸í™”í ìƒíƒœê³„ ===
    'saylor': {
        'frequency': 'daily',
        'max_results': 50,
        'priority': 1,
        'category': 'crypto',
        'token': 'X_API_BEARER_TOKEN_1',
        'wait_minutes': 30
    },
    'brian_armstrong': {
        'frequency': 'every_3_days',
        'max_results': 50,
        'priority': 2,
        'category': 'crypto',
        'token': 'X_API_BEARER_TOKEN_3',
        'wait_minutes': 45
    },
    'CoinbaseAssets': {
        'frequency': 'weekly',
        'max_results': 50,
        'priority': 3,
        'category': 'crypto',
        'token': 'X_API_BEARER_TOKEN_1',
        'weekly_day': 6,  # ì¼ìš”ì¼
        'wait_minutes': 60
    },
    
    # === ì¶”ê°€ ë¹…í…Œí¬ CEOë“¤ ===
    'jeffbezos': {
        'frequency': 'every_2_days',
        'max_results': 50,
        'priority': 1,
        'category': 'tech_ceo',
        'token': 'X_API_BEARER_TOKEN_3',
        'wait_minutes': 30
    },
    'Meta': {
        'frequency': 'twice_weekly',
        'max_results': 50,
        'priority': 2,
        'category': 'tech_ceo',
        'token': 'X_API_BEARER_TOKEN_1',
        'weekly_days': [0, 3],  # ì›”, ëª©
        'wait_minutes': 45
    },
    'BitCoin': {
        'frequency': 'weekly',
        'max_results': 50,
        'priority': 3,
        'category': 'tech_ceo',
        'token': 'X_API_BEARER_TOKEN_3',
        'weekly_day': 5,  # í† ìš”ì¼
        'wait_minutes': 75
    },
    
    # === íˆ¬ìž ê¸°ê´€ & ì¸í”Œë£¨ì–¸ì„œë“¤ ===
    'Palantir': {
        'frequency': 'twice_weekly',
        'max_results': 50,
        'priority': 2,
        'category': 'institutional',
        'token': 'X_API_BEARER_TOKEN_1',
        'weekly_days': [2, 5],  # ìˆ˜, í† 
        'wait_minutes': 45
    },
    'IonQ': {
        'frequency': 'twice_weekly',
        'max_results': 50,
        'priority': 2,
        'category': 'institutional',
        'token': 'X_API_BEARER_TOKEN_3',
        'weekly_days': [1, 4],  # í™”, ê¸ˆ
        'wait_minutes': 45
    },
    'Ethereum': {
        'frequency': 'weekly',
        'max_results': 50,
        'priority': 3,
        'category': 'institutional',
        'token': 'X_API_BEARER_TOKEN_1',
        'weekly_day': 6,  # ì¼ìš”ì¼
        'wait_minutes': 90
    },
    
    # === ê¸ˆìœµ ë¯¸ë””ì–´ ===
    'CNBC': {
        'frequency': 'twice_weekly',
        'max_results': 50,
        'priority': 2,
        'category': 'media',
        'token': 'X_API_BEARER_TOKEN_3',
        'weekly_days': [0, 3],  # ì›”, ëª©
        'wait_minutes': 45
    },
    'Tether': {
        'frequency': 'weekly',
        'max_results': 50,
        'priority': 3,
        'category': 'media',
        'token': 'X_API_BEARER_TOKEN_1',
        'weekly_day': 1,  # í™”ìš”ì¼
        'wait_minutes': 60
    },
    'WSJ': {
        'frequency': 'weekly',
        'max_results': 50,
        'priority': 3,
        'category': 'media',
        'token': 'X_API_BEARER_TOKEN_3',
        'weekly_day': 2,  # ìˆ˜ìš”ì¼
        'wait_minutes': 60
    },
    
    # === ê¸°ì—… ê³µì‹ ê³„ì •ë“¤ (ë‚®ì€ ìš°ì„ ìˆœìœ„) ===
    'Tesla': {
        'frequency': 'weekly',
        'max_results': 50,
        'priority': 4,
        'category': 'corporate',
        'token': 'X_API_BEARER_TOKEN_1',
        'weekly_day': 4,  # ê¸ˆìš”ì¼
        'wait_minutes': 90
    },
    'nvidia': {
        'frequency': 'weekly',
        'max_results': 50,
        'priority': 4,
        'category': 'corporate',
        'token': 'X_API_BEARER_TOKEN_3',
        'weekly_day': 3,  # ëª©ìš”ì¼
        'wait_minutes': 90
    }
}

def get_user_id_from_db(username):
    """DBì—ì„œ usernameìœ¼ë¡œ user_id ì¡°íšŒ"""
    try:
        hook = PostgresHook(postgres_conn_id='postgres_default')
        result = hook.get_first(
            "SELECT user_id FROM x_user_profiles WHERE username = %s",
            parameters=[username]
        )
        
        if result:
            user_id = result[0]
            print(f"âœ… DB ì¡°íšŒ ì„±ê³µ: {username} â†’ {user_id}")
            return user_id
        else:
            print(f"âŒ DBì—ì„œ {username}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
            
    except Exception as e:
        print(f"âŒ DB ì¡°íšŒ ì‹¤íŒ¨: {username} - {e}")
        return None

def should_run_account_today_secondary(username):
    """Secondary ê³„ì •ë“¤ì˜ ì˜¤ëŠ˜ ì‹¤í–‰ ì—¬ë¶€ íŒë‹¨"""
    config = SECONDARY_ACCOUNT_CONFIG.get(username)
    if not config:
        return False
    
    frequency = config['frequency']
    today = datetime.now()
    day_of_year = today.timetuple().tm_yday
    day_of_week = today.weekday()
    
    if frequency == 'daily':
        return True
    elif frequency == 'every_2_days':
        # jeffbezosëŠ” ì§ìˆ˜ì¼ì— ì‹¤í–‰ (Primaryì™€ ë‹¤ë¥´ê²Œ)
        return day_of_year % 2 == 0
    elif frequency == 'every_3_days':
        # brian_armstrongëŠ” 3ì¼ë§ˆë‹¤ (ë‹¤ë¥¸ ì˜¤í”„ì…‹)
        return (day_of_year + 2) % 3 == 0
    elif frequency == 'twice_weekly':
        assigned_days = config.get('weekly_days', [0, 3])
        return day_of_week in assigned_days
    elif frequency == 'weekly':
        assigned_day = config.get('weekly_day', 6)
        return day_of_week == assigned_day
    
    return False

def get_todays_secondary_accounts():
    """ì˜¤ëŠ˜ ìˆ˜ì§‘í•  Secondary ê³„ì •ë“¤ ë°˜í™˜ (ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬)"""
    todays_accounts = []
    
    for username in SECONDARY_ACCOUNT_CONFIG.keys():
        if should_run_account_today_secondary(username):
            todays_accounts.append(username)
    
    # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
    todays_accounts.sort(key=lambda x: SECONDARY_ACCOUNT_CONFIG[x]['priority'])
    
    return todays_accounts

def call_x_api_with_flexible_token(username, user_id, max_results, token_key):
    """ì§€ì •ëœ í† í°ìœ¼ë¡œ API í˜¸ì¶œ"""
    try:
        bearer_token = Variable.get(token_key)
        
        url = f"https://api.twitter.com/2/users/{user_id}/tweets"
        
        params = {
            "max_results": min(max_results, 50),  # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ìµœëŒ€ 50ê°œ
            "tweet.fields": "created_at,text,public_metrics,context_annotations,entities,lang,edit_history_tweet_ids",
            "expansions": "author_id",
            "user.fields": "name,username,verified,public_metrics"
        }
        
        headers = {
            "Authorization": f"Bearer {bearer_token}",
            "User-Agent": f"InvestmentAssistant-Secondary-{token_key[-1]}/2.0"
        }
        
        print(f"ðŸ”‘ ì‚¬ìš© í† í°: {token_key}")
        print(f"ðŸ” API í˜¸ì¶œ ì¤‘: {username} (user_id: {user_id}) - ìµœì‹  {max_results}ê°œ íŠ¸ìœ—")
        
        response = requests.get(url, headers=headers, params=params, timeout=30)
        
        if response.status_code == 400:
            print(f"âŒ 400 Bad Request: {response.text}")
        
        response.raise_for_status()
        data = response.json()
        
        print(f"âœ… API í˜¸ì¶œ ì„±ê³µ: {username} ({token_key})")
        return data
        
    except Exception as e:
        print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {username} ({token_key}) - {e}")
        raise

def process_tweet_data_secondary(tweet, user_info, source_account, category):
    """íŠ¸ìœ— ë°ì´í„° ì²˜ë¦¬ (Secondary Tokenìš©, ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€)"""
    
    processed_data = {
        'tweet_id': tweet['id'],
        'author_id': tweet['author_id'],
        'text': tweet['text'],
        'created_at': tweet['created_at'].replace('Z', '+00:00'),
        'lang': tweet.get('lang', 'en'),
        'source_account': source_account,
        'account_category': category,
        'collection_source': 'secondary_token_improved',
    }
    
    # ì°¸ì—¬ë„ ì§€í‘œ
    metrics = tweet.get('public_metrics', {})
    processed_data.update({
        'retweet_count': metrics.get('retweet_count', 0),
        'reply_count': metrics.get('reply_count', 0),
        'like_count': metrics.get('like_count', 0),
        'quote_count': metrics.get('quote_count', 0),
        'bookmark_count': metrics.get('bookmark_count', 0),
        'impression_count': metrics.get('impression_count', 0),
    })
    
    # ì—”í‹°í‹° ì •ë³´
    entities = tweet.get('entities', {})
    processed_data.update({
        'hashtags': json.dumps(entities.get('hashtags', [])) if entities.get('hashtags') else None,
        'mentions': json.dumps(entities.get('mentions', [])) if entities.get('mentions') else None,
        'urls': json.dumps(entities.get('urls', [])) if entities.get('urls') else None,
        'cashtags': json.dumps(entities.get('cashtags', [])) if entities.get('cashtags') else None,
        'annotations': json.dumps(entities.get('annotations', [])) if entities.get('annotations') else None,
    })
    
    # ì»¨í…ìŠ¤íŠ¸ ì£¼ì„
    context_annotations = tweet.get('context_annotations', [])
    processed_data['context_annotations'] = json.dumps(context_annotations) if context_annotations else None
    
    # ì‚¬ìš©ìž ì •ë³´
    if user_info:
        user_metrics = user_info.get('public_metrics', {})
        processed_data.update({
            'username': user_info.get('username', source_account),
            'display_name': user_info.get('name', ''),
            'user_verified': user_info.get('verified', False),
            'user_followers_count': user_metrics.get('followers_count', 0),
            'user_following_count': user_metrics.get('following_count', 0),
            'user_tweet_count': user_metrics.get('tweet_count', 0),
        })
    else:
        processed_data.update({
            'username': source_account,
            'display_name': '',
            'user_verified': False,
            'user_followers_count': 0,
            'user_following_count': 0,
            'user_tweet_count': 0,
        })
    
    # íŽ¸ì§‘ ì´ë ¥
    edit_history = tweet.get('edit_history_tweet_ids', [])
    processed_data['edit_history_tweet_ids'] = json.dumps(edit_history) if edit_history else None
    
    return processed_data

def fetch_secondary_tweets_improved(**context):
    """ê°œì„ ëœ Secondary íŠ¸ìœ— ìˆ˜ì§‘ (30ë¶„+ ë”œë ˆì´, í† í° ë¶„ì‚°)"""
    
    # ì˜¤ëŠ˜ ìˆ˜ì§‘í•  ê³„ì •ë“¤ ê²°ì •
    todays_accounts = get_todays_secondary_accounts()
    
    if not todays_accounts:
        print("ðŸ“… [SECONDARY] ì˜¤ëŠ˜ì€ ìˆ˜ì§‘í•  ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤")
        context['ti'].xcom_push(key='collected_tweets', value=[])
        context['ti'].xcom_push(key='api_calls_made', value=0)
        context['ti'].xcom_push(key='category_stats', value={})
        return 0
    
    print(f"ðŸŽ¯ [SECONDARY IMPROVED] ì˜¤ëŠ˜ ìˆ˜ì§‘ ëŒ€ìƒ: {len(todays_accounts)}ê°œ ê³„ì •")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë° í† í°ë³„ ë¶„ë¥˜ ë¯¸ë¦¬ë³´ê¸°
    category_counts = {}
    token_groups = {}
    total_wait_time = 0
    
    for i, username in enumerate(todays_accounts):
        config = SECONDARY_ACCOUNT_CONFIG[username]
        category = config['category']
        token = config['token']
        wait_min = config['wait_minutes']
        
        if category not in category_counts:
            category_counts[category] = 0
        category_counts[category] += 1
        
        if token not in token_groups:
            token_groups[token] = []
        token_groups[token].append(username)
        
        print(f"   - {username}: {config['frequency']} ({category}, {token}, {wait_min}ë¶„ ê°„ê²©, ìš°ì„ ìˆœìœ„ {config['priority']})")
        if i > 0:
            total_wait_time += wait_min
    
    print(f"ðŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ê³„íš: {dict(category_counts)}")
    print(f"ðŸ“Š í† í°ë³„ ë¶„ì‚°: {dict((k, len(v)) for k, v in token_groups.items())}")
    print(f"â° ì˜ˆìƒ ì´ ì†Œìš” ì‹œê°„: {total_wait_time}ë¶„")
    
    # ì›”ê°„ API ì‚¬ìš©ëŸ‰ ì²´í¬ (40íšŒ ì •ë„ë¡œ ì œí•œ)
    if len(todays_accounts) > 8:  # í•˜ë£¨ ìµœëŒ€ 8íšŒ í˜¸ì¶œë¡œ ì œí•œ
        print(f"âš ï¸ ê²½ê³ : ì¼ì¼ í˜¸ì¶œ ì œí•œ ì ìš© - 8ê°œ ê³„ì •ìœ¼ë¡œ ì œí•œ")
        todays_accounts = todays_accounts[:8]
    
    # ê° ê³„ì •ë³„ íŠ¸ìœ— ìˆ˜ì§‘
    all_tweets = []
    total_api_calls = 0
    category_stats = {}
    successful_accounts = []
    failed_accounts = []
    token_usage = {}
    
    for i, username in enumerate(todays_accounts):
        try:
            # ë‘ ë²ˆì§¸ ê³„ì •ë¶€í„° ëŒ€ê¸°
            if i > 0:
                prev_config = SECONDARY_ACCOUNT_CONFIG[todays_accounts[i-1]]
                wait_minutes = prev_config['wait_minutes']
                
                print(f"\nâ° [RATE LIMIT] {wait_minutes}ë¶„ ëŒ€ê¸° ì¤‘... ({i+1}/{len(todays_accounts)})")
                print(f"   ë‹¤ìŒ ê³„ì •: {username}")
                
                time.sleep(wait_minutes * 60)
                
                print(f"âœ… ëŒ€ê¸° ì™„ë£Œ! {username} ìˆ˜ì§‘ ì‹œìž‘")
            
            # DBì—ì„œ user_id ì¡°íšŒ
            user_id = get_user_id_from_db(username)
            if not user_id:
                print(f"âŒ {username}: DBì—ì„œ user_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤")
                failed_accounts.append(f"{username} (user_id ì—†ìŒ)")
                continue
            
            config = SECONDARY_ACCOUNT_CONFIG[username]
            max_results = config['max_results']
            category = config['category']
            token_key = config['token']
            
            print(f"\nðŸ” [{i+1}/{len(todays_accounts)}] [{category.upper()}] {username} ìˆ˜ì§‘ ì¤‘...")
            print(f"   User ID: {user_id}")
            print(f"   ìµœëŒ€ ê²°ê³¼: {max_results}ê°œ")
            print(f"   ìš°ì„ ìˆœìœ„: {config['priority']}")
            
            # API í˜¸ì¶œ
            api_response = call_x_api_with_flexible_token(username, user_id, max_results, token_key)
            total_api_calls += 1
            
            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
            if token_key not in token_usage:
                token_usage[token_key] = 0
            token_usage[token_key] += 1
            
            if 'data' not in api_response or not api_response['data']:
                print(f"âš ï¸ {username}: ìµœê·¼ íŠ¸ìœ— ì—†ìŒ")
                successful_accounts.append(f"{username} (íŠ¸ìœ— ì—†ìŒ)")
                
                # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ (íŠ¸ìœ— ì—†ìŒ)
                if category not in category_stats:
                    category_stats[category] = {'tweets': 0, 'accounts': 0, 'api_calls': 0}
                category_stats[category]['accounts'] += 1
                category_stats[category]['api_calls'] += 1
                continue
            
            # ì‚¬ìš©ìž ì •ë³´ ì¶”ì¶œ
            user_info = {}
            if 'includes' in api_response and 'users' in api_response['includes']:
                user_info = api_response['includes']['users'][0]
            
            # íŠ¸ìœ— ë°ì´í„° ì²˜ë¦¬
            account_tweets = []
            for tweet in api_response['data']:
                processed_tweet = process_tweet_data_secondary(tweet, user_info, username, category)
                account_tweets.append(processed_tweet)
            
            all_tweets.extend(account_tweets)
            successful_accounts.append(f"{username} ({len(account_tweets)}ê°œ)")
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì—…ë°ì´íŠ¸
            if category not in category_stats:
                category_stats[category] = {'tweets': 0, 'accounts': 0, 'api_calls': 0}
            category_stats[category]['tweets'] += len(account_tweets)
            category_stats[category]['accounts'] += 1
            category_stats[category]['api_calls'] += 1
            
            print(f"âœ… {username}: {len(account_tweets)}ê°œ íŠ¸ìœ— ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ {username} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            failed_accounts.append(f"{username} ({str(e)[:50]})")
            total_api_calls += 1
            
            # ì‹¤íŒ¨ë„ í†µê³„ì— ë°˜ì˜
            config = SECONDARY_ACCOUNT_CONFIG.get(username, {})
            category = config.get('category', 'unknown')
            token_key = config.get('token', 'unknown')
            
            if category not in category_stats:
                category_stats[category] = {'tweets': 0, 'accounts': 0, 'api_calls': 0}
            category_stats[category]['api_calls'] += 1
            
            if token_key not in token_usage:
                token_usage[token_key] = 0
            token_usage[token_key] += 1
            continue
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\nðŸ“Š [SECONDARY IMPROVED] ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"   ðŸ“± ì´ íŠ¸ìœ—: {len(all_tweets)}ê°œ")
    print(f"   ðŸ”‘ ì´ API í˜¸ì¶œ: {total_api_calls}íšŒ")
    print(f"   âœ… ì„±ê³µ: {len(successful_accounts)}ê°œ ê³„ì •")
    
    if successful_accounts:
        for account in successful_accounts:
            print(f"      - {account}")
    
    if failed_accounts:
        print(f"   âŒ ì‹¤íŒ¨: {len(failed_accounts)}ê°œ ê³„ì •")
        for account in failed_accounts:
            print(f"      - {account}")
    
    print(f"ðŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ê²°ê³¼:")
    for category, stats in category_stats.items():
        print(f"   - {category}: {stats['tweets']}ê°œ íŠ¸ìœ— ({stats['accounts']}ê°œ ê³„ì •, {stats['api_calls']}íšŒ í˜¸ì¶œ)")
    
    print(f"ðŸ“ˆ í† í°ë³„ ì‚¬ìš©ëŸ‰:")
    for token, count in token_usage.items():
        print(f"   - {token}: {count}íšŒ ì‚¬ìš©")
    
    # XComì— ê²°ê³¼ ì €ìž¥
    context['ti'].xcom_push(key='collected_tweets', value=all_tweets)
    context['ti'].xcom_push(key='api_calls_made', value=total_api_calls)
    context['ti'].xcom_push(key='category_stats', value=category_stats)
    context['ti'].xcom_push(key='token_usage', value=token_usage)
    context['ti'].xcom_push(key='successful_accounts', value=successful_accounts)
    context['ti'].xcom_push(key='failed_accounts', value=failed_accounts)
    
    return len(all_tweets)

def store_secondary_tweets_to_db(**context):
    """ìˆ˜ì§‘ëœ í™•ìž¥ ê³„ì • íŠ¸ìœ—ì„ DBì— ì €ìž¥"""
    
    all_tweets = context['ti'].xcom_pull(key='collected_tweets') or []
    api_calls = context['ti'].xcom_pull(key='api_calls_made') or 0
    category_stats = context['ti'].xcom_pull(key='category_stats') or {}
    token_usage = context['ti'].xcom_pull(key='token_usage') or {}
    successful_accounts = context['ti'].xcom_pull(key='successful_accounts') or []
    failed_accounts = context['ti'].xcom_pull(key='failed_accounts') or []
    
    if not all_tweets:
        print("â„¹ï¸ ì €ìž¥í•  íŠ¸ìœ—ì´ ì—†ìŠµë‹ˆë‹¤")
        return 0
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    success_count = 0
    error_count = 0
    
    print(f"ðŸ’¾ [SECONDARY IMPROVED] DB ì €ìž¥ ì‹œìž‘: {len(all_tweets)}ê°œ íŠ¸ìœ—")
    
    for tweet_data in all_tweets:
        try:
            hook.run(UPSERT_SQL, parameters=tweet_data)
            success_count += 1
            
            if success_count % 25 == 0:
                print(f"ðŸ“Š ì €ìž¥ ì§„í–‰ë¥ : {success_count}/{len(all_tweets)}")
                
        except Exception as e:
            print(f"âŒ íŠ¸ìœ— ì €ìž¥ ì‹¤íŒ¨: {tweet_data.get('tweet_id', 'Unknown')} - {e}")
            error_count += 1
            continue
    
    print(f"âœ… [SECONDARY IMPROVED] ì €ìž¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
    
    # í†µê³„ ì¡°íšŒ ë° ìµœì¢… ë¦¬í¬íŠ¸
    try:
        result = hook.get_first("""
            SELECT COUNT(*) FROM x_posts 
            WHERE collected_at >= NOW() - INTERVAL '1 day'
            AND collection_source = 'secondary_token_improved'
        """)
        secondary_today = result[0] if result else 0
        
        # ì¹´í…Œê³ ë¦¬ë³„ DB ì €ìž¥ í†µê³„
        category_db_stats = {}
        for category in category_stats.keys():
            result = hook.get_first("""
                SELECT COUNT(*) FROM x_posts 
                WHERE collected_at >= NOW() - INTERVAL '1 day'
                AND collection_source = 'secondary_token_improved'
                AND account_category = %s
            """, parameters=[category])
            category_db_stats[category] = result[0] if result else 0
        
        print(f"\nðŸ“ˆ [ìµœì¢… ë¦¬í¬íŠ¸]")
        print(f"   ðŸ“Š ì˜¤ëŠ˜ Secondary ê°œì„  ë²„ì „ ìˆ˜ì§‘: {secondary_today}ê°œ")
        print(f"   ðŸ”¥ ì˜¤ëŠ˜ ì´ API í˜¸ì¶œ: {api_calls}íšŒ")
        print(f"   âœ… ì„±ê³µ ê³„ì •: {len(successful_accounts)}ê°œ")
        print(f"   âŒ ì‹¤íŒ¨ ê³„ì •: {len(failed_accounts)}ê°œ")
        
        print(f"\nðŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ìµœì¢… ê²°ê³¼:")
        for category, stats in category_stats.items():
            db_count = category_db_stats.get(category, 0)
            print(f"   - {category}: {stats['tweets']}ê°œ ìˆ˜ì§‘ â†’ {db_count}ê°œ DB ì €ìž¥ ({stats['api_calls']}íšŒ í˜¸ì¶œ)")
        
        print(f"\nðŸ“ˆ í† í°ë³„ ì‚¬ìš©ëŸ‰:")
        for token, count in token_usage.items():
            print(f"   - {token}: {count}íšŒ ì‚¬ìš©")
        
        # ì›”ê°„ ì‚¬ìš©ëŸ‰ ì¶”ì •
        monthly_estimated = api_calls * 30
        print(f"   ðŸ“Š ì›”ê°„ ì˜ˆìƒ ì‚¬ìš©ëŸ‰: {monthly_estimated}íšŒ (40íšŒ ëª©í‘œ)")
        
    except Exception as e:
        print(f"âš ï¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    return success_count

# DAG ì •ì˜
with DAG(
    dag_id='ingest_x_posts_secondary_improved_k8s',
    default_args=default_args,
    schedule_interval='0 5 * * *',  # ë§¤ì¼ ìƒˆë²½ 5ì‹œ ì‹¤í–‰ (Primaryì™€ 4ì‹œê°„ ì°¨ì´)
    catchup=False,
    description='X API Secondary ê°œì„ : 30ë¶„+ ë”œë ˆì´, í† í° ë¶„ì‚°, ì›” 40íšŒ ì œí•œ',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['x_api', 'twitter', 'secondary_improved', 'token_distributed', 'crypto', 'bigtech', 'k8s'],
) as dag:
    
    # í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_x_posts_table_secondary_improved',
        postgres_conn_id='postgres_default',
        sql='create_x_posts.sql',
    )
    
    # ê°œì„ ëœ í™•ìž¥ ê³„ì • íŠ¸ìœ— ìˆ˜ì§‘
    fetch_tweets = PythonOperator(
        task_id='fetch_secondary_tweets_improved',
        python_callable=fetch_secondary_tweets_improved,
    )
    
    # DB ì €ìž¥
    store_tweets = PythonOperator(
        task_id='store_secondary_tweets_to_db',
        python_callable=store_secondary_tweets_to_db,
    )
    
    # Task ì˜ì¡´ì„±
    create_table >> fetch_tweets >> store_tweets