from datetime import datetime, timedelta
import requests
import os
import time

from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.exceptions import AirflowSkipException

# í‘œì¤€ ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_market_news.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': None,
    'retry_delay': timedelta(minutes=1),
}

with DAG(
    dag_id='ingest_market_newsapi_to_db_k8s',
    default_args=default_args,
    schedule_interval='0 4 * * *',  # ë§¤ì¼ ìƒˆë²½ 4ì‹œ
    catchup=False,
    description='Comprehensive news collection via NewsAPI - business, technology, markets',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['market', 'news', 'newsapi', 'comprehensive', 'business', 'k8s'],
) as dag:

    create_table = PostgresOperator(
        task_id='create_market_news_table',
        postgres_conn_id='postgres_default',
        sql='create_market_news.sql',
    )

    def fetch_comprehensive_news(**context):
        """ê´‘ë²”ìœ„í•œ ë¹„ì¦ˆë‹ˆìŠ¤/ì‹œìž¥ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        hook = PostgresHook(postgres_conn_id='postgres_default')
        api_key = Variable.get('NEWSAPI_API_KEY')
        
        print(f"ðŸ”‘ API í‚¤ í™•ì¸: {api_key[:8]}...")
        
        # ì‹œê°„ ë²”ìœ„ ì„¤ì • (ì–´ì œ í•˜ë£¨)
        yesterday = (datetime.utcnow() - timedelta(days=1)).date()
        from_date = yesterday.isoformat()
        to_date = yesterday.isoformat()
        
        print(f"ðŸ“… ìˆ˜ì§‘ ë‚ ì§œ: {from_date}")
        
        total_articles = 0
        error_count = 0
        
        # â­ ì „ëžµ 1: ì¹´í…Œê³ ë¦¬ë³„ í—¤ë“œë¼ì¸ ìˆ˜ì§‘ (ëŒ€ëŸ‰ ìˆ˜ì§‘)
        categories = ['business', 'technology', 'general', 'politics']
        
        for category in categories:
            try:
                print(f"ðŸ“° ì¹´í…Œê³ ë¦¬ '{category}' í—¤ë“œë¼ì¸ ìˆ˜ì§‘ ì¤‘...")
                
                # NewsAPI Headlines ì—”ë“œí¬ì¸íŠ¸ (ë” ë§Žì€ ê²°ê³¼)
                resp = requests.get(
                    "https://newsapi.org/v2/top-headlines",
                    params={
                        'category': category,
                        'language': 'en',
                        'country': 'us',  # ë¯¸êµ­ ë‰´ìŠ¤
                        'apiKey': api_key,
                        'pageSize': 100,  # ìµœëŒ€ 100ê°œ
                    },
                    timeout=30
                )
                
                if resp.status_code == 429:
                    print(f"âš ï¸ Rate Limit ë„ë‹¬")
                    break
                
                resp.raise_for_status()
                data = resp.json()
                articles = data.get("articles", [])
                
                # ì–´ì œ ë‚ ì§œ í•„í„°ë§ (í—¤ë“œë¼ì¸ì€ ë‚ ì§œ í•„í„°ê°€ ì—†ìŒ)
                yesterday_articles = []
                for article in articles:
                    if article.get('publishedAt'):
                        pub_date = datetime.fromisoformat(article['publishedAt'].replace('Z', '+00:00')).date()
                        if pub_date >= yesterday - timedelta(days=1):  # ì–´ì œ ë˜ëŠ” ì˜¤ëŠ˜
                            yesterday_articles.append(article)
                
                # ì €ìž¥
                saved_count = 0
                for article in yesterday_articles:
                    try:
                        if not article.get('url'):
                            continue
                        
                        # ì¤‘ë³µ ì²´í¬
                        existing = hook.get_first("""
                            SELECT 1 FROM market_news WHERE url = %s
                        """, parameters=[article['url']])
                        
                        if existing:
                            continue
                        
                        hook.run(UPSERT_SQL, parameters={
                            'source': article["source"]["name"] if article.get("source") else "",
                            'url': article["url"],
                            'author': article.get("author", ""),
                            'title': article.get("title", ""),
                            'description': article.get("description", ""),
                            'content': article.get("content", ""),
                            'published_at': article["publishedAt"],
                        })
                        saved_count += 1
                        
                    except Exception as e:
                        print(f"âŒ ê¸°ì‚¬ ì €ìž¥ ì‹¤íŒ¨: {str(e)}")
                        continue
                
                total_articles += saved_count
                print(f"âœ… {category}: {saved_count}ê°œ ì €ìž¥ (ì „ì²´ {len(articles)}ê°œ ì¤‘)")
                
                # API í˜¸ì¶œ ê°„ê²©
                time.sleep(2.0)
                
            except Exception as e:
                print(f"âŒ ì¹´í…Œê³ ë¦¬ {category} ì‹¤íŒ¨: {str(e)}")
                error_count += 1
                continue
        
        # â­ ì „ëžµ 2: ê°„ë‹¨í•œ í‚¤ì›Œë“œë¡œ Everything ê²€ìƒ‰ (ì¶”ê°€ ìˆ˜ì§‘)
        simple_keywords = [
            'economy',          # ê²½ì œ
            'business',         # ë¹„ì¦ˆë‹ˆìŠ¤
            'technology',       # ê¸°ìˆ 
            'IPO',             # ê³µê°œìƒìž¥
            'inflation',        # ì¸í”Œë ˆì´ì…˜
            'tariff',           # ê´€ì„¸
            'trade war',        # ë¬´ì—­ ì „ìŸ
            'sanctions',        # ì œìž¬
            'war',             # ì „ìŸ
            'politics',         # ì •ì¹˜
            'election',         # ì„ ê±°
            'government policy', # ì •ë¶€ ì •ì±…
            'congress',         # ì˜íšŒ
            'diplomatic',       # ì™¸êµ
            'nuclear',          # í•µ ê´€ë ¨
            'military'          # êµ°ì‚¬
        ]
        
        print(f"\nðŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œìž‘...")
        
        for i, keyword in enumerate(simple_keywords):
            try:
                print(f"ðŸ” í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘... ({i+1}/{len(simple_keywords)})")
                
                resp = requests.get(
                    "https://newsapi.org/v2/everything",
                    params={
                        'q': keyword,
                        'from': from_date,
                        'to': to_date,
                        'sortBy': 'popularity',  # ì¸ê¸°ë„ ìˆœ
                        'language': 'en',
                        'apiKey': api_key,
                        'pageSize': 30,  # í‚¤ì›Œë“œë‹¹ 30ê°œ
                    },
                    timeout=30
                )
                
                if resp.status_code == 429:
                    print(f"âš ï¸ Rate Limit ë„ë‹¬, í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘ë‹¨")
                    break
                
                resp.raise_for_status()
                data = resp.json()
                articles = data.get("articles", [])
                
                # ì €ìž¥
                saved_count = 0
                for article in articles:
                    try:
                        if not article.get('url'):
                            continue
                        
                        # ì¤‘ë³µ ì²´í¬
                        existing = hook.get_first("""
                            SELECT 1 FROM market_news WHERE url = %s
                        """, parameters=[article['url']])
                        
                        if existing:
                            continue
                        
                        hook.run(UPSERT_SQL, parameters={
                            'source': article["source"]["name"] if article.get("source") else "",
                            'url': article["url"],
                            'author': article.get("author", ""),
                            'title': article.get("title", ""),
                            'description': article.get("description", ""),
                            'content': article.get("content", ""),
                            'published_at': article["publishedAt"],
                        })
                        saved_count += 1
                        
                    except Exception as e:
                        continue
                
                total_articles += saved_count
                print(f"âœ… '{keyword}': {saved_count}ê°œ ì €ìž¥")
                
                # API í˜¸ì¶œ ê°„ê²©
                time.sleep(1.5)
                
            except Exception as e:
                print(f"âŒ í‚¤ì›Œë“œ '{keyword}' ì‹¤íŒ¨: {str(e)}")
                error_count += 1
                continue
        
        # â­ ì „ëžµ 3: ì£¼ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ì†ŒìŠ¤ì—ì„œ ìµœì‹  ë‰´ìŠ¤ (ë³´ë„ˆìŠ¤)
        business_sources = [
            'bloomberg',
            'reuters', 
            'cnbc',
            'the-wall-street-journal',
            'business-insider',
            'financial-times'
        ]
        
        print(f"\nðŸ“º ì£¼ìš” ì†ŒìŠ¤ë³„ ìˆ˜ì§‘...")
        
        for source in business_sources:
            try:
                print(f"ðŸ“º {source} ìµœì‹  ë‰´ìŠ¤...")
                
                resp = requests.get(
                    "https://newsapi.org/v2/top-headlines",
                    params={
                        'sources': source,
                        'apiKey': api_key,
                        'pageSize': 20,
                    },
                    timeout=30
                )
                
                if resp.status_code == 429:
                    print(f"âš ï¸ Rate Limit ë„ë‹¬, ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                resp.raise_for_status()
                data = resp.json()
                articles = data.get("articles", [])
                
                # ì €ìž¥ (ì¤‘ë³µ ì œê±°)
                saved_count = 0
                for article in articles:
                    try:
                        if not article.get('url'):
                            continue
                        
                        existing = hook.get_first("""
                            SELECT 1 FROM market_news WHERE url = %s
                        """, parameters=[article['url']])
                        
                        if existing:
                            continue
                        
                        hook.run(UPSERT_SQL, parameters={
                            'source': article["source"]["name"] if article.get("source") else "",
                            'url': article["url"],
                            'author': article.get("author", ""),
                            'title': article.get("title", ""),
                            'description': article.get("description", ""),
                            'content': article.get("content", ""),
                            'published_at': article["publishedAt"],
                        })
                        saved_count += 1
                        
                    except Exception as e:
                        continue
                
                total_articles += saved_count
                print(f"âœ… {source}: {saved_count}ê°œ ì €ìž¥")
                
                time.sleep(1.0)
                
            except Exception as e:
                print(f"âŒ ì†ŒìŠ¤ {source} ì‹¤íŒ¨: {str(e)}")
                error_count += 1
                continue
        
        # ìµœì¢… í†µê³„
        result = hook.get_first("SELECT COUNT(*) FROM market_news")
        total_records = result[0] if result else 0
        
        today_added = hook.get_first("""
            SELECT COUNT(*) FROM market_news 
            WHERE fetched_at >= CURRENT_DATE
        """)
        today_count = today_added[0] if today_added else 0
        
        print(f"\nðŸ ì™„ë£Œ!")
        print(f"âœ… ì˜¤ëŠ˜ ìˆ˜ì§‘: {total_articles}ê°œ")
        print(f"ðŸ“Š ì˜¤ëŠ˜ ì „ì²´: {today_count}ê°œ, ì´ ë ˆì½”ë“œ: {total_records}ê°œ")
        print(f"âŒ ì—ëŸ¬: {error_count}ê°œ")
        
        if total_articles == 0:
            raise AirflowSkipException("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ - Rate Limit ë˜ëŠ” API ë¬¸ì œ")
        
        return total_articles

    fetch_upsert = PythonOperator(
        task_id='fetch_comprehensive_news',
        python_callable=fetch_comprehensive_news,
    )

    # Task ì˜ì¡´ì„±
    create_table >> fetch_upsert