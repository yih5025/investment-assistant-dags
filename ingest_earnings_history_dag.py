from datetime import datetime, timedelta
import yfinance as yf
import pandas as pd
import os
import time
import logging

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

# ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_earnings_history.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
}

logger = logging.getLogger(__name__)

def fetch_and_process_earnings_history(**context):
    """
    S&P 500 ì¢…ëª©ì˜ ê³¼ê±° ì‹¤ì ì¼ê³¼ ë‹¹ì‹œ ì£¼ê°€ ë°˜ì‘ì„ ìˆ˜ì§‘
    """
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # 1. ëŒ€ìƒ ì¢…ëª© ê°€ì ¸ì˜¤ê¸° (S&P 500)
    # limitì„ ê±¸ì–´ í…ŒìŠ¤íŠ¸ í›„ ì „ì²´ë¡œ í™•ì¥ ê¶Œì¥
    symbols_df = pg_hook.get_pandas_df("SELECT symbol FROM sp500_companies")
    symbols = symbols_df['symbol'].tolist()
    
    processed_count = 0
    error_count = 0
    
    print(f"ğŸš€ ì´ {len(symbols)}ê°œ ì¢…ëª©ì— ëŒ€í•œ ì‹¤ì  íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì‹œì‘")
    
    for symbol in symbols:
        try:
            ticker = yf.Ticker(symbol)
            
            # 2. ì‹¤ì  ë°œí‘œì¼ ê°€ì ¸ì˜¤ê¸°
            earnings_dates = ticker.earnings_dates
            if earnings_dates is None or earnings_dates.empty:
                continue
                
            # ìµœê·¼ 2ë…„ ë°ì´í„°ë§Œ í•„í„°ë§ (ë„ˆë¬´ ì˜¤ë˜ëœ ê±´ íŒ¨í„´ ì˜ë¯¸ ê°ì†Œ)
            cutoff_date = pd.Timestamp.now(tz='UTC') - pd.Timedelta(days=730)
            recent_earnings = earnings_dates[earnings_dates.index >= cutoff_date]
            
            for date_idx, row in recent_earnings.iterrows():
                report_dt = date_idx.to_pydatetime()
                report_date = report_dt.date()
                
                # ë¯¸ë˜ ë‚ ì§œëŠ” ìŠ¤í‚µ
                if report_date > datetime.now().date():
                    continue
                
                # 3. ë°œí‘œì¼ ì „í›„ ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (D-5 ~ D+5)
                # ë„‰ë„‰í•˜ê²Œ ê°€ì ¸ì™€ì„œ ì „ì¼ ì¢…ê°€ì™€ ë‹¹ì¼ ì‹œê°€ë¥¼ ì°¾ìŒ
                start_d = report_date - timedelta(days=5)
                end_d = report_date + timedelta(days=5)
                
                hist = ticker.history(start=start_d, end=end_d)
                
                if hist.empty:
                    continue
                
                # Timezone ì œê±°
                hist.index = hist.index.date
                
                if report_date not in hist.index:
                    # ë°œí‘œì¼ ë‹¹ì¼ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ (íœ´ì¥ì¼ ë“±) ìŠ¤í‚µ
                    continue
                    
                # ë°ì´í„° ìœ„ì¹˜ ì°¾ê¸°
                try:
                    idx_loc = hist.index.get_loc(report_date)
                    if idx_loc == 0: continue # ì „ì¼ ë°ì´í„° ì—†ìŒ
                    
                    # D-1 (ë°œí‘œ ì „ì¼)
                    prev_day_row = hist.iloc[idx_loc - 1]
                    price_before_close = float(prev_day_row['Close'])
                    
                    # D-Day (ë°œí‘œ ë‹¹ì¼ - ì‹œì¥ ë°˜ì‘)
                    # ì°¸ê³ : ì¥ ë§ˆê° í›„(AMC) ë°œí‘œì¸ ê²½ìš° ë‹¤ìŒë‚  ì‹œê°€ë¥¼ ë´ì•¼ í•˜ì§€ë§Œ,
                    # yfinanceì—ëŠ” ì‹œê°„ ì •ë³´ê°€ ë¶€ì •í™•í•  ë•Œê°€ ë§ì•„ ì¼ë‹¨ ë‹¹ì¼ ë³€ë™ì„±ìœ¼ë¡œ ê·¼ì‚¬ì¹˜ ê³„ì‚°
                    day_row = hist.iloc[idx_loc]
                    price_open = float(day_row['Open'])
                    price_close = float(day_row['Close'])
                    
                    # ì§€í‘œ ê³„ì‚°
                    gap_pct = ((price_open - price_before_close) / price_before_close) * 100
                    move_pct = ((price_close - price_before_close) / price_before_close) * 100
                    
                    # EPS ì •ë³´
                    eps_est = row.get('EPS Estimate')
                    eps_act = row.get('Reported EPS')
                    surprise = row.get('Surprise(%)')
                    
                    # DB ì €ì¥ íŒŒë¼ë¯¸í„°
                    params = {
                        'symbol': symbol,
                        'report_date': report_date,
                        'eps_estimate': float(eps_est) if pd.notna(eps_est) else None,
                        'eps_actual': float(eps_act) if pd.notna(eps_act) else None,
                        'surprise_pct': float(surprise) if pd.notna(surprise) else None,
                        'price_before_close': price_before_close,
                        'price_open': price_open,
                        'price_close': price_close,
                        'gap_pct': gap_pct,
                        'move_pct': move_pct
                    }
                    
                    pg_hook.run(UPSERT_SQL, parameters=params)
                    processed_count += 1
                    
                except Exception as e:
                    # ì¸ë±ìŠ¤ ì—ëŸ¬ ë“±ì€ ìŠ¤í‚µ
                    continue
            
            # API ì œí•œ ë°©ì§€ ë”œë ˆì´
            time.sleep(1.0)
            
            if processed_count % 50 == 0:
                print(f"ğŸ“Š ì§„í–‰ì¤‘: {processed_count}ê°œ ë ˆì½”ë“œ ì €ì¥ ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ {symbol} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
            error_count += 1
            
    print(f"âœ… ì™„ë£Œ: ì´ {processed_count}ê°œ ì‹¤ì  ê¸°ë¡ ì €ì¥, {error_count}ê°œ ì¢…ëª© ì—ëŸ¬")

with DAG(
    dag_id='ingest_earnings_history_k8s',
    default_args=default_args,
    schedule_interval='@weekly',  # ì£¼ 1íšŒ ì—…ë°ì´íŠ¸ë©´ ì¶©ë¶„ (ê³¼ê±° ë°ì´í„°ì´ë¯€ë¡œ)
    catchup=False,
    template_searchpath=[INITDB_SQL_DIR],
    tags=['earnings', 'history', 'yfinance', 'analysis'],
) as dag:

    # 1. í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_earnings_history_table',
        postgres_conn_id='postgres_default',
        sql='create_earnings_history.sql',
    )

    # 2. ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
    ingest_data = PythonOperator(
        task_id='fetch_and_store_earnings_history',
        python_callable=fetch_and_process_earnings_history,
    )

    create_table >> ingest_data