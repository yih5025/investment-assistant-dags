# social_media_analysis_dag_optimized.py
from airflow import DAG
from airflow.decorators import task, dag
from airflow.hooks.postgres_hook import PostgresHook
from datetime import datetime, timedelta
import json
import logging

try:
    from utils.asset_matcher import SocialMediaAnalyzer
    from utils.market_data_collector import MarketDataCollector
except ImportError:
    from dags.utils.asset_matcher import SocialMediaAnalyzer
    from dags.utils.market_data_collector import MarketDataCollector

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'investment_assistant',
    'depends_on_past': False,
    'start_date': datetime(2025, 9, 21),
    'retries': None,
    'retry_delay': timedelta(minutes=2)
}

# ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
BATCH_SIZE = 100  # ê° ì†ŒìŠ¤ë³„ 100ê°œì”©

@dag(
    dag_id='batch_processing_social_media_market_analysis',
    default_args=default_args,
    description='ì†Œì…œë¯¸ë””ì–´ ê²Œì‹œê¸€ ì‹œì¥ ì˜í–¥ ë¶„ì„ - ë°°ì¹˜ ì²˜ë¦¬',
    schedule_interval='0 * * * *', 
    catchup=False,
    max_active_runs=1,
    tags=['social_media', 'market_analysis', 'batch_processing']
)
def social_media_analysis_dag():
    
    @task
    def get_processing_status():
        """í˜„ì¬ ì²˜ë¦¬ ì§„í–‰ ìƒí™© í™•ì¸"""
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        status_queries = {
            'x': """
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN tweet_id IN (
                        SELECT post_id FROM post_analysis_cache WHERE post_source = 'x'
                    ) THEN 1 END) as processed
                FROM x_posts
            """,
            'truth_social_posts': """
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN id IN (
                        SELECT post_id FROM post_analysis_cache WHERE post_source = 'truth_social_posts'
                    ) THEN 1 END) as processed
                FROM truth_social_posts
            """,
            'truth_social_trends': """
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN id IN (
                        SELECT post_id FROM post_analysis_cache WHERE post_source = 'truth_social_trends'
                    ) THEN 1 END) as processed
                FROM truth_social_trends
            """
        }
        
        status = {}
        for source, query in status_queries.items():
            result = pg_hook.get_first(query)
            total, processed = result
            remaining = total - processed
            status[source] = {
                'total': total,
                'processed': processed,
                'remaining': remaining,
                'has_work': remaining > 0
            }
        
        logger.info(f"Processing status: {status}")
        return status
    
    @task
    def get_batch_posts(status):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¯¸ë¶„ì„ ê²Œì‹œê¸€ ì¡°íšŒ (ê° ì†ŒìŠ¤ë³„ 100ê°œì”©)"""
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        all_posts = []
        
        # X ê²Œì‹œê¸€ ë°°ì¹˜ ì¡°íšŒ
        if status['x']['has_work']:
            x_query = """
            SELECT tweet_id as post_id, 'x' as source, username, 
                   text as content, created_at as post_timestamp
            FROM x_posts 
            WHERE tweet_id NOT IN (
                SELECT post_id FROM post_analysis_cache WHERE post_source = 'x'
            )
            ORDER BY created_at DESC
            LIMIT %s
            """
            x_posts = pg_hook.get_records(x_query, parameters=[BATCH_SIZE])
            
            for row in x_posts:
                all_posts.append({
                    'post_id': row[0],
                    'source': row[1], 
                    'username': row[2],
                    'content': row[3],
                    'post_timestamp': row[4]
                })
            
            logger.info(f"Retrieved {len(x_posts)} X posts")
        
        # Truth Social Posts ë°°ì¹˜ ì¡°íšŒ
        if status['truth_social_posts']['has_work']:
            truth_posts_query = """
            SELECT id as post_id, 'truth_social_posts' as source, username,
                   clean_content as content, created_at as post_timestamp
            FROM truth_social_posts
            WHERE id NOT IN (
                SELECT post_id FROM post_analysis_cache WHERE post_source = 'truth_social_posts'
            )
            ORDER BY created_at DESC
            LIMIT %s
            """
            truth_posts = pg_hook.get_records(truth_posts_query, parameters=[BATCH_SIZE])
            
            for row in truth_posts:
                all_posts.append({
                    'post_id': row[0],
                    'source': row[1],
                    'username': row[2], 
                    'content': row[3],
                    'post_timestamp': row[4]
                })
            
            logger.info(f"Retrieved {len(truth_posts)} Truth Social posts")
        
        # Truth Social Trends ë°°ì¹˜ ì¡°íšŒ
        if status['truth_social_trends']['has_work']:
            truth_trends_query = """
            SELECT id as post_id, 'truth_social_trends' as source, username,
                   clean_content as content, created_at as post_timestamp
            FROM truth_social_trends
            WHERE id NOT IN (
                SELECT post_id FROM post_analysis_cache WHERE post_source = 'truth_social_trends'
            )
            ORDER BY created_at DESC
            LIMIT %s
            """
            truth_trends = pg_hook.get_records(truth_trends_query, parameters=[BATCH_SIZE])
            
            for row in truth_trends:
                all_posts.append({
                    'post_id': row[0],
                    'source': row[1],
                    'username': row[2], 
                    'content': row[3],
                    'post_timestamp': row[4]
                })
            
            logger.info(f"Retrieved {len(truth_trends)} Truth Social trends")
        
        total_posts = len(all_posts)
        logger.info(f"Total posts in this batch: {total_posts}")
        
        return all_posts
    
    @task
    def check_if_work_needed(status):
        """ì²˜ë¦¬í•  ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        has_work = any(s['has_work'] for s in status.values())
        
        if not has_work:
            logger.info("ğŸ‰ ëª¨ë“  ê²Œì‹œê¸€ ë¶„ì„ ì™„ë£Œ! DAG ì‹¤í–‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
        
        # ì§„í–‰ ìƒí™© ë¡œê·¸
        for source, info in status.items():
            if info['remaining'] > 0:
                progress_pct = (info['processed'] / info['total'] * 100) if info['total'] > 0 else 0
                logger.info(f"{source}: {info['processed']}/{info['total']} ì™„ë£Œ ({progress_pct:.1f}%), {info['remaining']}ê°œ ë‚¨ìŒ")
        
        return True
    
    @task
    def analyze_posts_batch_optimized(posts):
        """ê²Œì‹œê¸€ ë°°ì¹˜ ë¶„ì„ - ë©”ëª¨ë¦¬ ìµœì í™”"""
        if not posts:
            logger.info("No posts to analyze in this batch")
            return []
        
        analyzer = SocialMediaAnalyzer()
        collector = MarketDataCollector()
        results = []
        
        logger.info(f"ğŸš€ Starting analysis of {len(posts)} posts in this batch")
        
        for i, post in enumerate(posts):
            try:
                logger.info(f"Analyzing post {i+1}/{len(posts)}: {post['post_id']} from {post['source']}")
                
                # 1ë‹¨ê³„: ìì‚° ë§¤ì¹­ (í†µê³„ì  ë¶„ì„ ê°„ì†Œí™”)
                affected_assets = analyzer.determine_affected_assets_optimized(
                    username=post['username'],
                    content=post['content'],
                    timestamp=post['post_timestamp'],
                    post_id=post['post_id'],
                    post_source=post['source']
                )
                
                # 2ë‹¨ê³„: ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (ì œí•œì )
                market_data = collector.collect_market_data_optimized(
                    affected_assets, post['post_timestamp']
                )
                
                # ë¶„ì„ ìƒíƒœ ê²°ì •
                analysis_status = 'complete' if affected_assets else 'partial'
                
                result = {
                    'post_id': post['post_id'],
                    'post_source': post['source'],
                    'post_timestamp': post['post_timestamp'],
                    'author_username': post['username'],
                    'affected_assets': affected_assets,
                    'market_data': market_data,
                    'analysis_status': analysis_status
                }
                
                results.append(result)
                logger.info(f"âœ… Analyzed post {post['post_id']} - Found {len(affected_assets)} assets")
                
            except Exception as e:
                logger.error(f"âŒ Analysis failed for post {post['post_id']}: {e}")
                results.append({
                    'post_id': post['post_id'],
                    'post_source': post['source'],
                    'post_timestamp': post['post_timestamp'],
                    'author_username': post['username'],
                    'affected_assets': [],
                    'market_data': {},
                    'analysis_status': 'failed',
                    'error': str(e)
                })
        
        logger.info(f"âœ… Batch analysis completed: {len(results)} posts processed")
        return results
    
    @task
    def save_analysis_results_optimized(analysis_results):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥ - ë°°ì¹˜ ìµœì í™”"""
        if not analysis_results:
            logger.info("No results to save in this batch")
            return
        
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        logger.info(f"ğŸ’¾ Saving {len(analysis_results)} analysis results")
        
        # ë°°ì¹˜ ì¸ì„œíŠ¸ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        insert_data = []
        for result in analysis_results:
            insert_data.append({
                'post_id': result['post_id'],
                'post_source': result['post_source'],
                'post_timestamp': result['post_timestamp'],
                'author_username': result['author_username'],
                'affected_assets': json.dumps(result['affected_assets']),
                'market_data': json.dumps(result['market_data']),
                'analysis_status': result['analysis_status'],
                'error_message': result.get('error', None)
            })
        
        # ë°°ì¹˜ UPSERT ì‹¤í–‰
        try:
            conn = pg_hook.get_conn()
            cursor = conn.cursor()
            
            upsert_query = """
            INSERT INTO post_analysis_cache 
            (post_id, post_source, post_timestamp, author_username, 
             affected_assets, market_data, analysis_status, error_message)
            VALUES (%(post_id)s, %(post_source)s, %(post_timestamp)s, 
                    %(author_username)s, %(affected_assets)s, %(market_data)s, 
                    %(analysis_status)s, %(error_message)s)
            ON CONFLICT (post_id, post_source) 
            DO UPDATE SET 
                affected_assets = EXCLUDED.affected_assets,
                market_data = EXCLUDED.market_data,
                analysis_status = EXCLUDED.analysis_status,
                error_message = EXCLUDED.error_message,
                updated_at = NOW()
            """
            
            # executemanyë¥¼ ì‚¬ìš©í•œ ë°°ì¹˜ ì²˜ë¦¬
            cursor.executemany(upsert_query, insert_data)
            conn.commit()
            cursor.close()
            
            logger.info(f"âœ… Successfully saved {len(analysis_results)} results")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save batch results: {e}")
            raise
    
    @task.branch
    def decide_execution_branch(work_needed):
        """ì²˜ë¦¬í•  ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ë¶„ì„ ì‹¤í–‰"""
        if work_needed:
            return "analyze_posts_batch_optimized"
        else:
            return "log_completion"
    
    @task
    def log_completion():
        """ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ë¡œê·¸"""
        logger.info("ğŸ‰ ëª¨ë“  ì†Œì…œë¯¸ë””ì–´ ê²Œì‹œê¸€ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return "All processing completed"
    
    # DAG íƒœìŠ¤í¬ í”Œë¡œìš°
    status = get_processing_status()
    work_needed = check_if_work_needed(status)
    
    # ë¸Œëœì¹˜: ì‘ì—…ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    branch_decision = decide_execution_branch(work_needed)
    
    # ë¶„ì„ ë¸Œëœì¹˜
    posts = get_batch_posts(status)
    analysis_results = analyze_posts_batch_optimized(posts)
    save_results = save_analysis_results_optimized(analysis_results)
    
    # ì™„ë£Œ ë¸Œëœì¹˜  
    completion_log = log_completion()
    
    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    status >> work_needed >> branch_decision
    branch_decision >> posts >> analysis_results >> save_results
    branch_decision >> completion_log

# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
dag_instance = social_media_analysis_dag()