# social_media_analysis_dag.py - 100ê°œ ì œí•œ ë²„ì „
from airflow import DAG
from airflow.decorators import task, dag
from airflow.hooks.postgres_hook import PostgresHook
from datetime import datetime, timedelta
import json
import logging

try:
    from utils.asset_matcher import SocialMediaAnalyzer
    from utils.market_data_collector import MarketDataCollector
    from utils.market_data_collector import MarketAnalyzer
except ImportError:
    from dags.utils.asset_matcher import SocialMediaAnalyzer
    from dags.utils.market_data_collector import MarketDataCollector
    from dags.utils.market_data_collector import MarketAnalyzer

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'investment_assistant',
    'depends_on_past': False,
    'start_date': datetime(2025, 9, 21),
    'retries': None,
    'retry_delay': timedelta(minutes=2)
}

@dag(
    dag_id='batch_processing_social_media_market_analysis',
    default_args=default_args,
    description='ì†Œì…œë¯¸ë””ì–´ ê²Œì‹œê¸€ ì‹œì¥ ì˜í–¥ ë¶„ì„ - 50ê°œì”© ì œí•œ ì²˜ë¦¬',
    schedule_interval='0 */2 * * *',  # 2ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,
    tags=['social_media', 'market_analysis', 'batch_limited']
)
def social_media_analysis_dag():
    
    @task
    def get_unanalyzed_posts_limited():
        """ë¶„ì„ë˜ì§€ ì•Šì€ ê²Œì‹œê¸€ 50ê°œì”©ë§Œ ì¡°íšŒ"""
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        # ê° ì†ŒìŠ¤ë³„ë¡œ ìµœëŒ€ 50ê°œì”©ë§Œ (ì´ 150ê°œ ì œí•œ)
        LIMIT_PER_SOURCE = 50
        
        # X ê²Œì‹œê¸€ - 50ê°œë§Œ
        x_query = """
        SELECT tweet_id as post_id, 'x' as source, username,
                text as content, created_at as post_timestamp
        FROM x_posts
        WHERE tweet_id NOT IN (
            SELECT post_id FROM post_analysis_cache WHERE post_source = 'x'
        )
        AND text NOT LIKE '@%%'
        AND text IS NOT NULL
        ORDER BY created_at DESC
        LIMIT %s
        """
        
        # Truth Social ê²Œì‹œê¸€ - 50ê°œë§Œ
        truth_posts_query = """
        SELECT id as post_id, 'truth_social_posts' as source, username,
               clean_content as content, created_at as post_timestamp
        FROM truth_social_posts
        WHERE id NOT IN (
            SELECT post_id FROM post_analysis_cache
            WHERE (post_source = 'truth_social_posts' OR post_source = 'truth_social_trends')
        )
        ORDER BY created_at DESC
        LIMIT %s
        """
        
        # Truth Social íŠ¸ë Œë“œ - 50ê°œë§Œ
        truth_trends_query = """
        SELECT id as post_id, 'truth_social_trends' as source, username,
               clean_content as content, created_at as post_timestamp
        FROM truth_social_trends
        WHERE id NOT IN (
            SELECT post_id FROM post_analysis_cache
            WHERE (post_source = 'truth_social_posts' OR post_source = 'truth_social_trends')
        )
        ORDER BY created_at DESC
        LIMIT %s
        """
        
        x_posts = pg_hook.get_records(x_query, parameters=[LIMIT_PER_SOURCE])
        truth_posts = pg_hook.get_records(truth_posts_query, parameters=[LIMIT_PER_SOURCE])
        truth_trends = pg_hook.get_records(truth_trends_query, parameters=[LIMIT_PER_SOURCE])
        
        # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        all_posts = []
        
        for row in x_posts:
            all_posts.append({
                'post_id': row[0],
                'source': row[1], 
                'username': row[2],
                'content': row[3],
                'post_timestamp': row[4]
            })
        
        for row in truth_posts:
            all_posts.append({
                'post_id': row[0],
                'source': row[1],
                'username': row[2], 
                'content': row[3],
                'post_timestamp': row[4]
            })
            
        for row in truth_trends:
            all_posts.append({
                'post_id': row[0],
                'source': row[1],
                'username': row[2], 
                'content': row[3],
                'post_timestamp': row[4]
            })
        
        total_posts = len(all_posts)
        logger.info(f"ì´ë²ˆ ë°°ì¹˜ì—ì„œ ì²˜ë¦¬í•  ê²Œì‹œê¸€: {total_posts}ê°œ (X: {len(x_posts)}, Truth Posts: {len(truth_posts)}, Truth Trends: {len(truth_trends)})")
        
        if total_posts == 0:
            logger.info("ğŸ‰ ëª¨ë“  ê²Œì‹œê¸€ ë¶„ì„ ì™„ë£Œ!")
        
        return all_posts
    
    @task
    def analyze_posts_batch(posts):
        """ê²Œì‹œê¸€ ë°°ì¹˜ ë¶„ì„ + ì‹œì¥ ë¶„ì„ ì¶”ê°€"""
        if not posts:
            logger.info("ì²˜ë¦¬í•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        analyzer = SocialMediaAnalyzer()
        collector = MarketDataCollector()
        market_analyzer = MarketAnalyzer(collector.pg_hook)  # ìƒˆë¡œ ì¶”ê°€ëœ ë¶„ì„ê¸°
        results = []

        logger.info(f"ğŸš€ {len(posts)}ê°œ ê²Œì‹œê¸€ ë¶„ì„ ì‹œì‘ (ì‹œì¥ ë¶„ì„ í¬í•¨)")
        logger.info("ğŸ“Š MarketAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
        
        for i, post in enumerate(posts):
            try:
                logger.info(f"ê²Œì‹œê¸€ ë¶„ì„ ì¤‘ {i+1}/{len(posts)}: {post['post_id']}")
                
                # 1. ê¸°ì¡´ ìì‚° ë§¤ì¹­
                affected_assets = analyzer.determine_affected_assets(
                    username=post['username'],
                    content=post['content'],
                    timestamp=post['post_timestamp'],
                    post_id=post['post_id'],
                    post_source=post['source']
                )
                
                # 2. ê¸°ì¡´ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘
                market_data = collector.collect_market_data(
                    affected_assets, post['post_timestamp']
                )
                
                # 3. ìƒˆë¡œìš´ ì‹œì¥ ë¶„ì„ ì¶”ê°€
                price_analysis = {}
                volume_analysis = {}

                logger.info(f"ğŸ“ˆ {len(affected_assets)}ê°œ ìì‚°ì— ëŒ€í•œ ì‹œì¥ ë¶„ì„ ì‹œì‘")

                for asset in affected_assets:
                    symbol = asset['symbol']
                    if symbol in market_data:
                        # ê°€ê²© ë³€í™” ë¶„ì„
                        price_changes = market_analyzer.calculate_price_changes(
                            symbol, post['post_timestamp'], market_data[symbol]
                        )
                        if price_changes:
                            price_analysis[symbol] = price_changes

                        # ê±°ë˜ëŸ‰ ë³€í™” ë¶„ì„
                        volume_changes = market_analyzer.calculate_volume_changes(
                            symbol, post['post_timestamp'], market_data[symbol]
                        )
                        if volume_changes:
                            volume_analysis[symbol] = volume_changes
                
                # ë¶„ì„ ìƒíƒœ ê²°ì •
                analysis_status = 'complete' if affected_assets else 'partial'
                
                result = {
                    'post_id': post['post_id'],
                    'post_source': post['source'],
                    'post_timestamp': post['post_timestamp'],
                    'author_username': post['username'],
                    'affected_assets': affected_assets,
                    'market_data': market_data,
                    'price_analysis': price_analysis,  # ìƒˆë¡œ ì¶”ê°€
                    'volume_analysis': volume_analysis,  # ìƒˆë¡œ ì¶”ê°€
                    'analysis_status': analysis_status
                }
                
                results.append(result)
                logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ {post['post_id']} - {len(affected_assets)}ê°œ ìì‚°, {len(price_analysis)}ê°œ ê°€ê²©ë¶„ì„, {len(volume_analysis)}ê°œ ê±°ë˜ëŸ‰ë¶„ì„")
                
            except Exception as e:
                logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨ {post['post_id']}: {e}")
                results.append({
                    'post_id': post['post_id'],
                    'post_source': post['source'],
                    'post_timestamp': post['post_timestamp'],
                    'author_username': post['username'],
                    'affected_assets': [],
                    'market_data': {},
                    'price_analysis': {},
                    'volume_analysis': {},
                    'analysis_status': 'failed',
                    'error': str(e)
                })
        
        logger.info(f"âœ… ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ê²Œì‹œê¸€ ì²˜ë¦¬ë¨")
        logger.info(f"ğŸ“Š ì „ì²´ í†µê³„ - ê°€ê²©ë¶„ì„: {sum(len(r.get('price_analysis', {})) for r in results)}ê°œ, ê±°ë˜ëŸ‰ë¶„ì„: {sum(len(r.get('volume_analysis', {})) for r in results)}ê°œ")
        return results
    
    @task
    def save_analysis_results(analysis_results):
        """ê°•í™”ëœ ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        if not analysis_results:
            logger.info("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        pg_hook = PostgresHook(postgres_conn_id='postgres_default')
        
        logger.info(f"ğŸ’¾ {len(analysis_results)}ê°œ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘")
        
        for result in analysis_results:
            try:
                upsert_query = """
                INSERT INTO post_analysis_cache 
                (post_id, post_source, post_timestamp, author_username, 
                affected_assets, market_data, price_analysis, volume_analysis, 
                analysis_status, error_message)
                VALUES (%(post_id)s, %(post_source)s, %(post_timestamp)s, 
                        %(author_username)s, %(affected_assets)s, %(market_data)s, 
                        %(price_analysis)s, %(volume_analysis)s,
                        %(analysis_status)s, %(error_message)s)
                ON CONFLICT (post_id, post_source) 
                DO UPDATE SET 
                    affected_assets = EXCLUDED.affected_assets,
                    market_data = EXCLUDED.market_data,
                    price_analysis = EXCLUDED.price_analysis,
                    volume_analysis = EXCLUDED.volume_analysis,
                    analysis_status = EXCLUDED.analysis_status,
                    error_message = EXCLUDED.error_message,
                    updated_at = NOW()
                """
                
                params = {
                    'post_id': result['post_id'],
                    'post_source': result['post_source'],
                    'post_timestamp': result['post_timestamp'],
                    'author_username': result['author_username'],
                    'affected_assets': json.dumps(result['affected_assets']),
                    'market_data': json.dumps(result['market_data']),
                    'price_analysis': json.dumps(result['price_analysis']),
                    'volume_analysis': json.dumps(result['volume_analysis']),
                    'analysis_status': result['analysis_status'],
                    'error_message': result.get('error', None)
                }
                
                pg_hook.run(upsert_query, parameters=params)
                
            except Exception as e:
                logger.error(f"ì €ì¥ ì‹¤íŒ¨ {result['post_id']}: {e}")
                continue
        
        logger.info("âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    
    @task
    def finalize_keywords():
        """í‚¤ì›Œë“œ ì •ë¦¬ ì‘ì—…"""
        analyzer = SocialMediaAnalyzer()
        analyzer.finalize_keywords()
        logger.info("í‚¤ì›Œë“œ ì •ë¦¬ ì™„ë£Œ")

    # DAG íƒœìŠ¤í¬ ì‹¤í–‰ íë¦„ (ê¸°ì¡´ê³¼ ë™ì¼)
    posts = get_unanalyzed_posts_limited()
    analysis_results = analyze_posts_batch(posts)
    save_analysis_results(analysis_results)
    finalize_keywords()

# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
dag_instance = social_media_analysis_dag()