from datetime import datetime, timedelta, date
import subprocess
import json
import os

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "..", "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_truth_social_tags.sql"), encoding="utf-8") as f:
    UPSERT_TAGS_SQL = f.read()

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': None,
    'retry_delay': timedelta(minutes=1),
}

def run_truthbrush_command(command_args):
    """Truthbrush ëª…ë ¹ì–´ ì‹¤í–‰"""
    username = Variable.get('TRUTHSOCIAL_USERNAME')
    password = Variable.get('TRUTHSOCIAL_PASSWORD')
    try:
        cmd = ['truthbrush', '--username', username, '--password', password] + command_args
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            return result.stdout
        else:
            raise Exception(f"Truthbrush ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
    except subprocess.TimeoutExpired:
        raise Exception("Truthbrush ëª…ë ¹ì–´ íƒ€ì„ì•„ì›ƒ")

def parse_tag_data(raw_tag):
    """í•´ì‹œíƒœê·¸ ë°ì´í„° ê°„ë‹¨ íŒŒì‹±"""
    history = raw_tag.get('history', [])
    
    # í•„ìˆ˜ ë°ì´í„° ê²€ì¦
    if not raw_tag.get('name') or not history:
        raise ValueError("í•„ìˆ˜ ë°ì´í„° ëˆ„ë½")
    
    # 7ì¼ê°„ ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
    day_uses = []
    for i in range(7):
        if i < len(history):
            day_uses.append(int(history[i].get('uses', 0)))
        else:
            day_uses.append(0)
    
    return {
        'name': raw_tag['name'],
        'collected_date': date.today(),
        'url': raw_tag.get('url'),
        'total_uses': day_uses[0],
        'total_accounts': int(history[0].get('accounts', 0)) if history else 0,
        'recent_statuses_count': raw_tag.get('recent_statuses_count', 0),
        'history_data': json.dumps(history),
        'day_0_uses': day_uses[0],
        'day_1_uses': day_uses[1],
        'day_2_uses': day_uses[2],
        'day_3_uses': day_uses[3],
        'day_4_uses': day_uses[4],
        'day_5_uses': day_uses[5],
        'day_6_uses': day_uses[6],
        'trend_score': day_uses[0],  # ë‹¨ìˆœíˆ ì˜¤ëŠ˜ ì‚¬ìš©ëŸ‰
        'growth_rate': 0,  # ê³„ì‚° ìƒëµ
        'weekly_average': sum(day_uses) / 7,
        'tag_category': 'general',  # ê¸°ë³¸ê°’
        'market_relevance': 0  # ê¸°ë³¸ê°’
    }

def fetch_trending_tags(**context):
    """íŠ¸ë Œë”© í•´ì‹œíƒœê·¸ ìˆ˜ì§‘"""
    print("ğŸ·ï¸ íŠ¸ë Œë”© í•´ì‹œíƒœê·¸ ìˆ˜ì§‘ ì¤‘...")
    
    output = run_truthbrush_command(['tags'])
    
    if not output.strip():
        raise Exception("ë¹ˆ ì‘ë‹µ ë°›ìŒ")
    
    # JSON íŒŒì‹±
    first_line = output.strip().split('\n')[0]
    tag_list = json.loads(first_line)
    
    if not isinstance(tag_list, list) or len(tag_list) == 0:
        raise Exception("ìœ íš¨í•œ í•´ì‹œíƒœê·¸ ë°ì´í„° ì—†ìŒ")
    
    # ë°ì´í„° íŒŒì‹±
    tags = []
    for tag_data in tag_list:
        try:
            processed_tag = parse_tag_data(tag_data)
            tags.append(processed_tag)
        except Exception as e:
            print(f"âš ï¸ íƒœê·¸ íŒŒì‹± ì‹¤íŒ¨: {tag_data.get('name', 'Unknown')} - {e}")
            continue
    
    if len(tags) == 0:
        raise Exception("íŒŒì‹±ëœ í•´ì‹œíƒœê·¸ ì—†ìŒ")
    
    print(f"âœ… íŠ¸ë Œë”© í•´ì‹œíƒœê·¸ {len(tags)}ê°œ ìˆ˜ì§‘")
    context['ti'].xcom_push(key='trending_tags', value=tags)
    return len(tags)

def store_tags_to_db(**context):
    """íŠ¸ë Œë”© í•´ì‹œíƒœê·¸ë¥¼ DBì— ì €ì¥"""
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    tags = context['ti'].xcom_pull(key='trending_tags')
    
    if not tags:
        raise Exception("ì €ì¥í•  í•´ì‹œíƒœê·¸ ë°ì´í„° ì—†ìŒ")
    
    success_count = 0
    for tag in tags:
        try:
            hook.run(UPSERT_TAGS_SQL, parameters=tag)
            success_count += 1
        except Exception as e:
            print(f"âŒ í•´ì‹œíƒœê·¸ ì €ì¥ ì‹¤íŒ¨: {tag.get('name', 'Unknown')} - {e}")
            raise  # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ ì „ì²´ ì‹¤íŒ¨
    
    print(f"âœ… í•´ì‹œíƒœê·¸ ì €ì¥ ì™„ë£Œ: {success_count}ê°œ")
    return success_count

# DAG ì •ì˜
with DAG(
    dag_id='ingest_truth_social_tags_k8s',
    default_args=default_args,
    schedule_interval='0 0 * * *',  # ë§¤ì¼ ìì •
    catchup=False,
    description='Truth Social íŠ¸ë Œë”© í•´ì‹œíƒœê·¸ ìˆ˜ì§‘',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['truth_social', 'hashtags', 'market_trends', 'k8s'],
) as dag:
    
    # í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_truth_social_tags_table',
        postgres_conn_id='postgres_default',
        sql='create_truth_social_tags.sql',
    )
    
    # íŠ¸ë Œë”© í•´ì‹œíƒœê·¸ ìˆ˜ì§‘
    fetch_tags = PythonOperator(
        task_id='fetch_trending_tags',
        python_callable=fetch_trending_tags,
    )
    
    # DB ì €ì¥
    store_tags = PythonOperator(
        task_id='store_tags_to_db',
        python_callable=store_tags_to_db,
    )
    
    # Task ì˜ì¡´ì„±
    create_table >> fetch_tags >> store_tags