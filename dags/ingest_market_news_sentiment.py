from datetime import datetime, timedelta
import os
import requests
from decimal import Decimal
import json

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# í‘œì¤€ ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "..", "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_market_news_sentiment.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': None,
    'retry_delay': timedelta(minutes=1),
}

# 15ê°œ ì¶”ì²œ ì¿¼ë¦¬ ì¡°í•©
QUERY_COMBINATIONS = [
    {'type': 'daily_market', 'params': 'sort=LATEST&limit=30'},
    {'type': 'fed_news', 'params': 'topics=economy_monetary&sort=LATEST&limit=20'},
    {'type': 'earnings_season', 'params': 'topics=earnings&sort=LATEST&limit=40'},
    {'type': 'bigtech_megacap', 'params': 'tickers=AAPL,MSFT,NVDA,GOOGL,AMZN,TSLA,META&sort=LATEST&limit=30'},
    {'type': 'bigtech_ai', 'params': 'tickers=NVDA,AMD,INTC,AAPL,MSFT,GOOGL&topics=technology&limit=25'},
    {'type': 'bigtech_cloud', 'params': 'tickers=MSFT,GOOGL,AMZN,CRM,ORCL&topics=technology&limit=20'},
    {'type': 'crypto_coins', 'params': 'tickers=CRYPTO:BTC,CRYPTO:ETH,CRYPTO:SOL&sort=LATEST&limit=20'},
    {'type': 'crypto_stocks', 'params': 'tickers=COIN,MSTR,RIOT,MARA,CLSK&sort=LATEST&limit=25'},
    {'type': 'crypto_surge', 'params': 'tickers=CRYPTO:BTC,CRYPTO:ETH&topics=blockchain,financial_markets&limit=30'},
    {'type': 'crypto_ecosystem', 'params': 'topics=blockchain&sort=LATEST&limit=25'},
    {'type': 'tech_innovation', 'params': 'topics=technology&sort=RELEVANCE&limit=30'},
    {'type': 'ma_opportunities', 'params': 'topics=mergers_and_acquisitions&sort=LATEST&limit=20'},
    {'type': 'financial_policy', 'params': 'tickers=JPM,BAC,WFC,C,GS&topics=economy_monetary&limit=20'},
]

def collect_news_sentiment(**context):
    """
    Market News & Sentiment APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
    """
    # API í‚¤ ê°€ì ¸ì˜¤ê¸°
    api_key = Variable.get('ALPHA_VANTAGE_NEWS_API_KEY_2')
    if not api_key:
        raise ValueError("ðŸ”‘ ALPHA_VANTAGE_NEWS_API_KEY_2ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # DB ì—°ê²° ë° batch_id ìƒì„±
    hook = PostgresHook(postgres_conn_id='postgres_default')
    max_batch_result = hook.get_first("SELECT COALESCE(MAX(batch_id), 0) FROM market_news_sentiment")
    current_batch_id = (max_batch_result[0] if max_batch_result else 0) + 1
    
    print(f"ðŸ†” ë°°ì¹˜ ID: {current_batch_id}")
    
    all_news_data = []
    total_api_calls = 0
    
    for query_combo in QUERY_COMBINATIONS:
        try:
            # API ìš”ì²­
            url = "https://www.alphavantage.co/query"
            params = f"function=NEWS_SENTIMENT&{query_combo['params']}&apikey={api_key}"
            
            print(f"ðŸš€ API ìš”ì²­: {query_combo['type']}")
            response = requests.get(f"{url}?{params}", timeout=60)
            response.raise_for_status()
            total_api_calls += 1
            
            # API ì‘ë‹µ ê²€ì¦
            if 'Error Message' in response.text:
                print(f"âŒ API ì˜¤ë¥˜ ({query_combo['type']}): {response.text[:100]}")
                continue
                
            if 'Note' in response.text and 'API call frequency' in response.text:
                print(f"âš ï¸ API í˜¸ì¶œ ì œí•œ ë„ë‹¬: {total_api_calls}ë²ˆì§¸ í˜¸ì¶œì—ì„œ ì¤‘ë‹¨")
                break
            
            try:
                data = response.json()
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨ ({query_combo['type']}): {str(e)}")
                continue
            
            # ë‰´ìŠ¤ í”¼ë“œ í™•ì¸
            if 'feed' not in data:
                print(f"âš ï¸ ë‰´ìŠ¤ í”¼ë“œ ì—†ìŒ ({query_combo['type']})")
                continue
            
            news_count = len(data['feed'])
            print(f"âœ… {query_combo['type']}: {news_count}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            
            # ë‰´ìŠ¤ ë°ì´í„°ì— ì¿¼ë¦¬ ì •ë³´ ì¶”ê°€
            for article in data['feed']:
                article['query_type'] = query_combo['type']
                article['query_params'] = query_combo['params']
                article['batch_id'] = current_batch_id
            
            all_news_data.extend(data['feed'])
            
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì‹¤íŒ¨ ({query_combo['type']}): {str(e)}")
            continue
    
    print(f"ðŸŽ¯ ì´ {total_api_calls}ë²ˆ API í˜¸ì¶œ, {len(all_news_data)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
    
    # XComì— ì €ìž¥
    context['ti'].xcom_push(key='news_data', value=all_news_data)
    context['ti'].xcom_push(key='batch_id', value=current_batch_id)
    
    return {
        'batch_id': current_batch_id,
        'total_api_calls': total_api_calls,
        'total_news': len(all_news_data)
    }

def process_and_store_news(**context):
    """
    ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ê³µí•˜ì—¬ PostgreSQLì— ì €ìž¥
    """
    # XComì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    news_data = context['ti'].xcom_pull(task_ids='collect_news_sentiment', key='news_data')
    batch_id = context['ti'].xcom_pull(task_ids='collect_news_sentiment', key='batch_id')
    
    if not news_data:
        raise ValueError("âŒ ì´ì „ íƒœìŠ¤í¬ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    success_count = 0
    error_count = 0
    
    print(f"ðŸš€ ë°°ì¹˜ {batch_id} ë‰´ìŠ¤ ì €ìž¥ ì‹œìž‘: {len(news_data)}ê°œ")
    
    for article in news_data:
        try:
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if not article.get('url') or not article.get('title'):
                print(f"âš ï¸ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {article.get('title', 'Unknown')[:50]}")
                error_count += 1
                continue
            
            # ì‹œê°„ íŒŒì‹±
            time_published = datetime.strptime(article['time_published'], '%Y%m%dT%H%M%S')
            
            # ìž‘ì„±ìž ì²˜ë¦¬ (ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìžì—´ë¡œ ë³€í™˜)
            authors = ', '.join(article.get('authors', [])) if article.get('authors') else None
            
            # íŒŒë¼ë¯¸í„° ì¤€ë¹„
            params = {
                'batch_id': batch_id,
                'title': article['title'][:500],  # ê¸¸ì´ ì œí•œ
                'url': article['url'],
                'time_published': time_published,
                'authors': authors[:200] if authors else None,  # ê¸¸ì´ ì œí•œ
                'summary': article.get('summary', '')[:1000],  # ê¸¸ì´ ì œí•œ
                'source': article.get('source', '')[:100],
                'overall_sentiment_score': Decimal(str(article.get('overall_sentiment_score', 0))),
                'overall_sentiment_label': article.get('overall_sentiment_label', 'Neutral'),
                'ticker_sentiment': json.dumps(article.get('ticker_sentiment', [])),
                'topics': json.dumps(article.get('topics', [])),
                'query_type': article['query_type'],
                'query_params': article['query_params']
            }
            
            # SQL ì‹¤í–‰
            hook.run(UPSERT_SQL, parameters=params)
            success_count += 1
            
        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ì €ìž¥ ì‹¤íŒ¨: {article.get('title', 'Unknown')[:50]} - {str(e)}")
            error_count += 1
            continue
    
    print(f"ðŸŽ¯ ë°°ì¹˜ {batch_id} ì €ìž¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
    
    return {
        'batch_id': batch_id,
        'success_count': success_count,
        'error_count': error_count
    }

# DAG ì •ì˜
with DAG(
    dag_id='ingest_market_news_sentiment',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
    description='Alpha Vantage News & Sentiment APIì—ì„œ ì‹œìž¥ ë‰´ìŠ¤ ë° ê°ì„± ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['news', 'sentiment', 'alpha_vantage', 'market', 'daily'],
) as dag:
    
    # 1. í…Œì´ë¸” ìƒì„±
    create_table = PostgresOperator(
        task_id='create_market_news_sentiment_table',
        postgres_conn_id='postgres_default',
        sql='create_market_news_sentiment.sql',
    )
    
    # 2. ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
    collect_news = PythonOperator(
        task_id='collect_news_sentiment',
        python_callable=collect_news_sentiment,
    )
    
    # 3. ë°ì´í„° ê°€ê³µ ë° ì €ìž¥
    process_news = PythonOperator(
        task_id='process_and_store_news',
        python_callable=process_and_store_news,
    )
    
    # Task ì˜ì¡´ì„±
    create_table >> collect_news >> process_news