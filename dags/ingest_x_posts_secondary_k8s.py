from datetime import datetime, timedelta
import requests
import json
import os

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.models import Variable

# ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "..", "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_x_posts.sql"), encoding="utf-8") as f:
    UPSERT_SQL = f.read()

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment_assistant',
    'start_date': datetime(2025, 1, 1),
    'retries': None,
    'retry_delay': timedelta(minutes=1),
}

# ===== ë‘ ë²ˆì§¸ í† í°ìš© í™•ìž¥ ê³„ì •ë“¤ (ì›” 100íšŒ ë°°ë¶„) =====
SECONDARY_ACCOUNT_SCHEDULE = {
    # === 1. ì•”í˜¸í™”í ìƒíƒœê³„ (35íšŒ/ì›”) ===
    'saylor': {  # Michael Saylor (MicroStrategy)
        'user_id': '244647486',
        'frequency': 'daily',              # 30íšŒ/ì›”
        'max_results': 50,
        'priority': 1,
        'category': 'crypto'
    },
    'brian_armstrong': {  # Coinbase CEO
        'user_id': '9224862',
        'frequency': 'twice_weekly',       # 8íšŒ/ì›” (í™”, ê¸ˆ)
        'max_results': 50,
        'priority': 2,
        'category': 'crypto'
    },
    'CoinbaseAssets': {  # Coinbase ê³µì‹
        'user_id': '1087818612',
        'frequency': 'weekly',             # 4íšŒ/ì›” (ì¼ìš”ì¼)
        'max_results': 30,
        'priority': 3,
        'category': 'crypto'
    },
    
    # === 2. ì¶”ê°€ ë¹…í…Œí¬ CEOë“¤ (25íšŒ/ì›”) ===
    'jeffbezos': {  # Amazon ì°½ë¦½ìž
        'user_id': '12071242',
        'frequency': 'every_2_days',       # 15íšŒ/ì›”
        'max_results': 50,
        'priority': 1,
        'category': 'tech_ceo'
    },
    'sundarpichai': {  # Google CEO (ì¤‘ë³µ ë°©ì§€ - ë‹¤ë¥¸ ìŠ¤ì¼€ì¤„)
        'user_id': '16144047',
        'frequency': 'twice_weekly',       # 8íšŒ/ì›” (ì›”, ëª©)
        'max_results': 50,
        'priority': 2,
        'category': 'tech_ceo'
    },
    'IBM': {  # IBM ê³µì‹ ê³„ì •
        'user_id': '18994444',
        'frequency': 'weekly',             # 4íšŒ/ì›” (í† ìš”ì¼)
        'max_results': 30,
        'priority': 3,
        'category': 'tech_ceo'
    },
    
    # === 3. íˆ¬ìž ê¸°ê´€ & ì¸í”Œë£¨ì–¸ì„œë“¤ (20íšŒ/ì›”) ===
    'CathieDWood': {  # ARK Invest
        'user_id': '2899617799',
        'frequency': 'twice_weekly',       # 8íšŒ/ì›” (ìˆ˜, í† )
        'max_results': 50,
        'priority': 2,
        'category': 'institutional'
    },
    'mcuban': {  # Mark Cuban
        'user_id': '15164565',
        'frequency': 'twice_weekly',       # 8íšŒ/ì›” (í™”, ê¸ˆ)
        'max_results': 50,
        'priority': 2,
        'category': 'institutional'
    },
    'chamath': {  # Chamath Palihapitiya
        'user_id': '3291691',
        'frequency': 'weekly',             # 4íšŒ/ì›” (ì¼ìš”ì¼)
        'max_results': 40,
        'priority': 3,
        'category': 'institutional'
    },
    
    # === 4. ê¸ˆìœµ ë¯¸ë””ì–´ (15íšŒ/ì›”) ===
    'CNBC': {
        'user_id': '20402945',
        'frequency': 'twice_weekly',       # 8íšŒ/ì›” (ì›”, ëª©)
        'max_results': 40,
        'priority': 2,
        'category': 'media'
    },
    'business': {  # Bloomberg
        'user_id': '34713362',
        'frequency': 'weekly',             # 4íšŒ/ì›” (í™”ìš”ì¼)
        'max_results': 30,
        'priority': 3,
        'category': 'media'
    },
    'WSJ': {  # Wall Street Journal
        'user_id': '3108351',
        'frequency': 'weekly',             # 4íšŒ/ì›” (ìˆ˜ìš”ì¼)
        'max_results': 30,
        'priority': 3,
        'category': 'media'
    },
    
    # === 5. ê¸°ì—… ê³µì‹ ê³„ì •ë“¤ (5íšŒ/ì›”) ===
    'Tesla': {
        'user_id': '13298072',
        'frequency': 'weekly',             # 4íšŒ/ì›” (ê¸ˆìš”ì¼)
        'max_results': 25,
        'priority': 3,
        'category': 'corporate'
    },
    'nvidia': {
        'user_id': '61559439',
        'frequency': 'weekly',             # 4íšŒ/ì›” (ëª©ìš”ì¼)
        'max_results': 25,
        'priority': 3,
        'category': 'corporate'
    }
    # ì´ í˜¸ì¶œ: 30+8+4+15+8+4+8+8+4+4+4+4+4 = 105íšŒ/ì›” (5íšŒ ì—¬ìœ )
}

def should_run_account_today_secondary(username):
    """ë‘ ë²ˆì§¸ í† í° ê³„ì •ë“¤ì˜ ì˜¤ëŠ˜ ì‹¤í–‰ ì—¬ë¶€ íŒë‹¨"""
    config = SECONDARY_ACCOUNT_SCHEDULE.get(username)
    if not config:
        return False
    
    frequency = config['frequency']
    today = datetime.now()
    day_of_year = today.timetuple().tm_yday  # 1-365
    day_of_week = today.weekday()  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
    
    if frequency == 'daily':
        return True
    elif frequency == 'every_2_days':
        # jeffbezosëŠ” í™€ìˆ˜ì¼ì— ì‹¤í–‰
        return day_of_year % 2 == 1
    elif frequency == 'every_3_days':
        # í•„ìš”ì‹œ ì¶”ê°€
        return (day_of_year + 1) % 3 == 0
    elif frequency == 'twice_weekly':
        # ê³„ì •ë³„ë¡œ ë‹¤ë¥¸ ìš”ì¼ í• ë‹¹ (Primary í† í°ê³¼ ê²¹ì¹˜ì§€ ì•Šê²Œ)
        twice_weekly_schedule = {
            'brian_armstrong': [1, 4],    # í™”, ê¸ˆ
            'sundarpichai': [0, 3],       # ì›”, ëª© (Primaryì™€ ë‹¤ë¦„)
            'CathieDWood': [2, 5],        # ìˆ˜, í† 
            'mcuban': [1, 4],             # í™”, ê¸ˆ
            'CNBC': [0, 3]                # ì›”, ëª©
        }
        assigned_days = twice_weekly_schedule.get(username, [0, 3])
        return day_of_week in assigned_days
    elif frequency == 'weekly':
        # ê³„ì •ë³„ë¡œ ë‹¤ë¥¸ ìš”ì¼ í• ë‹¹
        weekly_schedule = {
            'CoinbaseAssets': 6,    # ì¼ìš”ì¼
            'IBM': 5,               # í† ìš”ì¼  
            'chamath': 6,           # ì¼ìš”ì¼
            'business': 1,          # í™”ìš”ì¼
            'WSJ': 2,               # ìˆ˜ìš”ì¼
            'Tesla': 4,             # ê¸ˆìš”ì¼
            'nvidia': 3             # ëª©ìš”ì¼
        }
        return day_of_week == weekly_schedule.get(username, 6)
    
    return False

def get_todays_secondary_accounts():
    """ì˜¤ëŠ˜ ìˆ˜ì§‘í•  ë‘ ë²ˆì§¸ í† í° ê³„ì •ë“¤ ë°˜í™˜"""
    todays_accounts = []
    
    for username in SECONDARY_ACCOUNT_SCHEDULE.keys():
        if should_run_account_today_secondary(username):
            todays_accounts.append(username)
    
    # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
    todays_accounts.sort(key=lambda x: SECONDARY_ACCOUNT_SCHEDULE[x]['priority'])
    
    return todays_accounts

def call_x_api_secondary(username, user_id, max_results=50):
    """ë‘ ë²ˆì§¸ Bearer Tokenìœ¼ë¡œ X API í˜¸ì¶œ"""
    # ë‘ ë²ˆì§¸ í† í° ì‚¬ìš©
    bearer_token = Variable.get('X_API_BEARER_TOKEN_2')
    
    url = f"https://api.twitter.com/2/users/{user_id}/tweets"
    
    # 24ì‹œê°„ ì „ë¶€í„° ìˆ˜ì§‘
    start_time = (datetime.utcnow() - timedelta(hours=24)).isoformat() + 'Z'
    
    params = {
        "max_results": min(max_results, 100),
        "start_time": start_time,
        "tweet.fields": "created_at,text,public_metrics,context_annotations,entities,lang,edit_history_tweet_ids",
        "expansions": "author_id",
        "user.fields": "name,username,verified,public_metrics"
    }
    
    headers = {
        "Authorization": f"Bearer {bearer_token}",
        "User-Agent": "InvestmentAssistant-Secondary/1.0"
    }
    
    response = requests.get(url, headers=headers, params=params, timeout=30)
    response.raise_for_status()
    
    data = response.json()
    return data

def process_tweet_data_secondary(tweet, user_info, source_account, category):
    """íŠ¸ìœ— ë°ì´í„° ì²˜ë¦¬ (ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€)"""
    
    # ê¸°ë³¸ íŠ¸ìœ— ì •ë³´
    processed_data = {
        'tweet_id': tweet['id'],
        'author_id': tweet['author_id'],
        'text': tweet['text'],
        'created_at': tweet['created_at'].replace('Z', '+00:00'),
        'lang': tweet.get('lang', 'en'),
        'source_account': source_account,
        'account_category': category,  # ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        'collection_source': 'secondary_token',  # í† í° êµ¬ë¶„
    }
    
    # ì°¸ì—¬ë„ ì§€í‘œ
    metrics = tweet.get('public_metrics', {})
    processed_data.update({
        'retweet_count': metrics.get('retweet_count', 0),
        'reply_count': metrics.get('reply_count', 0),
        'like_count': metrics.get('like_count', 0),
        'quote_count': metrics.get('quote_count', 0),
        'bookmark_count': metrics.get('bookmark_count', 0),
        'impression_count': metrics.get('impression_count', 0),
    })
    
    # ì—”í‹°í‹° ì •ë³´ (JSONìœ¼ë¡œ ì €ìž¥)
    entities = tweet.get('entities', {})
    processed_data.update({
        'hashtags': json.dumps(entities.get('hashtags', [])) if entities.get('hashtags') else None,
        'mentions': json.dumps(entities.get('mentions', [])) if entities.get('mentions') else None,
        'urls': json.dumps(entities.get('urls', [])) if entities.get('urls') else None,
        'cashtags': json.dumps(entities.get('cashtags', [])) if entities.get('cashtags') else None,
        'annotations': json.dumps(entities.get('annotations', [])) if entities.get('annotations') else None,
    })
    
    # ì»¨í…ìŠ¤íŠ¸ ì£¼ì„
    context_annotations = tweet.get('context_annotations', [])
    processed_data['context_annotations'] = json.dumps(context_annotations) if context_annotations else None
    
    # ì‚¬ìš©ìž ì •ë³´
    if user_info:
        user_metrics = user_info.get('public_metrics', {})
        processed_data.update({
            'username': user_info.get('username', source_account),
            'display_name': user_info.get('name', ''),
            'user_verified': user_info.get('verified', False),
            'user_followers_count': user_metrics.get('followers_count', 0),
            'user_following_count': user_metrics.get('following_count', 0),
            'user_tweet_count': user_metrics.get('tweet_count', 0),
        })
    else:
        processed_data.update({
            'username': source_account,
            'display_name': '',
            'user_verified': False,
            'user_followers_count': 0,
            'user_following_count': 0,
            'user_tweet_count': 0,
        })
    
    # íŽ¸ì§‘ ì´ë ¥
    edit_history = tweet.get('edit_history_tweet_ids', [])
    processed_data['edit_history_tweet_ids'] = json.dumps(edit_history) if edit_history else None
    
    return processed_data

def fetch_secondary_tweets(**context):
    """ë‘ ë²ˆì§¸ í† í°ìœ¼ë¡œ í™•ìž¥ ê³„ì •ë“¤ì˜ íŠ¸ìœ— ìˆ˜ì§‘"""
    
    # ì˜¤ëŠ˜ ìˆ˜ì§‘í•  ê³„ì •ë“¤ ê²°ì •
    todays_accounts = get_todays_secondary_accounts()
    
    if not todays_accounts:
        print("ðŸ“… [SECONDARY] ì˜¤ëŠ˜ì€ ìˆ˜ì§‘í•  ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤")
        context['ti'].xcom_push(key='collected_tweets', value=[])
        return 0
    
    print(f"ðŸŽ¯ [SECONDARY TOKEN] ì˜¤ëŠ˜ ìˆ˜ì§‘ ëŒ€ìƒ: {len(todays_accounts)}ê°œ ê³„ì •")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
    category_counts = {}
    for account in todays_accounts:
        config = SECONDARY_ACCOUNT_SCHEDULE[account]
        category = config['category']
        if category not in category_counts:
            category_counts[category] = 0
        category_counts[category] += 1
        print(f"   - {account}: {config['frequency']} ({category}, ìš°ì„ ìˆœìœ„ {config['priority']})")
    
    print(f"ðŸ“Š ì¹´í…Œê³ ë¦¬ë³„: {dict(category_counts)}")
    
    # ê° ê³„ì •ë³„ íŠ¸ìœ— ìˆ˜ì§‘
    all_tweets = []
    total_api_calls = 0
    category_stats = {}
    
    for username in todays_accounts:
        try:
            config = SECONDARY_ACCOUNT_SCHEDULE[username]
            user_id = config['user_id']
            max_results = config['max_results']
            category = config['category']
            
            print(f"\nðŸ” [{category.upper()}] {username} íŠ¸ìœ— ìˆ˜ì§‘ ì¤‘ (ìµœëŒ€ {max_results}ê°œ)...")
            
            # API í˜¸ì¶œ (ë‘ ë²ˆì§¸ í† í° ì‚¬ìš©)
            api_response = call_x_api_secondary(username, user_id, max_results)
            total_api_calls += 1
            
            if 'data' not in api_response or not api_response['data']:
                print(f"âš ï¸ {username}: ìµœê·¼ 24ì‹œê°„ ë‚´ íŠ¸ìœ— ì—†ìŒ")
                continue
            
            # ì‚¬ìš©ìž ì •ë³´ ì¶”ì¶œ
            user_info = {}
            if 'includes' in api_response and 'users' in api_response['includes']:
                user_info = api_response['includes']['users'][0]
            
            # íŠ¸ìœ— ë°ì´í„° ì²˜ë¦¬
            account_tweets = []
            for tweet in api_response['data']:
                processed_tweet = process_tweet_data_secondary(tweet, user_info, username, category)
                account_tweets.append(processed_tweet)
            
            all_tweets.extend(account_tweets)
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            if category not in category_stats:
                category_stats[category] = {'tweets': 0, 'accounts': 0, 'api_calls': 0}
            category_stats[category]['tweets'] += len(account_tweets)
            category_stats[category]['accounts'] += 1
            category_stats[category]['api_calls'] += 1
            
            print(f"âœ… {username}: {len(account_tweets)}ê°œ íŠ¸ìœ— ìˆ˜ì§‘")
            
        except Exception as e:
            print(f"âŒ {username} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            total_api_calls += 1  # ì‹¤íŒ¨í•´ë„ API í˜¸ì¶œì€ ì°¨ê°
            
            # ì‹¤íŒ¨ë„ í†µê³„ì— ë°˜ì˜
            category = config.get('category', 'unknown')
            if category not in category_stats:
                category_stats[category] = {'tweets': 0, 'accounts': 0, 'api_calls': 0}
            category_stats[category]['api_calls'] += 1
            continue
    
    print(f"\nðŸ“Š [SECONDARY TOKEN] ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"   ðŸ“± ì´ íŠ¸ìœ—: {len(all_tweets)}ê°œ")
    print(f"   ðŸ”‘ API í˜¸ì¶œ: {total_api_calls}íšŒ")
    print(f"   ðŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼:")
    for category, stats in category_stats.items():
        print(f"      - {category}: {stats['tweets']}ê°œ íŠ¸ìœ— ({stats['accounts']}ê°œ ê³„ì •, {stats['api_calls']}íšŒ í˜¸ì¶œ)")
    
    # XComì— ê²°ê³¼ ì €ìž¥
    context['ti'].xcom_push(key='collected_tweets', value=all_tweets)
    context['ti'].xcom_push(key='api_calls_made', value=total_api_calls)
    context['ti'].xcom_push(key='category_stats', value=category_stats)
    
    return len(all_tweets)

def store_secondary_tweets_to_db(**context):
    """ìˆ˜ì§‘ëœ í™•ìž¥ ê³„ì • íŠ¸ìœ—ì„ DBì— ì €ìž¥"""
    
    # XComì—ì„œ ìˆ˜ì§‘ëœ íŠ¸ìœ— ê°€ì ¸ì˜¤ê¸°
    all_tweets = context['ti'].xcom_pull(key='collected_tweets') or []
    api_calls = context['ti'].xcom_pull(key='api_calls_made') or 0
    category_stats = context['ti'].xcom_pull(key='category_stats') or {}
    
    if not all_tweets:
        print("â„¹ï¸ ì €ìž¥í•  íŠ¸ìœ—ì´ ì—†ìŠµë‹ˆë‹¤")
        return 0
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    success_count = 0
    error_count = 0
    
    print(f"ðŸ’¾ [SECONDARY] DB ì €ìž¥ ì‹œìž‘: {len(all_tweets)}ê°œ íŠ¸ìœ—")
    
    for tweet_data in all_tweets:
        try:
            hook.run(UPSERT_SQL, parameters=tweet_data)
            success_count += 1
            
            # ì§„í–‰ë¥  í‘œì‹œ (100ê°œë§ˆë‹¤)
            if success_count % 100 == 0:
                print(f"ðŸ“Š ì €ìž¥ ì§„í–‰ë¥ : {success_count}/{len(all_tweets)}")
                
        except Exception as e:
            print(f"âŒ íŠ¸ìœ— ì €ìž¥ ì‹¤íŒ¨: {tweet_data.get('tweet_id', 'Unknown')} - {e}")
            error_count += 1
            continue
    
    print(f"âœ… [SECONDARY] ì €ìž¥ ì™„ë£Œ: {success_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
    
    # í†µê³„ ì¡°íšŒ
    try:
        # ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ secondary í† í° íŠ¸ìœ—
        result = hook.get_first("""
            SELECT COUNT(*) FROM x_posts 
            WHERE collected_at >= NOW() - INTERVAL '1 day'
            AND collection_source = 'secondary_token'
        """)
        secondary_today = result[0] if result else 0
        
        # ì „ì²´ íŠ¸ìœ— ìˆ˜
        result = hook.get_first("SELECT COUNT(*) FROM x_posts")
        total_all = result[0] if result else 0
        
        print(f"ðŸ“Š ì˜¤ëŠ˜ Secondary í† í° ìˆ˜ì§‘: {secondary_today}ê°œ")
        print(f"ðŸ“Š ì „ì²´ ì €ìž¥ëœ íŠ¸ìœ—: {total_all}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì¶œë ¥
        print(f"ðŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ê²°ê³¼:")
        for category, stats in category_stats.items():
            print(f"   - {category}: {stats['tweets']}ê°œ íŠ¸ìœ—, {stats['api_calls']}íšŒ API í˜¸ì¶œ")
        
    except Exception as e:
        print(f"âš ï¸ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    print(f"ðŸ”¥ Secondary í† í° ì˜¤ëŠ˜ API í˜¸ì¶œ: {api_calls}íšŒ")
    
    return success_count

# DAG ì •ì˜
with DAG(
    dag_id='ingest_x_posts_secondary_k8s',
    default_args=default_args,
    schedule_interval='0 */8 * * *',  # 8ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (Primaryì™€ ë™ì¼)
    catchup=False,
    description='X API ë‘ ë²ˆì§¸ í† í°ìœ¼ë¡œ í™•ìž¥ ê³„ì • íŠ¸ìœ— ìˆ˜ì§‘ (ì•”í˜¸í™”í, ë¹…í…Œí¬, íˆ¬ìžê¸°ê´€)',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['x_api', 'twitter', 'secondary_token', 'crypto', 'bigtech', 'investment', 'k8s'],
) as dag:
    
    # í…Œì´ë¸” ìƒì„± (ì´ë¯¸ ì¡´ìž¬í•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ê°™ì€ í…Œì´ë¸” ì‚¬ìš©)
    create_table = PostgresOperator(
        task_id='create_x_posts_table_secondary',
        postgres_conn_id='postgres_default',
        sql='create_x_posts.sql',
    )
    
    # ë‘ ë²ˆì§¸ í† í°ìœ¼ë¡œ í™•ìž¥ ê³„ì •ë“¤ì˜ íŠ¸ìœ— ìˆ˜ì§‘
    fetch_tweets = PythonOperator(
        task_id='fetch_secondary_tweets',
        python_callable=fetch_secondary_tweets,
    )
    
    # DB ì €ìž¥
    store_tweets = PythonOperator(
        task_id='store_secondary_tweets_to_db',
        python_callable=store_secondary_tweets_to_db,
    )
    
    # Task ì˜ì¡´ì„±
    create_table >> fetch_tweets >> store_tweets