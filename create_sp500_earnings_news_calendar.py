# dags/create_sp500_earnings_news_calendar.py

import os
from datetime import datetime, timedelta, date
from typing import Dict, List, Any

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.hooks.postgres_hook import PostgresHook
from airflow.utils.dates import days_ago

# ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'investment-assistant',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': None,
    'retry_delay': timedelta(minutes=1),
}

# ê²½ë¡œ ì„¤ì •
DAGS_SQL_DIR = os.path.join(os.path.dirname(__file__), "sql")
INITDB_SQL_DIR = os.path.join(os.path.dirname(__file__), "initdb")

# SQL íŒŒì¼ ì½ê¸°
with open(os.path.join(DAGS_SQL_DIR, "upsert_sp500_earnings_calendar.sql"), encoding="utf-8") as f:
    UPSERT_CALENDAR_SQL = f.read()

with open(os.path.join(DAGS_SQL_DIR, "upsert_sp500_earnings_news.sql"), encoding="utf-8") as f:
    UPSERT_NEWS_SQL = f.read()

def check_data_sources(**context):
    """ê¸°ì¡´ ë°ì´í„° ì†ŒìŠ¤ í™•ì¸ ë° í†µê³„"""
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    print("ðŸ” SP500 ì‹¤ì  ìº˜ë¦°ë” ë°ì´í„° ì†ŒìŠ¤ ì ê²€...")
    
    # 1. earnings_calendar í…Œì´ë¸” í™•ì¸
    earnings_count = hook.get_first("SELECT COUNT(*) FROM earnings_calendar")[0]
    print(f"ðŸ“Š earnings_calendar: {earnings_count:,}ê°œ ë ˆì½”ë“œ")
    
    # 2. sp500_companies í…Œì´ë¸” í™•ì¸
    sp500_count = hook.get_first("SELECT COUNT(*) FROM sp500_companies")[0] 
    print(f"ðŸ¢ sp500_companies: {sp500_count:,}ê°œ ê¸°ì—…")
    
    # 3. SP500 ê¸°ì—…ì˜ ì‹¤ì  ì¼ì • í™•ì¸
    sp500_earnings = hook.get_first("""
        SELECT COUNT(*)
        FROM earnings_calendar ec
        INNER JOIN sp500_companies sp ON ec.symbol = sp.symbol
        WHERE ec.report_date >= CURRENT_DATE - INTERVAL '30 days'
          AND ec.report_date <= CURRENT_DATE + INTERVAL '365 days'
    """)[0]
    print(f"ðŸŽ¯ SP500 ê¸°ì—… ì‹¤ì  ì¼ì •: {sp500_earnings:,}ê°œ")
    
    # 4. ë‰´ìŠ¤ í…Œì´ë¸”ë³„ í†µê³„
    news_tables = ['earnings_news_finnhub', 'company_news', 'market_news', 'market_news_sentiment']
    news_stats = {}
    
    for table in news_tables:
        try:
            count = hook.get_first(f"SELECT COUNT(*) FROM {table}")[0]
            news_stats[table] = count
            print(f"ðŸ“° {table}: {count:,}ê°œ ë‰´ìŠ¤")
        except Exception as e:
            news_stats[table] = 0
            print(f"âŒ {table}: ì¡°íšŒ ì‹¤íŒ¨ - {e}")
    
    context['ti'].xcom_push(key='data_stats', value={
        'earnings_count': earnings_count,
        'sp500_count': sp500_count,
        'sp500_earnings': sp500_earnings,
        'news_stats': news_stats
    })
    
    return sp500_earnings

def extract_sp500_earnings_schedule(**context):
    """SP500 ê¸°ì—…ì˜ ì‹¤ì  ì¼ì • ì¶”ì¶œ"""
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    print("ðŸ“‹ SP500 ê¸°ì—… ì‹¤ì  ì¼ì • ì¶”ì¶œ ì¤‘...")
    
    query = """
        SELECT 
            ec.symbol,
            ec.report_date,
            ec.fiscal_date_ending,
            ec.estimate,
            ec.currency,
            sp.company_name,
            sp.gics_sector,
            sp.gics_sub_industry,
            sp.headquarters
        FROM earnings_calendar ec
        INNER JOIN sp500_companies sp ON ec.symbol = sp.symbol
        WHERE ec.report_date >= CURRENT_DATE - INTERVAL '30 days'
          AND ec.report_date <= CURRENT_DATE + INTERVAL '365 days'
        ORDER BY ec.report_date, ec.symbol
    """
    
    earnings_schedules = hook.get_records(query)
    
    print(f"ðŸ“Š ì¶”ì¶œëœ SP500 ì‹¤ì  ì¼ì •: {len(earnings_schedules)}ê°œ")
    
    if earnings_schedules:
        print("\nðŸ“‹ ìƒ˜í”Œ ì‹¤ì  ì¼ì •:")
        for i, schedule in enumerate(earnings_schedules[:5]):
            symbol, report_date, _, estimate, _, company_name, sector, _, _ = schedule
            print(f"   {i+1}. {symbol} ({company_name}): {report_date} - {sector}")
    
    earnings_data = [
        {
            'symbol': row[0],
            'report_date': row[1],
            'fiscal_date_ending': row[2],
            'estimate': row[3],
            'currency': row[4],
            'company_name': row[5],
            'gics_sector': row[6],
            'gics_sub_industry': row[7],
            'headquarters': row[8]
        } for row in earnings_schedules
    ]
    
    context['ti'].xcom_push(key='earnings_schedules', value=earnings_data)
    
    return len(earnings_schedules)

def collect_news_from_table(hook: PostgresHook, table_name: str, symbol: str, company_name: str, 
                           start_date: date, end_date: date, news_section: str, report_date: date) -> List[Dict]:
    """íŠ¹ì • í…Œì´ë¸”ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    
    news_list = []
    
    try:
        if table_name == 'earnings_news_finnhub':
            query = """
                SELECT headline, summary, source, datetime, url
                FROM earnings_news_finnhub 
                WHERE symbol = %s 
                  AND datetime BETWEEN %s AND %s
                  AND url IS NOT NULL
                ORDER BY datetime DESC 
                LIMIT 10
            """
            params = (symbol, start_date, end_date)
            
        elif table_name == 'company_news':
            query = """
                SELECT title, description, source, published_at, url
                FROM company_news 
                WHERE symbol = %s 
                  AND published_at BETWEEN %s AND %s
                  AND url IS NOT NULL
                ORDER BY published_at DESC 
                LIMIT 10
            """
            params = (symbol, start_date, end_date)
            
        elif table_name == 'market_news':
            query = """
                SELECT title, description, source, published_at, url
                FROM market_news 
                WHERE (title ILIKE %s OR title ILIKE %s OR content ILIKE %s OR content ILIKE %s)
                  AND published_at BETWEEN %s AND %s
                  AND url IS NOT NULL
                ORDER BY published_at DESC 
                LIMIT 10
            """
            params = (f'%{symbol}%', f'%{company_name}%', f'%{symbol}%', f'%{company_name}%', 
                     start_date, end_date)
            
        elif table_name == 'market_news_sentiment':
            query = """
                SELECT title, summary, source, time_published, url
                FROM market_news_sentiment 
                WHERE ticker_sentiment::text LIKE %s
                  AND time_published BETWEEN %s AND %s
                  AND url IS NOT NULL
                ORDER BY time_published DESC 
                LIMIT 10
            """
            params = (f'%"ticker": "{symbol}"%', start_date, end_date)
            
        else:
            return []
        
        results = hook.get_records(query, params)
        
        for row in results:
            title, content, source, published_at, url = row
            
            if published_at and hasattr(published_at, 'date'):
                days_diff = (published_at.date() - report_date).days
            else:
                days_diff = 0
                
            news_list.append({
                'source_table': table_name,
                'title': title or '',
                'content': content or '',
                'source': source or '',
                'published_at': published_at,
                'url': url,
                'news_section': news_section,
                'days_from_earnings': days_diff
            })
        
    except Exception as e:
        print(f"âŒ {table_name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ({symbol}): {e}")
    
    return news_list

def collect_related_news_for_each_schedule(**context):
    """ê° ì‹¤ì  ì¼ì •ë³„ë¡œ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    earnings_schedules = context['ti'].xcom_pull(key='earnings_schedules', task_ids='extract_sp500_earnings_schedule')
    
    if not earnings_schedules:
        print("âŒ ì‹¤ì  ì¼ì • ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    print(f"ðŸŽ¯ {len(earnings_schedules)}ê°œ ì‹¤ì  ì¼ì •ì— ëŒ€í•œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìž‘...")
    
    news_tables = ['earnings_news_finnhub', 'company_news', 'market_news', 'market_news_sentiment']
    processed_count = 0
    total_news_collected = 0
    
    consolidated_data = []
    
    for i, schedule in enumerate(earnings_schedules):
        symbol = schedule['symbol']
        company_name = schedule['company_name'] or symbol
        report_date = schedule['report_date']
        
        if isinstance(report_date, str):
            report_date = datetime.strptime(report_date, '%Y-%m-%d').date()
        
        try:
            print(f"ðŸ“Š ì²˜ë¦¬ ì¤‘ ({i+1}/{len(earnings_schedules)}): {symbol} ({company_name}) - {report_date}")
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            forecast_start = report_date - timedelta(days=14)
            forecast_end = report_date - timedelta(days=1)
            reaction_start = report_date + timedelta(days=1)
            reaction_end = report_date + timedelta(days=7)
            
            all_news = []
            
            # ê° ë‰´ìŠ¤ í…Œì´ë¸”ì—ì„œ ìˆ˜ì§‘
            for table in news_tables:
                # ì˜ˆìƒ êµ¬ê°„ ë‰´ìŠ¤
                forecast_news = collect_news_from_table(
                    hook, table, symbol, company_name, 
                    forecast_start, forecast_end, 'forecast', report_date
                )
                all_news.extend(forecast_news)
                
                # ë°˜ì‘ êµ¬ê°„ ë‰´ìŠ¤
                reaction_news = collect_news_from_table(
                    hook, table, symbol, company_name, 
                    reaction_start, reaction_end, 'reaction', report_date
                )
                all_news.extend(reaction_news)
            
            # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
            unique_news = {}
            for news in all_news:
                url = news['url']
                if url not in unique_news:
                    unique_news[url] = news
            
            unique_news_list = list(unique_news.values())
            total_count = len(unique_news_list)
            total_news_collected += total_count
            
            # ì´ë²¤íŠ¸ ì •ë³´ ìƒì„±
            event_title = f"{symbol} ì‹¤ì  ë°œí‘œ"
            if schedule.get('estimate'):
                event_title += f" (ì˜ˆìƒ EPS: ${schedule['estimate']})"
                
            event_description = f"{company_name}ì˜ ì‹¤ì  ë°œí‘œ ì¼ì •ìž…ë‹ˆë‹¤."
            if total_count > 0:
                event_description += f" ê´€ë ¨ ë‰´ìŠ¤ {total_count}ê°œê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤."
            
            
            # í†µí•© ë°ì´í„° ì¤€ë¹„
            consolidated_item = {
                'schedule': schedule,
                'news_list': unique_news_list,
                'event_info': {
                    'event_title': event_title,
                    'event_description': event_description,
                    'total_news_count': total_count,
                    'forecast_news_count': sum(1 for n in unique_news_list if n['news_section'] == 'forecast'),
                    'reaction_news_count': sum(1 for n in unique_news_list if n['news_section'] == 'reaction')
                }
            }
            
            consolidated_data.append(consolidated_item)
            processed_count += 1
            
            forecast_count = sum(1 for n in unique_news_list if n['news_section'] == 'forecast')
            reaction_count = sum(1 for n in unique_news_list if n['news_section'] == 'reaction')
            print(f"âœ… {symbol} ì™„ë£Œ: ì˜ˆìƒ {forecast_count}ê°œ, ë°˜ì‘ {reaction_count}ê°œ, ì´ {total_count}ê°œ")
            
            if (i + 1) % 50 == 0:
                print(f"ðŸ“ˆ ì§„í–‰ë¥ : {i+1}/{len(earnings_schedules)} ({(i+1)/len(earnings_schedules)*100:.1f}%)")
            
        except Exception as e:
            print(f"âŒ {symbol} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    print(f"\nðŸ“Š ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"   âœ… ì²˜ë¦¬ëœ ì‹¤ì  ì¼ì •: {processed_count}/{len(earnings_schedules)}ê°œ")
    print(f"   ðŸ“° ì´ ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {total_news_collected}ê°œ")
    if processed_count > 0:
        print(f"   ðŸ“ˆ í‰ê·  ë‰´ìŠ¤/ì¼ì •: {total_news_collected/processed_count:.1f}ê°œ")
    
    context['ti'].xcom_push(key='consolidated_data', value=consolidated_data)
    
    return processed_count

def upsert_consolidated_data(**context):
    """í†µí•©ëœ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ìž¥"""
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    consolidated_data = context['ti'].xcom_pull(key='consolidated_data', task_ids='collect_related_news_for_each_schedule')
    
    if not consolidated_data:
        print("âŒ í†µí•© ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0
    
    print(f"ðŸ’¾ {len(consolidated_data)}ê°œ ì‹¤ì  ìº˜ë¦°ë” ë°ì´í„° ì €ìž¥ ì‹œìž‘...")
    
    calendar_saved = 0
    news_saved = 0
    
    for item in consolidated_data:
        schedule = item['schedule']
        news_list = item['news_list']
        event_info = item['event_info']
        
        try:
            # 1. ìº˜ë¦°ë” ë°ì´í„° ì €ìž¥
            calendar_params = {
                'symbol': schedule['symbol'],
                'company_name': schedule['company_name'],
                'report_date': schedule['report_date'],
                'fiscal_date_ending': schedule['fiscal_date_ending'],
                'estimate': schedule['estimate'],
                'currency': schedule['currency'],
                'gics_sector': schedule['gics_sector'],
                'gics_sub_industry': schedule['gics_sub_industry'],
                'headquarters': schedule['headquarters'],
                'event_type': 'earnings_report',
                'event_title': event_info['event_title'],
                'event_description': event_info['event_description'],
                'total_news_count': event_info['total_news_count'],
                'forecast_news_count': event_info['forecast_news_count'],
                'reaction_news_count': event_info['reaction_news_count']
            }
            
            hook.run(UPSERT_CALENDAR_SQL, parameters=calendar_params)
            calendar_saved += 1
            
            # 2. calendar_id ì¡°íšŒ
            calendar_id = hook.get_first("""
                SELECT id FROM sp500_earnings_calendar 
                WHERE symbol = %s AND report_date = %s
            """, (schedule['symbol'], schedule['report_date']))[0]
            
            # 3. ë‰´ìŠ¤ ë°ì´í„° ì €ìž¥
            for news in news_list:
                news_params = {
                    'calendar_id': calendar_id,
                    'source_table': news['source_table'],
                    'title': news['title'],
                    'url': news['url'],
                    'summary': news['content'],  # contentë¥¼ summaryë¡œ ë§¤í•‘
                    'content': None,  # content ì»¬ëŸ¼ì€ ë¹„ì›Œë‘ 
                    'source': news['source'],
                    'published_at': news['published_at'],
                    'news_section': news['news_section'],
                    'days_from_earnings': news['days_from_earnings']
                }
                
                try:
                    hook.run(UPSERT_NEWS_SQL, parameters=news_params)
                    news_saved += 1
                except Exception as e:
                    print(f"âš ï¸ ë‰´ìŠ¤ ì €ìž¥ ì‹¤íŒ¨ ({schedule['symbol']}, {news['url']}): {e}")
                    continue
            
            print(f"âœ… {schedule['symbol']} ì €ìž¥ ì™„ë£Œ: ìº˜ë¦°ë” 1ê°œ, ë‰´ìŠ¤ {len(news_list)}ê°œ")
            
        except Exception as e:
            print(f"âŒ {schedule['symbol']} ì €ìž¥ ì‹¤íŒ¨: {e}")
            continue
    
    print(f"\nðŸ’¾ ë°ì´í„° ì €ìž¥ ì™„ë£Œ:")
    print(f"   ðŸ“… ìº˜ë¦°ë”: {calendar_saved}ê°œ")
    print(f"   ðŸ“° ë‰´ìŠ¤: {news_saved}ê°œ")
    
    return calendar_saved

# DAG ì •ì˜
with DAG(
    dag_id='create_sp500_earnings_calendar',
    default_args=default_args,
    schedule_interval='@weekly',
    catchup=False,
    description='SP500 ê¸°ì—… ì‹¤ì  ìº˜ë¦°ë” ìƒì„± (ë‰´ìŠ¤ í†µí•©)',
    template_searchpath=[INITDB_SQL_DIR],
    tags=['sp500', 'earnings', 'calendar', 'news'],
) as dag:
    
    # í…Œì´ë¸” ìƒì„±
    create_tables = PostgresOperator(
        task_id='create_sp500_earnings_tables',
        postgres_conn_id='postgres_default',
        sql='create_sp500_earnings_calendar.sql',
    )
    
    # ë°ì´í„° ì†ŒìŠ¤ í™•ì¸
    check_sources = PythonOperator(
        task_id='check_data_sources',
        python_callable=check_data_sources,
    )
    
    # SP500 ì‹¤ì  ì¼ì • ì¶”ì¶œ
    extract_schedules = PythonOperator(
        task_id='extract_sp500_earnings_schedule',
        python_callable=extract_sp500_earnings_schedule,
    )
    
    # ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘
    collect_news = PythonOperator(
        task_id='collect_related_news_for_each_schedule',
        python_callable=collect_related_news_for_each_schedule,
    )
    
    # ë°ì´í„° ì €ìž¥
    save_data = PythonOperator(
        task_id='upsert_consolidated_data',
        python_callable=upsert_consolidated_data,
    )
    
    # Task ì˜ì¡´ì„±
    create_tables >> check_sources >> extract_schedules >> collect_news >> save_data